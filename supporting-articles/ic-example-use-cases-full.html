<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>INTIENT Clinical Suite Use Cases</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: no-wrap;}
    img{max-width: 100%}
    figcaption{font-weight: bold}
    pre{width: 100%; overflow-x:auto; background-color: GhostWhite; font-family: Arial, Helvetica, sans-serif; font-size: .9rem; line-height: 1.2rem}
    h1{font-family: Arial, Helvetica, sans-serif; font-size: 2rem; font-weight: bold;}
    h2{font-family: Arial, Helvetica, sans-serif; font-size: 1.4rem; font-weight: normal;}
    h3{font-family: Arial, Helvetica, sans-serif; font-size: 1.25rem; font-weight: normal;}
    h4{font-family: Arial, Helvetica, sans-serif; font-size: 1.1rem; font-weight: normal;}
    h5{font-family: Arial, Helvetica, sans-serif; font-size: 1rem; font-style: italic; font-weight: normal;}
    h6{font-family: Arial, Helvetica, sans-serif; font-size: .9rem; font-style: italic; font-weight: normal;}
    p{line-height: 1.6; font-family: Arial, Helvetica, sans-serif; font-size: 0.9rem}
    li{line-height: 1.6; font-family: Arial, Helvetica, sans-serif; font-size: 0.9rem}
    #tcss{line-height: 1.6; font-family: Arial, Helvetica, sans-serif; font-size: 0.9rem; border-collapse: collapse; width: 100%;}
    #tcss td, #tcss th{border: 1px solid #ddd; padding: 4px; vertical-align: top;}
    #tcss tr:nth-child(even){background-color: #f2f2f2;}
    #tcss tr:hover{background-color: #ddd;}
    #tcss th{padding: 4px; text-align: left; background-color: grey; color: white;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">INTIENT Clinical Suite Use Cases</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#classification"><span class="toc-section-number">1</span> Classification</a></li>
<li><a href="#purpose-and-scope"><span class="toc-section-number">2</span> Purpose and Scope</a></li>
<li><a href="#module-description"><span class="toc-section-number">3</span> Module Description</a></li>
<li><a href="#references"><span class="toc-section-number">4</span> References</a></li>
<li><a href="#product-overview"><span class="toc-section-number">5</span> Product Overview</a></li>
<li><a href="#use-cases"><span class="toc-section-number">6</span> Use Cases</a>
<ul>
<li><a href="#access-intient-clinical"><span class="toc-section-number">6.1</span> Access INTIENT Clinical</a>
<ul>
<li><a href="#base-flow---login-and-access-successfully"><span class="toc-section-number">6.1.1</span> Base Flow - Login and Access Successfully</a></li>
<li><a href="#alternatives"><span class="toc-section-number">6.1.2</span> Alternatives</a>
<ul>
<li><a href="#a---login-failure"><span class="toc-section-number">6.1.2.1</span> 2a - Login Failure</a></li>
<li><a href="#b---login-role-restriction"><span class="toc-section-number">6.1.2.2</span> 2b - Login Role Restriction</a></li>
</ul></li>
<li><a href="#feature-details"><span class="toc-section-number">6.1.3</span> Feature Details</a>
<ul>
<li><a href="#access-system"><span class="toc-section-number">6.1.3.1</span> Access System</a></li>
</ul></li>
</ul></li>
<li><a href="#ingest-data"><span class="toc-section-number">6.2</span> Ingest Data</a>
<ul>
<li><a href="#base-flow---ingest-data"><span class="toc-section-number">6.2.1</span> Base Flow - Ingest Data</a></li>
<li><a href="#feature-details-1"><span class="toc-section-number">6.2.2</span> Feature Details</a>
<ul>
<li><a href="#ingest-data-1"><span class="toc-section-number">6.2.2.1</span> Ingest Data</a></li>
</ul></li>
</ul></li>
<li><a href="#create-data-provider"><span class="toc-section-number">6.3</span> Create Data Provider</a>
<ul>
<li><a href="#base-flow---user-creates-data-provider"><span class="toc-section-number">6.3.1</span> Base Flow - User Creates Data Provider</a></li>
<li><a href="#alternatives-1"><span class="toc-section-number">6.3.2</span> Alternatives</a>
<ul>
<li><a href="#a---data-provider-for-operational-data-load-only"><span class="toc-section-number">6.3.2.1</span> 2a - Data Provider for Operational data load only</a></li>
<li><a href="#b---data-provider-for-study-data-load-only"><span class="toc-section-number">6.3.2.2</span> 2b - Data Provider for Study data load only</a></li>
<li><a href="#a---data-provider-already-exists"><span class="toc-section-number">6.3.2.3</span> 3a - Data Provider already exists</a></li>
</ul></li>
<li><a href="#feature-details-2"><span class="toc-section-number">6.3.3</span> Feature Details</a>
<ul>
<li><a href="#create-data-provider-1"><span class="toc-section-number">6.3.3.1</span> Create Data Provider</a></li>
</ul></li>
</ul></li>
<li><a href="#load-data"><span class="toc-section-number">6.4</span> Load Data</a>
<ul>
<li><a href="#base-flow---load-data"><span class="toc-section-number">6.4.1</span> Base Flow - Load Data</a></li>
<li><a href="#alternatives-2"><span class="toc-section-number">6.4.2</span> Alternatives</a>
<ul>
<li><a href="#a---user-lacks-permissions"><span class="toc-section-number">6.4.2.1</span> 1a - User lacks permissions</a></li>
<li><a href="#b---data-load-via-gcs"><span class="toc-section-number">6.4.2.2</span> 1b - Data Load via GCS</a></li>
<li><a href="#c---data-load-via-sftp"><span class="toc-section-number">6.4.2.3</span> 1c - Data Load via sFTP</a></li>
<li><a href="#a---data-load-for-study"><span class="toc-section-number">6.4.2.4</span> 3a - Data Load for Study</a></li>
<li><a href="#a---data-load-failed"><span class="toc-section-number">6.4.2.5</span> 6a - Data Load Failed</a></li>
</ul></li>
<li><a href="#feature-details-3"><span class="toc-section-number">6.4.3</span> Feature Details</a>
<ul>
<li><a href="#load-data-1"><span class="toc-section-number">6.4.3.1</span> Load Data</a></li>
</ul></li>
</ul></li>
<li><a href="#view-data-and-data-profile"><span class="toc-section-number">6.5</span> View Data and Data Profile</a>
<ul>
<li><a href="#base-flow---view-data-and-data-profiles"><span class="toc-section-number">6.5.1</span> Base Flow - View Data and Data Profiles</a></li>
<li><a href="#feature-details-4"><span class="toc-section-number">6.5.2</span> Feature Details</a>
<ul>
<li><a href="#view-and-profile-data"><span class="toc-section-number">6.5.2.1</span> View and Profile Data</a></li>
</ul></li>
</ul></li>
<li><a href="#browse-query-and-snapshot-data"><span class="toc-section-number">6.6</span> Browse, Query and Snapshot Data</a>
<ul>
<li><a href="#base-flow---create-and-use-snapshot-data"><span class="toc-section-number">6.6.1</span> Base Flow - Create and Use Snapshot Data</a></li>
<li><a href="#alternatives-3"><span class="toc-section-number">6.6.2</span> Alternatives</a>
<ul>
<li><a href="#a---user-selects-produce-sdtm-option"><span class="toc-section-number">6.6.2.1</span> 2a - User selects Produce SDTM option</a></li>
<li><a href="#a---build-snapshot-using-filters"><span class="toc-section-number">6.6.2.2</span> 3a - Build snapshot using filters</a></li>
<li><a href="#a---build-snapshot-using-criteria-file"><span class="toc-section-number">6.6.2.3</span> 5a - Build Snapshot Using Criteria File</a></li>
<li><a href="#a---user-enters-invalid-snapshot-name-non-unique-or-with-invalid-characters"><span class="toc-section-number">6.6.2.4</span> 6a - User enters invalid snapshot name (non-unique or with invalid characters)</a></li>
</ul></li>
<li><a href="#feature-details-5"><span class="toc-section-number">6.6.3</span> Feature Details</a>
<ul>
<li><a href="#browse-query-and-snapshot-data-1"><span class="toc-section-number">6.6.3.1</span> Browse, Query and Snapshot Data</a></li>
</ul></li>
</ul></li>
<li><a href="#create-data-filter"><span class="toc-section-number">6.7</span> Create Data Filter</a>
<ul>
<li><a href="#base-flow---create-data-filter"><span class="toc-section-number">6.7.1</span> Base Flow - Create Data Filter</a></li>
<li><a href="#feature-details-6"><span class="toc-section-number">6.7.2</span> Feature Details</a>
<ul>
<li><a href="#create-data-filter-1"><span class="toc-section-number">6.7.2.1</span> Create Data Filter</a></li>
</ul></li>
</ul></li>
<li><a href="#create-data-query"><span class="toc-section-number">6.8</span> Create Data Query</a>
<ul>
<li><a href="#base-flow---create-data-query"><span class="toc-section-number">6.8.1</span> Base Flow - Create Data Query</a></li>
<li><a href="#alternatives-4"><span class="toc-section-number">6.8.2</span> Alternatives</a>
<ul>
<li><a href="#a---user-opens-saved-query"><span class="toc-section-number">6.8.2.1</span> 1a - User opens saved query</a></li>
<li><a href="#a---use-query-builder"><span class="toc-section-number">6.8.2.2</span> 2a - Use Query Builder</a></li>
<li><a href="#a---invalid-query"><span class="toc-section-number">6.8.2.3</span> 4a - Invalid query</a></li>
<li><a href="#a---user-enters-invalid-query-name-non-unique-or-with-invalid-characters"><span class="toc-section-number">6.8.2.4</span> 5a - User enters invalid query name (non-unique or with invalid characters)</a></li>
</ul></li>
<li><a href="#feature-details-7"><span class="toc-section-number">6.8.3</span> Feature Details</a>
<ul>
<li><a href="#create-data-query-1"><span class="toc-section-number">6.8.3.1</span> Create Data Query</a></li>
</ul></li>
</ul></li>
<li><a href="#schedule-data-extraction"><span class="toc-section-number">6.9</span> Schedule Data Extraction</a>
<ul>
<li><a href="#base-flow---data-extraction"><span class="toc-section-number">6.9.1</span> Base Flow - Data Extraction</a></li>
<li><a href="#alternatives-5"><span class="toc-section-number">6.9.2</span> Alternatives</a>
<ul>
<li><a href="#a---user-does-not-have-permission-to-extract"><span class="toc-section-number">6.9.2.1</span> 2a - User does not have permission to extract</a></li>
<li><a href="#a---pauseresume-scheduling"><span class="toc-section-number">6.9.2.2</span> 3a - Pause/Resume Scheduling</a></li>
</ul></li>
<li><a href="#feature-details-8"><span class="toc-section-number">6.9.3</span> Feature Details</a>
<ul>
<li><a href="#data-extraction"><span class="toc-section-number">6.9.3.1</span> Data Extraction</a></li>
</ul></li>
</ul></li>
<li><a href="#on-demand-data-extraction"><span class="toc-section-number">6.10</span> On-Demand Data Extraction</a>
<ul>
<li><a href="#base-flow---data-extraction-on-demand"><span class="toc-section-number">6.10.1</span> Base Flow - Data Extraction On Demand</a></li>
<li><a href="#feature-details-9"><span class="toc-section-number">6.10.2</span> Feature Details</a>
<ul>
<li><a href="#on-demand-data-extraction-1"><span class="toc-section-number">6.10.2.1</span> On-Demand Data Extraction</a></li>
</ul></li>
</ul></li>
<li><a href="#manual-study-registration"><span class="toc-section-number">6.11</span> Manual Study Registration</a>
<ul>
<li><a href="#base-flow---register-study"><span class="toc-section-number">6.11.1</span> Base Flow - Register Study</a></li>
<li><a href="#alternatives-6"><span class="toc-section-number">6.11.2</span> Alternatives</a>
<ul>
<li><a href="#a---user-lacks-permissions-to-register-a-study"><span class="toc-section-number">6.11.2.1</span> 2a - User lacks permissions to register a study</a></li>
<li><a href="#b---all-required-information-is-not-entered"><span class="toc-section-number">6.11.2.2</span> 2b - All required information is not entered</a></li>
<li><a href="#a---duplicate-study-id-detected"><span class="toc-section-number">6.11.2.3</span> 3a - Duplicate Study ID detected</a></li>
<li><a href="#a---user-attempts-to-bypass-ui-restrictions-to-register-a-study"><span class="toc-section-number">6.11.2.4</span> 4a - User attempts to bypass UI restrictions to register a study</a></li>
</ul></li>
<li><a href="#feature-details-10"><span class="toc-section-number">6.11.3</span> Feature Details</a>
<ul>
<li><a href="#manual-study-registration-1"><span class="toc-section-number">6.11.3.1</span> Manual Study Registration</a></li>
</ul></li>
<li><a href="#study-registration"><span class="toc-section-number">6.11.4</span> Study Registration</a></li>
</ul></li>
<li><a href="#batch-study-registration"><span class="toc-section-number">6.12</span> Batch Study Registration</a>
<ul>
<li><a href="#base-flow---batch-study-registration"><span class="toc-section-number">6.12.1</span> Base Flow - Batch Study Registration</a></li>
<li><a href="#alternatives-7"><span class="toc-section-number">6.12.2</span> Alternatives</a>
<ul>
<li><a href="#a---multiple-registration-providers"><span class="toc-section-number">6.12.2.1</span> 1a - Multiple Registration Providers</a></li>
<li><a href="#a---incorrect-information-in-the-system-of-record"><span class="toc-section-number">6.12.2.2</span> 3a - Incorrect information in the system of record</a></li>
</ul></li>
<li><a href="#feature-details-11"><span class="toc-section-number">6.12.3</span> Feature Details</a>
<ul>
<li><a href="#batch-study-registration-1"><span class="toc-section-number">6.12.3.1</span> Batch Study Registration</a></li>
</ul></li>
</ul></li>
<li><a href="#clinical-insights"><span class="toc-section-number">6.13</span> Clinical Insights</a>
<ul>
<li><a href="#base-flow---visualize-real-time-study-data"><span class="toc-section-number">6.13.1</span> Base Flow - Visualize Real Time Study Data</a></li>
<li><a href="#feature-details-12"><span class="toc-section-number">6.13.2</span> Feature Details</a>
<ul>
<li><a href="#navigate-to-clinical-studies-clinical-insights"><span class="toc-section-number">6.13.2.1</span> Navigate to Clinical Studies-Clinical Insights</a></li>
</ul></li>
</ul></li>
<li><a href="#operational-insights"><span class="toc-section-number">6.14</span> Operational Insights</a>
<ul>
<li><a href="#base-flow---operational-insights"><span class="toc-section-number">6.14.1</span> Base Flow - Operational Insights</a></li>
<li><a href="#alternatives-8"><span class="toc-section-number">6.14.2</span> Alternatives</a>
<ul>
<li><a href="#a---threshold-settings"><span class="toc-section-number">6.14.2.1</span> 3a - Threshold settings</a></li>
</ul></li>
<li><a href="#feature-details-13"><span class="toc-section-number">6.14.3</span> Feature Details</a>
<ul>
<li><a href="#navigate-to-clinical-studies-operational-insights"><span class="toc-section-number">6.14.3.1</span> Navigate to Clinical Studies-Operational Insights</a></li>
</ul></li>
</ul></li>
<li><a href="#administer-access"><span class="toc-section-number">6.15</span> Administer Access</a>
<ul>
<li><a href="#base-flow---administer-access"><span class="toc-section-number">6.15.1</span> Base Flow - Administer Access</a></li>
<li><a href="#alternatives-9"><span class="toc-section-number">6.15.2</span> Alternatives</a>
<ul>
<li><a href="#a---user-lacks-permission-to-administer-access"><span class="toc-section-number">6.15.2.1</span> 1a - User lacks permission to Administer Access</a></li>
<li><a href="#a-3a-4a---the-system-provides-user-list"><span class="toc-section-number">6.15.2.2</span> 2a, 3a, 4a - The system provides user list</a></li>
</ul></li>
<li><a href="#feature-details-14"><span class="toc-section-number">6.15.3</span> Feature Details</a>
<ul>
<li><a href="#administer-user-roles"><span class="toc-section-number">6.15.3.1</span> Administer User Roles</a></li>
</ul></li>
</ul></li>
<li><a href="#associate-user-to-roles"><span class="toc-section-number">6.16</span> Associate User to Roles</a>
<ul>
<li><a href="#base-flow---associate-user-to-role"><span class="toc-section-number">6.16.1</span> Base Flow - Associate User to Role</a></li>
<li><a href="#feature-details-15"><span class="toc-section-number">6.16.2</span> Feature Details</a>
<ul>
<li><a href="#administer-user-roles-1"><span class="toc-section-number">6.16.2.1</span> Administer User Roles</a></li>
</ul></li>
</ul></li>
<li><a href="#associate-user-to-studies"><span class="toc-section-number">6.17</span> Associate User to Studies</a>
<ul>
<li><a href="#base-flow---user-to-study-access-association"><span class="toc-section-number">6.17.1</span> Base Flow - User to Study Access Association</a></li>
<li><a href="#feature-details-16"><span class="toc-section-number">6.17.2</span> Feature Details</a>
<ul>
<li><a href="#administer-user-studies"><span class="toc-section-number">6.17.2.1</span> Administer User Studies</a></li>
</ul></li>
</ul></li>
<li><a href="#associate-user-to-projects"><span class="toc-section-number">6.18</span> Associate User to Projects</a>
<ul>
<li><a href="#base-flow---user-to-project-access-association"><span class="toc-section-number">6.18.1</span> Base Flow - User to Project Access Association</a></li>
<li><a href="#alternatives-10"><span class="toc-section-number">6.18.2</span> Alternatives</a>
<ul>
<li><a href="#a---user-does-not-have-access-to-one-or-more-studies-associated-to-the-analytics-project"><span class="toc-section-number">6.18.2.1</span> 3a - User does not have access to one or more studies associated to the Analytics Project</a></li>
</ul></li>
<li><a href="#feature-details-17"><span class="toc-section-number">6.18.3</span> Feature Details</a>
<ul>
<li><a href="#administer-user-projects"><span class="toc-section-number">6.18.3.1</span> Administer User Projects</a></li>
</ul></li>
</ul></li>
<li><a href="#role-information"><span class="toc-section-number">6.19</span> Role Information</a></li>
<li><a href="#archive-data"><span class="toc-section-number">6.20</span> Archive Data</a>
<ul>
<li><a href="#process-archived-files"><span class="toc-section-number">6.20.1</span> Process Archived Files</a></li>
<li><a href="#base-flow---archive-data"><span class="toc-section-number">6.20.2</span> Base Flow - Archive Data</a></li>
<li><a href="#feature-details-18"><span class="toc-section-number">6.20.3</span> Feature Details</a>
<ul>
<li><a href="#data-archive"><span class="toc-section-number">6.20.3.1</span> Data Archive</a></li>
</ul></li>
</ul></li>
<li><a href="#process-versioning"><span class="toc-section-number">6.21</span> Process Versioning</a>
<ul>
<li><a href="#base-flow---user-level-actions"><span class="toc-section-number">6.21.1</span> Base Flow - User Level Actions</a></li>
<li><a href="#base-flow---study-level-actions"><span class="toc-section-number">6.21.2</span> Base Flow - Study Level Actions</a></li>
<li><a href="#feature-details-19"><span class="toc-section-number">6.21.3</span> Feature Details</a>
<ul>
<li><a href="#process-versioning-1"><span class="toc-section-number">6.21.3.1</span> Process Versioning</a></li>
</ul></li>
</ul></li>
<li><a href="#non-functional-requirements"><span class="toc-section-number">6.22</span> Non-Functional Requirements</a>
<ul>
<li><a href="#feature-details-20"><span class="toc-section-number">6.22.1</span> Feature Details</a>
<ul>
<li><a href="#non-functional-requirements-1"><span class="toc-section-number">6.22.1.1</span> Non-Functional Requirements</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#revision-history"><span class="toc-section-number">7</span> Revision History</a></li>
<li><a href="#document-approvals"><span class="toc-section-number">8</span> Document Approvals</a></li>
</ul>
</nav>
<h1 data-number="1" id="classification"><span class="header-section-number">1</span> Classification</h1>
<p>The relevant handling guidelines are bolded.</p>
<p><strong>Classification: CONFIDENTIAL</strong></p>
<ul>
<li><strong>Do not forward or copy data in part or full without explicit permission of the INTIENT Clinical Product Owner</strong></li>
<li><strong>Data access is limited to INTIENT Clinical Project Team</strong></li>
<li>Use Strong authentication / EFS Encryption / Lock in a Drawer</li>
<li>Log access in a register</li>
<li>Retention period is ——– ( default as per Policy 123)</li>
</ul>
<h1 data-number="2" id="purpose-and-scope"><span class="header-section-number">2</span> Purpose and Scope</h1>
<p>The purpose of the Use Case Document is to describe the INTIENT Clinical product, main use cases and references to individual module use cases. This document is organized by clinical use cases (e.g. Prepare Study Submission Package), with corresponding product modules (e.g. Study Data Engine) that support those use cases.<br />
The features detail definition of basic flows, alternative flows (where applicable) and traceability to feature files where the detail requirements and test scenarios are described.</p>
<h1 data-number="3" id="module-description"><span class="header-section-number">3</span> Module Description</h1>
<p>Clinical Core supports both Clinical Solutions with common features and functions.  Use cases supported by this module:  Login, Ingest Data, Register Study, Transform Data.</p>
<h1 data-number="4" id="references"><span class="header-section-number">4</span> References</h1>
<ol type="1">
<li><a href="definitions-and-acronyms.html">Definitions and Acronyms</a></li>
<li>INTIENT Clinical Validation Master Plan V2.0</li>
</ol>
<h1 data-number="5" id="product-overview"><span class="header-section-number">5</span> Product Overview</h1>
<p>INTIENT Clinical provides solutions to run better trials by enabling efficient aggregation of data, performing analysis and driving study and operational insights.</p>
<ul>
<li><strong>Study Data Solution:</strong> Solution includes study evidence repository; automated data management processes, SDTM conformance, and ADaM/TFL production, and managed analytics environment.</li>
<li><strong>Clinical Insights Solution:</strong> Solution includes visualization of real time data.</li>
<li><strong>Operational Insights Solution:</strong> Solution includes an operational evidence repository, a clinical control tower and integrated issue management.</li>
</ul>
<h1 data-number="6" id="use-cases"><span class="header-section-number">6</span> Use Cases</h1>
<h2 data-number="6.1" id="access-intient-clinical"><span class="header-section-number">6.1</span> Access INTIENT Clinical</h2>
<p>Features are available in INTIENT Clinical based on configurable modules in the environment. For example, if the environment has been configured to include the Control Tower module, then the Monitor Study feature will be available.</p>
<h3 data-number="6.1.1" id="base-flow---login-and-access-successfully"><span class="header-section-number">6.1.1</span> Base Flow - Login and Access Successfully</h3>
<ol type="1">
<li>User accesses INTIENT Clinical URL</li>
<li>System recognizes user using single sign on (SSO) infrastructure and authenticates user</li>
<li>The following use cases are accessed based on the assigned user role:
<ul>
<li>The Study Administrator, and Project Lead access <a href="#register-study">Register Study</a> - available with the Clinical Core Module.</li>
<li>Prepare Study Submission Package for SDTM using:
<ul>
<li>The Study Data Analyst accesses the <a href="study-data-engine-use-cases.html">Study Data Engine</a> to Produce SDTM using it’s configurable and automated set of features.</li>
<li>The Clinical Programmer accesses the <a href="mae-use-cases.html">Managed Analytics Environment</a> to Produce SDTM using it’s custom programming environment.</li>
</ul></li>
<li>Study Data Analyst, Clinical Programmer and Integration/Operations Data Analyst can <a href="#ingest-data">Ingest Data</a> - available with the Clinical Core Module.</li>
<li>The DMW DataBridge Admin can access the <a href="DMW-DataBridge-use-cases.html">DMW DataBridge</a></li>
<li>The System Administrator can Archive Data <a href="#archive-data">Archive Data</a> - available with the Clinical Core Module.</li>
<li>The Operations Data Analyst accesses <a href="#clinical-insights">Clinical Insights</a>.</li>
<li>The Project Lead accesses <a href="control-tower-use-cases.html">Operational Insights</a> - available with the Control Tower Module.</li>
<li>The Study Administrator and Clinical Programmer access <a href="#administer-access">Administer Access</a> - available with the Clinical Core Module.</li>
<li>The System Administrator can access all of the above</li>
</ul></li>
<li>User selects a feature, completes that use case, and when done repeats step 3 as needed</li>
</ol>
<h3 data-number="6.1.2" id="alternatives"><span class="header-section-number">6.1.2</span> Alternatives</h3>
<h4 data-number="6.1.2.1" id="a---login-failure"><span class="header-section-number">6.1.2.1</span> 2a - Login Failure</h4>
<ol type="1">
<li>System does not recognize user and blocks access.</li>
</ol>
<h4 data-number="6.1.2.2" id="b---login-role-restriction"><span class="header-section-number">6.1.2.2</span> 2b - Login Role Restriction</h4>
<ol type="1">
<li>System prevents access to users with no role assignment.</li>
</ol>
<h3 data-number="6.1.3" id="feature-details"><span class="header-section-number">6.1.3</span> Feature Details</h3>
<h4 data-number="6.1.3.1" id="access-system"><span class="header-section-number">6.1.3.1</span> Access System</h4>
<h5 data-number="6.1.3.1.1" id="base-flow-scenarios-for-login-and-access-successfully"><span class="header-section-number">6.1.3.1.1</span> Base Flow Scenarios for: Login and Access Successfully</h5>
<pre><code>Scenario: Integrate with Okta to provide authenticated access to a web page
  Given A user enters our product url in a browser
  When they navigate to the url
  Then they are prompted to enter credentials</code></pre>
<pre><code>Scenario: Provide access to the landing page from the root directory
  Given A user is on the login page to access
  And a user enters valid login credentials
  When they submit
  Then they are redirected to the landing page after successful login</code></pre>
<pre><code>Scenario: Redirect on login
  Given A user is redirected to the login page when attempting to access a web page
  And a user enters valid login credentials
  When they submit
  Then they are redirected to the web page they were originally trying to access</code></pre>
<pre><code>Scenario: Redirect from login when already logged in
  Given A user is already logged in to the site
  When they navigate to the login screen
  Then they are redirected to the landing page</code></pre>
<h5 data-number="6.1.3.1.2" id="alternative-scenarios-for-login-failure"><span class="header-section-number">6.1.3.1.2</span> Alternative Scenarios for: Login Failure</h5>
<pre><code>Scenario: Provide ability to retry when entering invalid credentials
  Given A user enters invalid credentials
  When they submit
  Then they are prompted to retry</code></pre>
<pre><code>Scenario: Prevent access to unauthorized users
  Given A user has not logged in to the web page
  When they try to access the upload page
  Then they are redirected to the login page</code></pre>
<pre><code>Scenario Outline: Prevent access to unauthorized users
  Given A user has not logged in to the web page
  When they try to access the &lt;Page&gt; page
  Then they are redirected to the login page
  Examples:
  | Page          |
  | Home          |
  | Upload SDTM   |
  | Study Details |</code></pre>
<h5 data-number="6.1.3.1.3" id="alternative-scenarios-for-login-role-restriction"><span class="header-section-number">6.1.3.1.3</span> Alternative Scenarios for: Login Role Restriction</h5>
<pre><code>Scenario: Prevent access to users with no role assignment
  Given A user enters valid credentials
  When they try to access the system
  And they have no role assigned
  Then they are redirected to the login page
  And a message that the are not authorized to use the application is displayed</code></pre>
<h2 data-number="6.2" id="ingest-data"><span class="header-section-number">6.2</span> Ingest Data</h2>
<p>The Ingest Data use case is available with the Clinical Core Module. This use case covers data ingestion for both Operational and Study Data.</p>
<h3 data-number="6.2.1" id="base-flow---ingest-data"><span class="header-section-number">6.2.1</span> Base Flow - Ingest Data</h3>
<ol type="1">
<li>User navigates to the Ingest Data feature</li>
<li>User <a href="#create-data-provider">Creates Data Provider</a></li>
<li>User <a href="#load-data">Loads Data</a></li>
<li>User <a href="#view-data-and-data-profile">Views Data and Data Profiles</a></li>
<li>User <a href="#browse-query-and-snapshot-data">Browses, Queries and Snapshots Data</a></li>
</ol>
<p>The following are the user roles that have access to this use case:</p>
<ul>
<li>System Administrator</li>
<li>Study Data Analyst</li>
<li>Clinical Programmer</li>
<li>Integration Analyst</li>
<li>Operations Data Analyst</li>
</ul>
<h3 data-number="6.2.2" id="feature-details-1"><span class="header-section-number">6.2.2</span> Feature Details</h3>
<h4 data-number="6.2.2.1" id="ingest-data-1"><span class="header-section-number">6.2.2.1</span> Ingest Data</h4>
<h5 data-number="6.2.2.1.1" id="base-flow-scenarios-for-ingest-data"><span class="header-section-number">6.2.2.1.1</span> Base Flow Scenarios for: Ingest Data</h5>
<pre><code>Scenario: Covered by sub-usecases</code></pre>
<h2 data-number="6.3" id="create-data-provider"><span class="header-section-number">6.3</span> Create Data Provider</h2>
<h3 data-number="6.3.1" id="base-flow---user-creates-data-provider"><span class="header-section-number">6.3.1</span> Base Flow - User Creates Data Provider</h3>
<ol type="1">
<li>From the Ingest Data feature, user navigates to Add New</li>
<li>User enters the Provider Name and the Data Category for Operational and Study</li>
<li>System notifies the user of successful Data Provider creation</li>
<li>User can view Data Quality dashboard</li>
</ol>
<h3 data-number="6.3.2" id="alternatives-1"><span class="header-section-number">6.3.2</span> Alternatives</h3>
<h4 data-number="6.3.2.1" id="a---data-provider-for-operational-data-load-only"><span class="header-section-number">6.3.2.1</span> 2a - Data Provider for Operational data load only</h4>
<ol type="1">
<li>User Creates a new Data Provider by entering the Provider Name and the Data Category for Operational</li>
</ol>
<h4 data-number="6.3.2.2" id="b---data-provider-for-study-data-load-only"><span class="header-section-number">6.3.2.2</span> 2b - Data Provider for Study data load only</h4>
<ol type="1">
<li>User Creates a new Data Provider by entering the Provider Name and the Data Category for Study</li>
</ol>
<h4 data-number="6.3.2.3" id="a---data-provider-already-exists"><span class="header-section-number">6.3.2.3</span> 3a - Data Provider already exists</h4>
<ol type="1">
<li>User attempts to create a new data provider when that data provider has already been configured</li>
<li>Create data provider action is canceled, and the user is notified that a provider already exists</li>
</ol>
<h3 data-number="6.3.3" id="feature-details-2"><span class="header-section-number">6.3.3</span> Feature Details</h3>
<h4 data-number="6.3.3.1" id="create-data-provider-1"><span class="header-section-number">6.3.3.1</span> Create Data Provider</h4>
<h5 data-number="6.3.3.1.1" id="base-flow-scenarios-for-user-creates-data-provider"><span class="header-section-number">6.3.3.1.1</span> Base Flow Scenarios for: User Creates Data Provider</h5>
<pre><code>Scenario: View data ingestion screen
  Given the user is on the Data Ingestion screen
  When the page is shown
  Then a list of configured data providers is displayed in a table
  And the columns are labeled Data Provider, Data Category, Actions
  And the user can select &#39;Provider Details&#39; for a given provider
  And the user can select &#39;Add Data Provider&#39;
  # And the user can select &#39;Create New Connection&#39; for a given provider</code></pre>
<pre><code>Scenario: Open add data provider form
  Given a user is on the Ingest Data screen
  When the user selects the &#39;Register Provider&#39; button
  Then the system displays the data provider registration form
  And the form displays the Data Provider Name, Data Category
  And the Data Provider Name is a free text field
  And the Data Category is selectable
  And the values for Data Category are Archive, Registration, Operational and Study
  And multiple Data Categories can be selected unless Archive is selected</code></pre>
<pre><code>Scenario: Add data provider
  Given the user has opened the Data Provider registration form
  When the user fills in values for Data Provider Name and Data Category
  And the user selects the &#39;Create&#39; button
  Then the new provider is created
  And the new provider is displayed in the Data Provider table</code></pre>
<pre><code>Scenario: Navigate to data provider details
  Given the user is on the Data Ingestion screen
  When the user selects &#39;Provider Details&#39; in the Actions column for an individual provider
  Then the user is brought to the Data Provider details page</code></pre>
<pre><code>Scenario: View data provider details
  Given the user has navigated to the Data Provider details page
  When the page is shown
  Then the Data Provider name is displayed
  And the user has an option to select the data category
  And the data category of Operational is selected by default
  # And if only one data category is included for a provider that should be defaulted.</code></pre>
<pre><code>Scenario: View data provider details for operational data category
  Given the user has navigated to the Data Provider details page
  When the page is shown
  And the data category of operational is selected
  Then a list of operational data tables is displayed for the selected data provider
  And the user can select a single table
  And the user can select &#39;Load data&#39;</code></pre>
<pre><code>Scenario: View data provider details for study data category
  Given a Data Provider exists with Data Category of Study and a study is registered
  When the user searches and selects a specific study
  Then a list of tables is displayed for the selected study
  And the user can view a single table at a time
  And load data is enabled</code></pre>
<pre><code>Scenario: Search for a registered study within a data provider
  Given a Data Provider exists with Data Category of Study
  When the data provider is selected
  And the user selects the data category of study
  Then the user can search for a specific study</code></pre>
<pre><code>Scenario: View list of tables for registered study within a data provider
  # preload exists
  Given a Data Provider exists with Data Category of Study and tables have been loaded for a study
  When the data provider is selected
  And the user selects the data category of study
  And the user selects that specific study
  Then the system displays a list of tables for that specific study</code></pre>
<pre><code>Scenario: View registered study with no tables within a data provider
  Given a Data Provider exists with Data Category of Study and no tables have been loaded for a study
  When the data provider is selected
  And the user selects the data category of study
  And the user selects that specific no-data study
  Then the system indicates to user no tables have been loaded for that specific study</code></pre>
<pre><code>Scenario: Disallow data provider registration if required metadata is not specified
  Given a user has navigated to the data provider registration form
  When a required metadata field is not specified by the user
  Then the system does not allow the user to submit the form</code></pre>
<h5 data-number="6.3.3.1.2" id="alternative-scenarios-for-data-provider-for-operational-data-load-only"><span class="header-section-number">6.3.3.1.2</span> Alternative Scenarios for: Data Provider for Operational data load only</h5>
<pre><code>Scenario: Covered by Base Flow use cases</code></pre>
<h5 data-number="6.3.3.1.3" id="alternative-scenarios-for-data-provider-for-study-data-load-only"><span class="header-section-number">6.3.3.1.3</span> Alternative Scenarios for: Data Provider for Study data load only</h5>
<pre><code>Scenario: Covered by Base Flow use cases</code></pre>
<h5 data-number="6.3.3.1.4" id="alternative-scenarios-for-data-provider-already-exists"><span class="header-section-number">6.3.3.1.4</span> Alternative Scenarios for: Data Provider already exists</h5>
<pre><code>Scenario: Covered by Base Flow use cases</code></pre>
<h2 data-number="6.4" id="load-data"><span class="header-section-number">6.4</span> Load Data</h2>
<p>For Study data loads, a Study must be registered first before data can be loaded. See steps for <a href="#Study-Registration">Study Registration</a>.</p>
<h3 data-number="6.4.1" id="base-flow---load-data"><span class="header-section-number">6.4.1</span> Base Flow - Load Data</h3>
<ol type="1">
<li>User navigates to the Ingest Data feature</li>
<li>User selects a Data Provider and views its details</li>
<li>User selects Load Data for Operational</li>
<li>User selects selects a file to load</li>
<li>System notifies user of successful initiation of data load</li>
<li>System completes data load and notifies users of successful data load</li>
<li>System presents the data tables loaded with the following features:
<ul>
<li>Summary: high level information on the table such as table name, size</li>
<li>Metadata: information on table variables and type</li>
<li>Preview and Profile: see <a href="#view-data-and-data-profile">View Data and Data Profile</a></li>
</ul></li>
</ol>
<h3 data-number="6.4.2" id="alternatives-2"><span class="header-section-number">6.4.2</span> Alternatives</h3>
<h4 data-number="6.4.2.1" id="a---user-lacks-permissions"><span class="header-section-number">6.4.2.1</span> 1a - User lacks permissions</h4>
<ol type="1">
<li>User does not have permission to ingest data</li>
</ol>
<h4 data-number="6.4.2.2" id="b---data-load-via-gcs"><span class="header-section-number">6.4.2.2</span> 1b - Data Load via GCS</h4>
<ol type="1">
<li>System encounters data files posted on a Google Cloud Storage bucket for a data provider</li>
<li>System initiates data load into the repository</li>
<li>System completes data load and notifies users of success or failure</li>
<li>User selects <a href="#view-data-and-data-profile">View Data and Data Profile</a> to view information about the data loaded</li>
</ol>
<h4 data-number="6.4.2.3" id="c---data-load-via-sftp"><span class="header-section-number">6.4.2.3</span> 1c - Data Load via sFTP</h4>
<ol type="1">
<li>System encounters data files posted to sFTP</li>
<li>System copies data files to Google Cloud Storage for a data provider</li>
<li>System initiates data load into the repository</li>
<li>System completes data load and notifies users of success or failure</li>
<li>User selects <a href="#view-data-and-data-profile">View Data and Data Profile</a> to view information about the data loaded</li>
</ol>
<h4 data-number="6.4.2.4" id="a---data-load-for-study"><span class="header-section-number">6.4.2.4</span> 3a - Data Load for Study</h4>
<ol type="1">
<li>User selects Study data</li>
<li>Continue at step 4</li>
</ol>
<h4 data-number="6.4.2.5" id="a---data-load-failed"><span class="header-section-number">6.4.2.5</span> 6a - Data Load Failed</h4>
<ol type="1">
<li>System completes the data load process and the process is terminated</li>
<li>System notifies user of data load failure</li>
</ol>
<h3 data-number="6.4.3" id="feature-details-3"><span class="header-section-number">6.4.3</span> Feature Details</h3>
<h4 data-number="6.4.3.1" id="load-data-1"><span class="header-section-number">6.4.3.1</span> Load Data</h4>
<h5 data-number="6.4.3.1.1" id="base-flow-scenarios-for-load-data"><span class="header-section-number">6.4.3.1.1</span> Base Flow Scenarios for: Load Data</h5>
<pre><code>Scenario: Ingest DMW study data and store metadata from all source files into BQ
  Given more than one data files from study MNO in a directory
  And each data file has a corresponding _metadata file
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And any non alphanumeric character is replaced with an &quot;_&quot; in the schemas
  And the tables have the same contents as the corresponding source file
  And a metadata table is created with the study name and &quot;_metadata&quot;
  And the metadata table has the same metadata contents per source file</code></pre>
<pre><code>Scenario: Display load data for operational data
  Given the user has navigated to the Data Provider details page
  When the page is shown
  And the data category of operational is selected
  And the user selects the &#39;Load data&#39; button
  Then a local file browser is displayed</code></pre>
<pre><code>Scenario: Load csv data file
  Given the user has navigated to the local file browser
  When the user selects a csv file to load
  And the user selects &#39;Load file&#39;
  Then the file is loaded to the system</code></pre>
<pre><code>Scenario: Load data file
  # Req: User has navigated to the Data provider details page and the local file browser is displayed
  # Req: No file type should be set as default
  Given the user wants to load a data file
  When the user selects a file to load
  And the file type is csv, sas7bdat, xlsx, xls, xport
  Then the file is loaded to the system
  And the system notifies user of success</code></pre>
<pre><code>Scenario: Load data file type not supported
  Given the user wants to load a data file
  When the picker window is displayed
  Then only the supported file types csv, sas7bdat, xlsx, xls, xpt are available</code></pre>
<pre><code>Scenario: Ingest study files from SAS7BDAT into BigQuery tables and generate profile report
  Given more than one sas7bdat files from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And a Great Expectations HTML profile report is created
  And the profile report is stored in a Profile subfolder under the study&#39;s Source folder</code></pre>
<pre><code>Scenario: Ingest study files from CSV into BigQuery tables and generate profile report
  Given more than one csv files from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And a Great Expectations HTML profile report is created
  And the profile report is stored in a Profile subfolder under the study&#39;s Source folder</code></pre>
<pre><code>Scenario: Ingest study tables from RDBMS into BigQuery tables and generate profile report
  Given multiple tables in a RDBMS from study XYZ
  When the tables are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the tables are loaded into BigQuery tables with the names matching the original names
  And the tables have the same schemas as the corresponding source table
  And the tables have the same contents as the corresponding source table
  And a Great Expectations HTML profile report is created
  And the profile report is stored in a Profile subfolder under the study&#39;s Source folder</code></pre>
<pre><code>Scenario: Ingest study files from SAS XPT into BigQuery tables and generate profile report
  Given more than one xpt files from study PDQ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And a Great Expectations HTML profile report is created
  And the profile report is stored in a Profile subfolder under the study&#39;s Source folder</code></pre>
<h5 data-number="6.4.3.1.2" id="base-flow-scenarios-for-ingest-operational-data-from-sftp"><span class="header-section-number">6.4.3.1.2</span> Base Flow Scenarios for: Ingest Operational Data from sftp</h5>
<pre><code>Scenario: Load operational files from data bucket into BigQuery with blanks and special characters in column names
  # Excel and csv files are present in data bucket and needs to be loaded to BigQuery operational dataset
  # All files must have matching schema and content
  # naming convention for BigQuery dataset - ops_ is fixed prefix, _source is fixed suffix, vendor1 is variable
  # files have column names with gaps and special characters
  # gaps - one or more - must be replaced by a single underscore,
  # special characters - one or more - must be replaced by a single underscore
  # if the last character is a special character then the character is deleted
  Given the source is Vendor1
  And the following operational xlsx,xls,csv files from the vendor are in the data bucket
  | filename                            |
  | client_clinical_study_milestone.csv |
  | client_clinical_study_subject.xlsx  |
  | client_clinical_study_site.xls      |
  And the following special characters are present in the column names of the files
  | special characters |
  | ,                  |
  | ;                  |
  | ~                  |
  | !                  |
  | #                  |
  | $                  |
  | %                  |
  | ^                  |
  | &amp;                  |
  | *                  |
  | (                  |
  | )                  |
  | =                  |
  | +                  |
  | [                  |
  | ]                  |
  | {                  |
  | }                  |
  | &lt;                  |
  | &gt;                  |
  | .                  |
  | blank space        |
  When the files are loaded
  Then the following tables are present in the BigQuery operational dataset OPS_VENDOR1_SOURCE
  | tablename                       |
  | CLIENT_CLINICAL_STUDY_MILESTONE |
  | CLIENT_CLINICAL_STUDY_SUBJECT   |
  | CLIENT_CLINICAL_STUDY_SITE      |
  And a SRC_SEQ column is added to the end of each table
  And the SRC_SEQ column contains the original row number of the corresponding source file
  And the tables in the BigQuery dataset has the same schema as the corresponding source file in the data bucket
  And the column names in the tables are in upper case
  And the special characters in the column names in the tables are replaced by one underscore
  And the column names in the tables with multiple special consecutive characters replaced by one underscore
  And the column names in the tables ending with special character is deleted
  And the column names in the tables starting with special character is deleted
  And the tables in the BigQuery dataset has the same content as the corresponding source file in the data bucket</code></pre>
<pre><code>Scenario Outline: Replace disallowed characters with underscores in identifiers
  Given an &lt;element&gt; with dashes in its name
  When the study is ingested
  Then the corresponding &lt;element&gt; name has underscores in place of dashes
  Examples:
  | element |
  | file    |
  | table   |
  | study   |
  | column  |</code></pre>
<pre><code>Scenario Outline: Name repair causes a conflict
  Given an &lt;element&gt; with dashes in its name
  And an &lt;element&gt; with the repaired name already exists and is being ingested at the same time
  When the study is ingested
  Then the ingestion should fail
  And an error should be reported
  Examples:
  | element |
  | file    |
  | table   |
  | column  |</code></pre>
<pre><code>Scenario: Ingest files from CSV containing special and non-printable characters from ASCII into BigQuery tables
  Given more than one csv file from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And the contents containing special and non-printable characters are represented the same as the corresponding source file</code></pre>
<pre><code>Scenario: Ingest files from SAS7BDAT containing special and non-printable characters from ASCII into BigQuery tables
  Given more than one sas7bdat file from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And the contents containing special and non-printable characters are represented the same as the corresponding source file</code></pre>
<pre><code>Scenario: Verify Persisted Data bucket has enabled versioning
  Given a Persisted Data bucket in an environment
  When the bucket is inspected
  Then the bucket has versioning enabled</code></pre>
<pre><code>Scenario: Verify Ingestion Staging bucket has enabled versioning
  Given an Ingestion Staging Data bucket in an environment
  When the bucket is inspected
  Then the bucket has versioning enabled</code></pre>
<h5 data-number="6.4.3.1.3" id="alternative-scenarios-for-data-load-via-gcs"><span class="header-section-number">6.4.3.1.3</span> Alternative Scenarios for: Data Load via GCS</h5>
<pre><code>Scenario: Create a string copy of OPS data and maintain an Original Copy
  Given the following operational files are in the data bucket folder
  | filename                        |
  | client!clinical&amp;study#milestone |
  When the files are loaded
  Then the following tables are present in the BigQuery operational dataset OPS_VENDOR1_SOURCE
  | tablename                            |
  | CLIENT_CLINICAL_STUDY_MILESTONE      |
  | CLIENT_CLINICAL_STUDY_MILESTONE_ORIG |
  And the tables in the BigQuery dataset has the same schema as the corresponding source file in the data bucket
  And the column names in the tables are in upper case
  And the tables in the BigQuery dataset has the same content as the corresponding source file in the data bucket</code></pre>
<pre><code>Scenario: Move initial operational CSV files from ingestion to data bucket for an existing vendor
  # Move one or more CSV files from ingestion to data bucket folder corresponding to the source/vendor
  # Special characters along with cases are retained in file names
  Given the following operational csv files are uploaded to the ingestion bucket intient-clinical-ingestion in the folder /clinical-operations/vendor1/Source/Data
  | filename                        |
  | client!clinical&amp;study#milestone |
  | client$clinical%study^subject   |
  | client;clinicalstudy;site       |
  | ClientAddress                   |
  | client,clinical.study           |
  | clientsite&#39;financials           |
  | client~site=milestone           |
  | client-country+milestone        |
  | client[subject]visit            |
  | client(study)documents          |
  | client{protocol}deviation       |
  When the files are copied
  Then the following files are present in the data bucket intient-clinical-data folder /clinical-operations/vendor1/Source/Data
  | filename                        |
  | client!clinical&amp;study#milestone |
  | client$clinical%study^subject   |
  | client;clinicalstudy;site       |
  | ClientAddress                   |
  | client,clinical.study           |
  | clientsite&#39;financials           |
  | client~site=milestone           |
  | client-country+milestone        |
  | client[subject]visit            |
  | client(study)documents          |
  | client{protocol}deviation       |
  And the operation file sizes in the data bucket should be the same file sizes as those in the ingestion bucket for each file
  And the operation files in ingestion have the same schema as the corresponding source file in the data bucket
  And the operation files in ingestion have the same contents as the corresponding source file in the data bucket
  And the operation files in the ingestion bucket are deleted</code></pre>
<pre><code>Scenario: Load initial operational CSV files from data bucket into tables in BigQuery operational dataset
  # CSV files that are present in data bucket needs to be loaded to BigQuery operational dataset
  # All files must have matching schema and copy 1 to 1
  # Align input files to meet GCP constraint - can only contain alphanumerics, underscores; other special characters such !@#$%^&amp;*(  are not allowed//
  # naming convention for BigQuery dataset - ops_ is fixed prefix, _source is fixed suffix, vendor1 is variable
  Given the following operational csv files are in the data bucket folder /clinical-operations/vendor1/Source/Data
  | filename                        |
  | client!clinical&amp;study#milestone |
  | client$clinical%study^subject   |
  | client;clinicalstudy;site       |
  | ClientAddress                   |
  | client,clinical.study           |
  | clientsite&#39;financials           |
  | client~site=milestone           |
  | client-country+milestone        |
  | client[subject]visit            |
  | client(study_documents          |
  | client{protocol}deviation       |
  When the files are loaded
  Then the following tables are present in the BigQuery operational dataset OPS_VENDOR1_SOURCE
  | tablename                       |
  | CLIENT_CLINICAL_STUDY_MILESTONE |
  | CLIENT_CLINICAL_STUDY_SUBJECT   |
  | CLIENT_CLINICALSTUDY_SITE       |
  | CLIENTADDRESS                   |
  | CLIENT_CLINICAL_STUDY           |
  | CLIENTSITE_FINANCIALS           |
  | CLIENT_SITE_MILESTONE           |
  | CLIENT_COUNTRY_MILESTONE        |
  | CLIENT_SUBJECT_VISIT            |
  | CLIENT_STUDY_DOCUMENTS          |
  | CLIENT_PROTOCOL_DEVIATION       |
  And the tables in the BigQuery dataset has the same schema as the corresponding source file in the data bucket
  And the column names in the tables are in upper case
  And the tables in the BigQuery dataset has the same content as the corresponding source file in the data bucket</code></pre>
<pre><code>Scenario: Move operational Excel xlsx files from ingestion to data bucket for an existing vendor
  # Move one or more Excel files from ingestion to data bucket folder corresponding to the source/vendor
  # only one tab is present in each Excel file
  # Special characters along with cases are retained in file names
  # Microsoft Excel xlsx format is retained
  Given the source is Vendor1
  And the following operational Excel xlsx files from the vendor are in the ingestion bucket
  | filename                             |
  | client_clinical_study_milestone.xlsx |
  | client_clinical_study_subject.xlsx   |
  And each Excel file contains one tab
  When the files are copied
  Then the following files are present in the data bucket allocated to Vendor1
  | client_clinical_study_milestone.xlsx |
  | client_clinical_study_subject.xlsx   |
  And the operation file sizes in the data bucket should be the same file sizes as those in the ingestion bucket for each file
  And the operation files in ingestion have the same schema as the corresponding source file in the data bucket
  And the operation files in ingestion have the same contents as the corresponding source file in the data bucket
  And the operation files in the ingestion bucket are deleted</code></pre>
<pre><code>Scenario: Move operational Excel xlsx files with multiple tabs from ingestion to data bucket for an existing vendor
  # Move an Excel file with multiple tabs to data bucket
  # Convert each tab to an Excel file with one tab
  # Special characters along with cases are retained in file names
  # Microsoft Excel xlsx format is retained
  Given the source is Vendor1
  And the following operational Excel xlsx file from the vendor are in the ingestion bucket
  | filename                | tabname         |
  | Ingestion_multitab.xlsx | (address)       |
  |                         | clinical_study  |
  |                         | study milestone |
  And the tab (address) contains
  | ADDRESS_ID | ADDRESS_SOURCE_ID | COUNTRY_NAME | POSTAL_CODE |
  | 201904001  | ADDCISRC101       | INDIA        | 400011      |
  | 201904002  | ADDCISRC102       | FIJI         | 6702        |
  | 201904003  | ADDCISRC103       | INDIA        | 600024      |
  And the tab clinical_study contains
  | drug_program_code | development_phase | methodology_study | partner_allocated | pediatric_study | post_market_surv |
  | ACI799            | PHASE II          | N                 | CHISQ             | N               | N                |
  And the tab study milestone contains
  | study_milestone_id | study_milestone_seq_num | study_id     | study_milestone_actual_dt | study_milestone_base_dt   | study_milestone_planned_dt |
  | DB_APP_ST          | 1                       | STDCITEST002 | 2018-03-13 00:00:00+05:30 | 2018-03-14 00:00:00+05:30 | 2018-03-04 00:00:00+05:30  |
  | 50_SUB_EN_ST       | 2                       | STDCITEST002 | 2017-12-14 00:00:00+05:30 | 2017-12-17 00:00:00+05:30 | 2017-12-06 00:00:00+05:30  |
  | 50_SUB_TR_ST       | 3                       | STDCITEST002 | 2018-10-29 00:00:00+05:30 | 2018-10-30 00:00:00+05:30 | 2018-09-29 00:00:00+05:30  |
  | INT_APP_INT_REP_ST | 251                     | STDCITEST002 | 2018-11-26 00:00:00+05:30 | 2018-11-28 00:00:00+05:30 | 2018-10-26 00:00:00+05:30  |
  When the files are copied
  Then the following files are present in the data bucket allocated to Vendor1
  | filename                |
  | Ingestion_multitab.xlsx |
  | (address).xlsx          |
  | clinical_study.xlsx     |
  | study milestone.xlsx    |
  And the tabs in ingestion have the same schema as the corresponding Excel file in the data bucket
  And the operation files in ingestion have the same contents as the corresponding source file in the data bucket
  And the operation files in the ingestion bucket are deleted
  And the following tables are present in the BigQuery operational dataset OPS_VENDOR1_SOURCE
  | tablename       |
  | ADDRESS         |
  | CLINICAL_STUDY  |
  | STUDY_MILESTONE |
  And a SRC_SEQ column is added to the end of each table
  And the SRC_SEQ column contains the original row number of the corresponding source file
  And the tables in the BigQuery dataset has the same schema as the corresponding source file in the data bucket
  And the column names in the tables are in upper case
  And the tables in the BigQuery dataset has the same content as the corresponding source file in the data bucket</code></pre>
<pre><code>Scenario: Move operational Excel xls files with multiple tabs from ingestion to data bucket for an existing vendor
  # Move an Excel file with multiple tabs to data bucket
  # Convert each tab to an Excel file with one tab
  # Special characters along with cases are retained in file names
  # Microsoft Excel xls format is retained
  Given the source is Vendor1
  And the following operational Excel xls file from the vendor are in the ingestion bucket
  | filename               | tabname         |
  | Ingestion_multitab.xls | (address)       |
  |                        | clinical_study  |
  |                        | study milestone |
  And the tab (address) contains
  | ADDRESS_ID | ADDRESS_SOURCE_ID | COUNTRY_NAME | POSTAL_CODE |
  | 201904001  | ADDCISRC101       | INDIA        | 400011      |
  | 201904002  | ADDCISRC102       | FIJI         | 6702        |
  | 201904003  | ADDCISRC103       | INDIA        | 600024      |
  And the tab clinical_study contains
  | drug_program_code | development_phase | methodology_study | partner_allocated | pediatric_study | post_market_surv |
  | ACI799            | PHASE II          | N                 | CHISQ             | N               | N                |
  And the tab study milestone contains
  | study_milestone_id | study_milestone_seq_num | study_id     | study_milestone_actual_dt | study_milestone_base_dt   | study_milestone_planned_dt |
  | DB_APP_ST          | 1                       | STDCITEST002 | 2018-03-13 00:00:00+05:30 | 2018-03-14 00:00:00+05:30 | 2018-03-04 00:00:00+05:30  |
  | 50_SUB_EN_ST       | 2                       | STDCITEST002 | 2017-12-14 00:00:00+05:30 | 2017-12-17 00:00:00+05:30 | 2017-12-06 00:00:00+05:30  |
  | 50_SUB_TR_ST       | 3                       | STDCITEST002 | 2018-10-29 00:00:00+05:30 | 2018-10-30 00:00:00+05:30 | 2018-09-29 00:00:00+05:30  |
  | INT_APP_INT_REP_ST | 251                     | STDCITEST002 | 2018-11-26 00:00:00+05:30 | 2018-11-28 00:00:00+05:30 | 2018-10-26 00:00:00+05:30  |
  When the files are copied
  Then the following files are present in the data bucket allocated to Vendor1
  | filename               |
  | Ingestion_multitab.xls |
  | (address).xls          |
  | clinical_study.xls     |
  | study milestone.xls    |
  And the tabs in ingestion have the same schema as the corresponding Excel file in the data bucket
  And the operation files in ingestion have the same contents as the corresponding source file in the data bucket
  And the operation files in the ingestion bucket are deleted
  And the following tables are present in the BigQuery operational dataset OPS_VENDOR1_SOURCE
  | tablename       |
  | ADDRESS         |
  | CLINICAL_STUDY  |
  | STUDY_MILESTONE |
  And a SRC_SEQ column is added to the end of each table
  And the SRC_SEQ column contains the original row number of the corresponding source file
  And the tables in the BigQuery dataset has the same schema as the corresponding source file in the data bucket
  And the column names in the tables are in upper case
  And the tables in the BigQuery dataset has the same content as the corresponding source file in the data bucket</code></pre>
<pre><code>Scenario: Load operational Excel xlsx files from data bucket into BigQuery
  # Excel files that are present in data bucket needs to be loaded to BigQuery operational dataset
  # All files must have matching schema and content
  # naming convention for BigQuery dataset - ops_ is fixed prefix, _source is fixed suffix, vendor1 is variable
  Given the source is Vendor1
  And the following operational Excel xlsx files from the vendor are in the data bucket
  | filename                             |
  | client_clinical_study_milestone.xlsx |
  | client_clinical_study_subject.xlsx   |
  And each Excel file contains one tab
  When the files are loaded
  Then the following tables are present in the BigQuery operational dataset OPS_VENDOR1_SOURCE
  | tablename                       |
  | CLIENT_CLINICAL_STUDY_MILESTONE |
  | CLIENT_CLINICAL_STUDY_SUBJECT   |
  And a SRC_SEQ column is added to the end of each table
  And the SRC_SEQ column contains the original row number of the corresponding source file
  And the tables in the BigQuery dataset has the same schema as the corresponding source file in the data bucket
  And the column names in the tables are in upper case
  And the tables in the BigQuery dataset has the same content as the corresponding source file in the data bucket</code></pre>
<pre><code>Scenario: Move operational Excel xls files from ingestion to data bucket from an existing vendor
  # Move one or more Excel files from ingestion to data bucket folder corresponding to the source/vendor
  # only one tab is present in each Excel file
  # Special characters along with cases are retained in file names
  # Microsoft Excel xls format is retained
  Given the source is Vendor1
  And the following operational Excel xls files from the vendor are in the ingestion bucket
  | filename                            |
  | client_clinical_study_milestone.xls |
  | client_clinical_study_subject.xls   |
  And each Excel file contains one tab
  When the files are copied
  Then the following files are present in the data bucket allocated to Vendor1
  | client_clinical_study_milestone.xls |
  | client_clinical_study_subject.xls   |
  And the operation file sizes in the data bucket should be the same file sizes as those in the ingestion bucket for each file
  And the operation files in ingestion have the same schema as the corresponding source file in the data bucket
  And the operation files in ingestion have the same contents as the corresponding source file in the data bucket
  And the operation files in the ingestion bucket are deleted</code></pre>
<pre><code>Scenario: Load operational Excel xls files from data bucket into BigQuery
  # Excel files that are present in data bucket needs to be loaded to BigQuery operational dataset
  # All files must have matching schema and content
  # naming convention for BigQuery dataset - ops_ is fixed prefix, _source is fixed suffix, vendor1 is variable
  Given the source is Vendor1
  And the following operational Excel xls files from the vendor are in the data bucket
  | filename                            |
  | client_clinical_study_milestone.xls |
  | client_clinical_study_subject.xls   |
  And each Excel file contains only one tab
  When the files are loaded
  Then the following tables are present in the BigQuery operational dataset OPS_VENDOR1_SOURCE
  | tablename                       |
  | CLIENT_CLINICAL_STUDY_MILESTONE |
  | CLIENT_CLINICAL_STUDY_SUBJECT   |
  And a SRC_SEQ column is added to the end of each table
  And the SRC_SEQ column contains the original row number of the corresponding source file
  And the tables in the BigQuery dataset has the same schema as the corresponding source file in the data bucket
  And the column names in the tables are in upper case
  And the tables in the BigQuery dataset has the same content as the corresponding source file in the data bucket</code></pre>
<pre><code>Scenario: Load operational Excel xlsx xls csv format files from data bucket into BigQuery
  # Excel and csv files are present in data bucket needs to be loaded to BigQuery operational dataset
  # All files must have matching schema and content
  # naming convention for BigQuery dataset - ops_ is fixed prefix, _source is fixed suffix, vendor1 is variable
  Given the source is Vendor1
  And the following operational xlsx,xls,csv files from the vendor are in the data bucket
  | filename                            |
  | client_clinical_study_milestone.csv |
  | client_clinical_study_subject.xlsx  |
  | client_clinical_study_site.xls      |
  And each Excel file contains only one tab
  When the files are loaded
  Then the following tables are present in the BigQuery operational dataset OPS_VENDOR1_SOURCE
  | tablename                       |
  | CLIENT_CLINICAL_STUDY_MILESTONE |
  | CLIENT_CLINICAL_STUDY_SUBJECT   |
  | CLIENT_CLINICAL_STUDY_SITE      |
  And a SRC_SEQ column is added to the end of each table
  And the SRC_SEQ column contains the original row number of the corresponding source file
  And the tables in the BigQuery dataset has the same schema as the corresponding source file in the data bucket
  And the column names in the tables are in upper case
  And the tables in the BigQuery dataset has the same content as the corresponding source file in the data bucket
  # And the operation file sizes in the data bucket should be the same file sizes as those in the ingestion bucket for each file</code></pre>
<pre><code>Scenario: Move a zip file containing from an ingestion bucket to data bucket and unzip
  # when moving data to BigQuery, zip file is not moved from data bucket to BigQuery
  Given the source is Vendor1
  And zipped file ops_source_date.zip from Vendor1 contains the following files
  | filename                            |
  | client_clinical_study_milestone.csv |
  | client_clinical_study_subject.xlsx  |
  | client_clinical_study_site.xls      |
  When the zip file is ingested
  Then the following files are present in the data bucket allocated to Vendor1
  | filename                            |
  | ops_source_date.zip                 |
  | client_clinical_study_milestone.csv |
  | client_clinical_study_subject.xlsx  |
  | client_clinical_study_site.xls      |
  And the file names in zipped file are retained
  And the file types are retained
  And the operation files in the ingestion bucket are deleted
  And the following tables are present in the BigQuery operational dataset OPS_VENDOR1_SOURCE
  | tablename                       |
  | CLIENT_CLINICAL_STUDY_MILESTONE |
  | CLIENT_CLINICAL_STUDY_SUBJECT   |
  | CLIENT_CLINICAL_STUDY_SITE      |
  And a SRC_SEQ column is added to the end of each table
  And the SRC_SEQ column contains the original row number of the corresponding source file
  And the tables in the BigQuery dataset has the same schema as the corresponding source file in the data bucket
  And the column names in the tables are in upper case
  And the tables in the BigQuery dataset has the same content as the corresponding source file in the data bucket</code></pre>
<pre><code>Scenario: Move a 7z file containing operational data from an ingestion bucket to data bucket and unzip the files
  # when moving data to BigQuery, zip file is not moved from data bucket to BigQuery
  Given the source is Vendor1
  And zipped file ops_source_date.7z from Vendor1 contains the following files
  | filename                            |
  | client_clinical_study_milestone.csv |
  | client_clinical_study_subject.xlsx  |
  | client_clinical_study_site.xls      |
  When the zip file is ingested
  Then the following files are present in the data bucket allocated to Vendor1
  | filename                            |
  | ops_source_date.7z                  |
  | client_clinical_study_milestone.csv |
  | client_clinical_study_subject.xlsx  |
  | client_clinical_study_site.xls      |
  And the file names in zipped file are retained
  And the file types are retained
  And the operation files in the ingestion bucket are deleted
  And the following tables are present in the BigQuery operational dataset OPS_VENDOR1_SOURCE
  | tablename                       |
  | CLIENT_CLINICAL_STUDY_MILESTONE |
  | CLIENT_CLINICAL_STUDY_SUBJECT   |
  | CLIENT_CLINICAL_STUDY_SITE      |
  And a SRC_SEQ column is added to the end of each table
  And the SRC_SEQ column contains the original row number of the corresponding source file
  And the tables in the BigQuery dataset has the same schema as the corresponding source file in the data bucket
  And the column names in the tables are in upper case
  And the tables in the BigQuery dataset has the same content as the corresponding source file in the data bucket</code></pre>
<pre><code>Scenario: Automatically ingest operational data when the process is triggered
  Given the file test.csv is present in ingestion bucket /clinical-operations/data_provider1/Source/Data
  And the data provider DATA_PROVIDER1 is present
  When the ingestion process is triggered
  Then the following information is present
  | category    | data_provider_name | study_name | connection_type | filename | metadata_id |
  | Operational | data_provider1     |            | Auto            | test.csv | &lt;GCS ID&gt;    |
  And the following data connection details are present
  # connection will be created if it does not exist based on data connection name
  | data_connection_name            | data_connection_description                         | data_provider_key    | data_category | study_key | data_connection_type |
  | auto_operational_DATA_PROVIDER1 | auto operational data connection for DATA_PROVIDER1 | &lt;data_provider1 key&gt; | Operational   |           | Auto                 |
  And the following data file details are present
  | data_file_key | file_name | file_ingest_timestamp | data_connection_key                   | ingestion_status | error_message | profile_path                                                |
  | &lt;metadata id&gt; | test.csv  | &lt;current_timestamp&gt;   | &lt;auto_operational_data_provider1 key&gt; | Success          |               | clinical-operations/data_provider1/Source/Profile/TEST.html |</code></pre>
<pre><code>Scenario: Data loaded via SFTP is pushed into Ingestion bucket
  Given a valid HIPAA vault configuration has been established
  And a valid .xls file are present
  When the files are loaded via a SFTP server
  And the files are present in ingestion bucket
  Then Ingestion is successfully completed for the files
  # @prepared
  # @S3047
  # @api
  # Scenario: Auto ingestion is triggered when Config file has all source tables mapped
  # Given the files there are valid csv files
  # | Filename                      |
  # | CLIENT_CLINICAL_STUDY_API.csv |
  # | CLIENT_CLINICAL_SITE_API.csv  |
  # And a valid excel operations config name of OPS_COMBINED with a DV Mappings tab of
  # | Source Table              | Source Variable | Instruction        | Target Table | Target Variable | Comments |
  # | CLIENT_CLINICAL_STUDY_API |                 | Hub(STUDY_ID)      | HUB_STUDY    |                 |          |
  # | CLIENT_CLINICAL_SITE_API  |                 | Hub(STUDY_SITE_ID) | HUB_SITE     |                 |          |
  # When the files are pushed into the ingestion bucket
  # Then the auto ingestion is triggered
  # @prepared
  # @S3048
  # @api
  # Scenario: Auto ingestion is not triggered when Config file does not have all source tables mapped
  # Given the files there are valid csv files
  # | Filename                      |
  # | CLIENT_CLINICAL_STUDY_API.csv |
  # | CLIENT_CLINICAL_SITE_API.csv  |
  # And a valid excel operations config name of OPS_COMBINED with a DV Mappings tab of
  # | Source Table              | Source Variable | Instruction   | Target Table | Target Variable | Comments |
  # | CLIENT_CLINICAL_STUDY_API |                 | Hub(STUDY_ID) | HUB_STUDY    |                 |          |
  # When the files are pushed into the ingestion bucket
  # Then the auto ingestion is not triggered
  # And the process is killed
  # @Completed
  # @S3049
  # @api
  # Scenario: Auto ingestion is triggered only for tables that are mapped in config table
  # Given the files there are valid csv files
  # | Filename                      |
  # | CLIENT_CLINICAL_STUDY_API.csv |
  # | CLIENT_CLINICAL_SITE_API.csv  |
  # | test.csv                      |
  # And a valid excel operations config name of OPS_COMBINED with a DV Mappings tab of
  # | Source Table              | Source Variable | Instruction        | Target Table | Target Variable | Comments |
  # | CLIENT_CLINICAL_STUDY_API |                 | Hub(STUDY_ID)      | HUB_STUDY    |                 |          |
  # | CLIENT_CLINICAL_SITE_API  |                 | Hub(STUDY_SITE_ID) | HUB_SITE     |                 |          |
  # When the files are pushed into the ingestion bucket
  # Then the auto ingestion is triggered only for
  # | Filename                      |
  # | CLIENT_CLINICAL_STUDY_API.csv |
  # | CLIENT_CLINICAL_SITE_API.csv  |</code></pre>
<pre><code>Scenario: Auto run conversion is triggered only once for a multi file ingestion
  Given active configuration for the provider exists
  When multiple csv files are loaded in to the ingestion bucket
  And  ingestion status is Success for the uploaded file
  Then Auto run conversion is executed only once</code></pre>
<pre><code>Scenario Outline: Auto run comversion
  Given active configuration for the &lt;data_provider&gt; is &lt;active_config&gt;
  When a csv file is loaded in to the ingestion bucket for the &lt;data_provider&gt;
  And  ingestion status is &lt;ingestion_status&gt; for the uploaded file
  And  Dependent source datasets are &lt;source_status&gt;
  Then status of auto run conversion is &lt;final_job_status&gt; with &lt;status_message&gt;
  Examples:
  | data_provider | active_config | ingestion_status | source_status  | final_job_status | status_message                                                                             |
  | provider1     | Available     | Success          | Not missing    | Success          | Auto run conversion triggered                                                              |
  | provider2     | Not Available | Success          | Not applicable | Success          | No active configuration exists.No run conversion executed                                  |
  | provider3     | Available     | Success          | Missing        | Failed           | Dependent tables either Failed or missing Ingestion.Will not start the auto run conversion |
  # | provider4     | Available     | Failure          | Not Missing    | Failed           | Dependent tables either Failed or missing Ingestion.Will not start the auto run conversion |</code></pre>
<pre><code>Scenario: Automatically ingest study data when the process is triggered
  Given the file test.csv is present in ingestion bucket /Project/data_provider1/study1/Source/Data
  And the data provider DATA_PROVIDER1 is present
  And the study STUDY1 is present
  When the ingestion process is triggered
  Then the following information is provided
  | category | data_provider_name | study_name | connection_type | filename | metadata_id |
  | Study    | data_provider1     | study1     | Auto            | test.csv | &lt;GCS ID&gt;    |
  And the following data connection details are present
  # connection will be created if it does not exist based on data connection name
  | data_connection_name             | data_connection_description                          | data_provider_key    | data_category | study_key    | data_connection_type |
  | auto_study_DATA_PROVIDER1_STUDY1 | auto study data connection for DATA_PROVIDER1 STUDY1 | &lt;data_provider1 key&gt; | Study         | &lt;study1 key&gt; | Auto                 |
  And the following data file details are present
  | data_file_key | file_name | file_ingest_timestamp | data_connection_key             | ingestion_status | error_message | profile_path                            |
  | &lt;metadata id&gt; | test.csv  | &lt;current_timestamp&gt;   | &lt;auto_study_data_provider1 key&gt; | Success          |               | Project/study1/Source/Profile/TEST.html |</code></pre>
<h5 data-number="6.4.3.1.4" id="alternative-scenarios-for-user-lacks-permissions"><span class="header-section-number">6.4.3.1.4</span> Alternative Scenarios for: User lacks permissions</h5>
<pre><code>Scenario: Covered by Base Flow use cases</code></pre>
<h5 data-number="6.4.3.1.5" id="alternative-scenarios-for-data-load-via-sftp"><span class="header-section-number">6.4.3.1.5</span> Alternative Scenarios for: Data Load via sFTP</h5>
<pre><code>Scenario: Covered by Base Flow use cases</code></pre>
<h5 data-number="6.4.3.1.6" id="alternative-scenarios-for-data-load-failed"><span class="header-section-number">6.4.3.1.6</span> Alternative Scenarios for: Data Load Failed</h5>
<pre><code>Scenario: Covered by Base Flow use cases</code></pre>
<h5 data-number="6.4.3.1.7" id="alternative-scenarios-for-data-load-for-study"><span class="header-section-number">6.4.3.1.7</span> Alternative Scenarios for: Data Load for Study</h5>
<pre><code>Scenario: Ingest files from CSV into BigQuery tables
  Given more than one csv files from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file</code></pre>
<pre><code>Scenario: Ingest files from SAS XPT into BigQuery tables
  Given more than one xpt files from study PDQ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file</code></pre>
<pre><code>Scenario: Ingest files from SAS7BDAT into BigQuery tables
  Given more than one sas7bdat files from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file</code></pre>
<pre><code>Scenario: Ingest and store Date, Time, and Datetime data values from SAS7BDAT as STRING values in BigQuery tables
  Given multiple sas7bdat files from study JKL in a directory with various date, time, and datetime formatted columns
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And the date, time and datetime columns have the formatted contents as the corresponding source file</code></pre>
<pre><code>Scenario: Ingest and store Date, Time, and Datetime data values from SAS7BDAT as STRING values in new fields in BigQuery tables
  Given multiple sas7bdat files from study JKL in a directory with various date, time, and datetime formatted columns
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And the date, time and datetime columns have the formatted contents as the corresponding source file
  And any date, time and datetime columns have corresponding new columns having column name suffixed with _ORG_DT</code></pre>
<pre><code>Scenario: Ingest and store metadata from all source SAS7BDAT files into a separate BigQuery table for the study
  Given more than one sas7bdat files from study MNO in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And a metadata table is created with the study name and &quot;_metadata&quot;
  And the metadata table has the same metadata contents per source file</code></pre>
<pre><code>Scenario: Ingest and store metadata from all source SAS XPT files into a separate BigQuery table for the study
  Given more than one xpt files from study PDQ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And a metadata table is created with the study name and &quot;_metadata&quot;
  And the metadata table has the same metadata contents per source file</code></pre>
<pre><code>Scenario: Ingest and store metadata from all source CSV files into a separate BigQuery table for the study
  Given more than one csv files from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And a metadata table is created with the study name and &quot;_metadata&quot;
  And the metadata table has the same metadata contents per source file</code></pre>
<pre><code>Scenario: Ingest XLS and store metadata from all source XLS files into a separate BigQuery table for the study
  Given more than one xls files from study RST in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And a metadata table is created with the study name and &quot;_metadata&quot;
  And the metadata table has the same metadata contents per source file</code></pre>
<pre><code>Scenario: Ingest XLSX and store metadata from all source XLSX files into a separate BigQuery table for the study
  Given more than one xlsx files from study RST in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And a metadata table is created with the study name and &quot;_metadata&quot;
  And the metadata table has the same metadata contents per source file</code></pre>
<pre><code>Scenario: Ingest and store metadata from all source files into a separate BigQuery table for the study
  Given more than one data files from study MNO in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And any non alphanumeric character is replaced with an &quot;_&quot; in the schemas
  And the tables have the same contents as the corresponding source file
  And a metadata table is created with the study name and &quot;_metadata&quot;
  And the metadata table has the same metadata contents per source file</code></pre>
<pre><code>Scenario: Ingest files from SAS7BDAT into BigQuery tables with added SRC_SEQ variable
  Given more than one sas7bdat files from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And a SRC_SEQ variable is added to the end of each corresponding source file
  And the SRC_SEQ variable is a string variable
  And the SRC_SEQ variable contains the name of the corresponding source file
  And the SRC_SEQ variable appends the original row number of the corresponding source file to the name separated by a period
  And the corresponding source file row number is zero-padded up 7 total digits
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And the variables are in the same order as the corresponding source file</code></pre>
<pre><code>Scenario: Ingest files from CSV into BigQuery tables with added SRC_SEQ variable
  Given more than one csv files from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And a SRC_SEQ variable is added to the end of each corresponding source file
  And the SRC_SEQ variable is a string variable
  And the SRC_SEQ variable contains the name of the corresponding source file
  And the SRC_SEQ variable appends the original row number of the corresponding source file to the name separated by a period
  And the corresponding source file row number is zero-padded up 7 total digits
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And the variables are in the same order as the corresponding source file</code></pre>
<pre><code>Scenario: Ingest tables from XLS into BigQuery tables with added SRC_SEQ variable
  Given more than one xls files from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And a SRC_SEQ variable is added to the end of each corresponding source file
  And the SRC_SEQ variable is a string variable
  And the SRC_SEQ variable contains the name of the corresponding source file
  And the SRC_SEQ variable appends the original row number of the corresponding source file to the name separated by a period
  And the corresponding source file row number is zero-padded up 7 total digits
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And the variables are in the same order as the corresponding source file</code></pre>
<pre><code>Scenario: Ingest tables from XLSX into BigQuery tables with added SRC_SEQ variable
  Given more than one xlsx files from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And a SRC_SEQ variable is added to the end of each corresponding source file
  And the SRC_SEQ variable is a string variable
  And the SRC_SEQ variable contains the name of the corresponding source file
  And the SRC_SEQ variable appends the original row number of the corresponding source file to the name separated by a period
  And the corresponding source file row number is zero-padded up 7 total digits
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And the variables are in the same order as the corresponding source file</code></pre>
<pre><code>Scenario: Ingest CSV files with spaces in headers
  Given a CSV file named LAB_LUT3 contains
  | conversion factor | precision | default unit | test code | test category | test subcategory | specimen | source unit | orres unit | stresu unit |
  |                   |           | N            | ALT       | CHEMISTRY     |                  | SERUM    |             |            |             |
  |                   |           | N            | CREAT     | CHEMISTRY     |                  | SERUM    |             |            |             |
  | 1                 |           | Y            | ALT       | CHEMISTRY     |                  | SERUM    | IU/L        | IU/L       | IU/L        |
  | 1                 |           | Y            | ALT       | CHEMISTRY     |                  | SERUM    | U/L         | U/L        | IU/L        |
  | 0.167             | 3         | Y            | ALT       | CHEMISTRY     |                  | SERUM    | U/dL        | U/dL       | ukat/L      |
  | 1000              | 2         | Y            | CREAT     | CHEMISTRY     |                  | SERUM    | mmol/L      | mmol/L     | umol/L      |
  | 88.402            | 2         | Y            | CREAT     | CHEMISTRY     |                  | SERUM    | mg/dL       | mg/dL      | umol/L      |
  | 8.84              | 2         | Y            | CREAT     | CHEMISTRY     |                  | SERUM    | mg/L        | mg/L       | umol/L      |
  |                   |           | Y            | CREAT     | CHEMISTRY     |                  | SERUM    | umol/L      | umol/L     | umol/L      |
  And study is named XYZ
  When the files are ingested
  Then the ingestion table LAB_LUT3 contains
  | conversion_factor | precision | default_unit | test_code | test_category | test_subcategory | specimen | source_unit | orres_unit | stresu_unit |
  |                   |           | N            | ALT       | CHEMISTRY     |                  | SERUM    |             |            |             |
  |                   |           | N            | CREAT     | CHEMISTRY     |                  | SERUM    |             |            |             |
  | 1                 |           | Y            | ALT       | CHEMISTRY     |                  | SERUM    | IU/L        | IU/L       | IU/L        |
  | 1                 |           | Y            | ALT       | CHEMISTRY     |                  | SERUM    | U/L         | U/L        | IU/L        |
  | 0.167             | 3         | Y            | ALT       | CHEMISTRY     |                  | SERUM    | U/dL        | U/dL       | ukat/L      |
  | 1000              | 2         | Y            | CREAT     | CHEMISTRY     |                  | SERUM    | mmol/L      | mmol/L     | umol/L      |
  | 88.402            | 2         | Y            | CREAT     | CHEMISTRY     |                  | SERUM    | mg/dL       | mg/dL      | umol/L      |
  | 8.84              | 2         | Y            | CREAT     | CHEMISTRY     |                  | SERUM    | mg/L        | mg/L       | umol/L      |
  |                   |           | Y            | CREAT     | CHEMISTRY     |                  | SERUM    | umol/L      | umol/L     | umol/L      |</code></pre>
<pre><code>Scenario: Ingest and store metadata from a source file into a separate BigQuery table for the study
  Given more than one csv files from study XYZ in a directory
  When the files are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And a metadata table is created with the study name and &quot;_metadata&quot;
  And the metadata table has the same metadata contents per source file
  And the metadata table has the BigQuery type represented in a column named &#39;bq_type&#39;</code></pre>
<pre><code>Scenario: Ingest tables from RDBMS into BigQuery tables
  Given multiple tables in a RDBMS from study XYZ
  When the tables are ingested
  Then a BigQuery Dataset is created with the study name
  And the contents of the tables are loaded into BigQuery tables with the names matching the original names
  And the tables have the same schemas as the corresponding source table
  And the tables have the same contents as the corresponding source table</code></pre>
<pre><code>Scenario: Ingest tables from RDBMS into BigQuery tables with added SRC_SEQ variable
  Given multiple tables in a RDBMS from study XYZ
  When the tables are ingested
  Then a BigQuery Dataset is created with the study name
  And a SRC_SEQ variable is added to the end of each corresponding source file
  And the SRC_SEQ variable is a string variable
  And the SRC_SEQ variable contains the name of the corresponding source file
  And the SRC_SEQ variable appends the original row number of the corresponding source file to the name separated by a period
  And the corresponding source file row number is zero-padded up 7 total digits
  And the contents of the tables are loaded into BigQuery tables with the names matching the original names
  And the tables have the same schemas as the corresponding source table
  And the tables have the same contents as the corresponding source table
  And the variables are in the same order as the corresponding source table</code></pre>
<pre><code>Scenario: Re-ingest source files and store metadata in a new version of the study metadata BigQuery table
  Given more than one source file from a study in a directory
  And a metadata table exists in that directory
  When the files are ingested
  Then the contents of the files are loaded into BigQuery tables with the names matching the file names
  And the tables have the same schemas as the corresponding source file
  And the tables have the same contents as the corresponding source file
  And the existing metadata table is removed
  And a new metadata table is created with the study name and &quot;_metadata&quot;
  And the metadata table has the same metadata contents per source file in the BigQuery Dataset</code></pre>
<pre><code>Scenario: Handle re-ingestion of the same study into BigQuery tables
  When a previously ingested study is ingested again
  Then the new study data is present
  And the previous study data is dropped</code></pre>
<pre><code>Scenario: Handle ingestion of the same file-named data from different providers for the same Study into tables
  Given there is a file named ABC already ingested by Data Provider DP1 for a study
  | category | study_name | data_provider_name | filename |
  | Study    | STUDYABC   | DP1                | ABC.csv  |
  When a file named ABC is ingested by Data Provider DP2 into the same study
  | category | study_name | data_provider_name | filename |
  | Study    | STUDYABC   | DP2                | ABC.csv  |
  Then the first file ABC from DP1 creates a table of ABC
  And the table loaded from the ABC file from DP2 is named ABC_DP2 i.e. &lt;filename&gt;&quot;_&quot;&lt;provider name&gt; instead of just ABC
  | category | study_name | data_provider_name | filename | table_name               |
  | Study    | STUDYABC   | DP1                | ABC.csv  | ABC                      |
  | Study    | STUDYABC   | DP2                | ABC.csv  | ABC_&lt;data_provider_name&gt; |</code></pre>
<pre><code>Scenario: A data provider that encountered a duplicate continues to create tables with the provider as a table name suffix
  Given there is a ABC previously ingested by DP1 and ABC previously ingested by DP2
  | category | study_name | data_provider_name | filename | table_name               | version
  | Study    | STUDYABC   | DP1                | ABC.csv  | ABC                      | 1       |
  | Study    | STUDYABC   | DP2                | ABC.csv  | ABC_&lt;data_provider_name&gt; | 1       |
  When ABC in ingested again by DP2
  | category | study_name | data_provider_name | filename |
  | Study    | STUDYABC   | DP2                | ABC.csv  |
  Then the table should be named ABC&lt;data_provider_name&gt;
  | category | study_name | data_provider_name | filename | table_name               | version |
  | Study    | STUDYABC   | DP1                | ABC.csv  | ABC                      | 1       |
  | Study    | STUDYABC   | DP2                | ABC.csv  | ABC_&lt;data_provider_name&gt; | 2       |</code></pre>
<pre><code>Scenario: After a duplicate is encountered the original provider continues to use the table name without a suffix
  Given there is a ABC previously ingested by DP1 and ABC previously ingested by DP2
  | category | study_name | data_provider_name | filename | table_name               | version
  | Study    | STUDYABC   | DP1                | ABC.csv  | ABC                      | 1       |
  | Study    | STUDYABC   | DP2                | ABC.csv  | ABC_&lt;data_provider_name&gt; | 1       |
  When ABC in ingested again by DP1
  | category | study_name | data_provider_name | filename |
  | Study    | STUDYABC   | DP1                | ABC.csv  |
  Then the table should be named ABC
  | category | study_name | data_provider_name | filename | table_name               | version |
  | Study    | STUDYABC   | DP2                | ABC.csv  | ABC                      | 2       |
  | Study    | STUDYABC   | DP2                | ABC.csv  | ABC_&lt;data_provider_name&gt; | 1       |</code></pre>
<pre><code>Scenario: Ingest extended SDTM standard
  # Req: SDTM metadata will have a standardized structure that easily be utilized by the SDTM conversion process
  # Req: Standard type and version will be specified for a given configuration in order to utilize the correct metadata
  # Req: Concepts such as target variable order, case and required variables will be specified within the standard metadata
  # Req: Standards metadata will be utilized across studies and should not be stored with study data
  # Req: Standards metadata will be loaded into the system and updated periodically
  # Req: All metadata for a given standard type will be stored in one table, there is no fixed list for the standard type names
  # Req: All versions of metadata for a given standard type will be stored in the same table, version may not always be numeric
  # Req: The database table will first be truncated each time this file is loaded
  Given an Excel reference data file called SDTM_Standards.xlsx containing
  | SDTM_VERSION | CLASS_NAME      | DOMAIN_NAME | VARIABLE_NAME | VARIABLE_LABEL                     | VARIABLE_TYPE | VARIABLE_LENGTH | VARIABLE_PERMISSIBILITY | VARIABLE_ORIGIN | VARIABLE_ROLE      | VARIABLE_POSITION | MAINTAIN_CASE |
  | IG 3.2       | Special Purpose | DM          | STUDYID       | Study Identifier                   | Char          | 200             | Req                     |                 | Identifier         | 1                 |               |
  | IG 3.2       | Special Purpose | DM          | DOMAIN        | Domain Abbreviation                | Char          | 2               | Req                     |                 | Identifier         | 2                 |               |
  | IG 3.2       | Special Purpose | DM          | USUBJID       | Unique Subject Identifier          | Char          | 200             | Req                     |                 | Identifier         | 3                 |               |
  | IG 3.2       | Special Purpose | DM          | POOLID        | Pool Identifier                    | Char          | 200             | Perm                    |                 | Identifier         | 4                 |               |
  | IG 3.2       | Special Purpose | DM          | SUBJID        | Subject Identifier for the Study   | Char          | 200             | Req                     |                 | Topic              | 5                 |               |
  | IG 3.2       | Special Purpose | DM          | RFSTDTC       | Subject Reference Start Date/Time  | Char          | 200             | Exp                     |                 | Record Qualifier   | 6                 |               |
  | IG 3.2       | Special Purpose | DM          | RFENDTC       | Subject Reference End Date/Time    | Char          | 200             | Exp                     |                 | Record Qualifier   | 7                 |               |
  | IG 3.2       | Special Purpose | DM          | RFXSTDTC      | Date/Time of First Study Treatment | Char          | 200             | Exp                     |                 | Record Qualifier   | 8                 |               |
  | IG 3.2       | Special Purpose | DM          | RFXENDTC      | Date/Time of Last Study Treatment  | Char          | 200             | Exp                     |                 | Record Qualifier   | 9                 |               |
  | IG 3.2       | Special Purpose | DM          | RFICDTC       | Date/Time of Informed Consent      | Char          | 200             | Exp                     |                 | Record Qualifier   | 10                |               |
  | IG 3.2       | Special Purpose | DM          | RFPENDTC      | Date/Time of End of Participation  | Char          | 200             | Exp                     |                 | Record Qualifier   | 11                |               |
  | IG 3.2       | Special Purpose | DM          | DTHDTC        | Date/Time of Death                 | Char          | 200             | Exp                     |                 | Record Qualifier   | 12                |               |
  | IG 3.2       | Special Purpose | DM          | DTHFL         | Subject Death Flag                 | Char          | 2               | Exp                     |                 | Record Qualifier   | 13                |               |
  | IG 3.2       | Special Purpose | DM          | SITEID        | Study Site Identifier              | Char          | 200             | Req                     |                 | Record Qualifier   | 14                |               |
  | IG 3.2       | Special Purpose | DM          | INVID         | Investigator Identifier            | Char          | 200             | Perm                    |                 | Record Qualifier   | 15                |               |
  | IG 3.2       | Special Purpose | DM          | INVNAM        | Investigator Name                  | Char          | 200             | Perm                    |                 | Synonym Qualifier  | 16                | Yes           |
  | IG 3.2       | Special Purpose | DM          | BRTHDTC       | Date/Time of Birth                 | Char          | 200             | Perm                    |                 | Record Qualifier   | 17                |               |
  | IG 3.2       | Special Purpose | DM          | AGE           | Age                                | Num           | 8               | Exp                     |                 | Record Qualifier   | 18                |               |
  | IG 3.2       | Special Purpose | DM          | AGEU          | Age Units                          | Char          | 6               | Exp                     |                 | Variable Qualifier | 19                |               |
  | IG 3.2       | Special Purpose | DM          | SEX           | Sex                                | Char          | 2               | Req                     |                 | Record Qualifier   | 20                |               |
  | IG 3.2       | Special Purpose | DM          | RACE          | Race                               | Char          | 200             | Exp                     |                 | Record Qualifier   | 21                |               |
  | IG 3.2       | Special Purpose | DM          | ETHNIC        | Ethnicity                          | Char          | 200             | Perm                    |                 | Record Qualifier   | 22                |               |
  | IG 3.2       | Special Purpose | DM          | ARMCD         | Planned Arm Code                   | Char          | 20              | Req                     |                 | Record Qualifier   | 23                |               |
  | IG 3.2       | Special Purpose | DM          | ARM           | Description of Planned Arm         | Char          | 200             | Req                     |                 | Synonym Qualifier  | 24                | Yes           |
  | IG 3.2       | Special Purpose | DM          | ACTARMCD      | Actual Arm Code                    | Char          | 20              | Req                     |                 | Record Qualifier   | 25                |               |
  | IG 3.2       | Special Purpose | DM          | ACTARM        | Description of Actual Arm          | Char          | 200             | Req                     |                 | Synonym Qualifier  | 26                | Yes           |
  | IG 3.2       | Special Purpose | DM          | COUNTRY       | Country                            | Char          | 3               | Req                     |                 | Record Qualifier   | 27                |               |
  | IG 3.2       | Special Purpose | DM          | DMXFN         | External File Name                 | Char          | 200             | Perm                    |                 | Record Qualifier   | 28                | Yes           |
  | IG 3.2       | Special Purpose | DM          | VISITNUM      | Visit Number                       | Num           | 8               | Perm                    |                 | Timing             | 29                |               |
  | IG 3.2       | Special Purpose | DM          | VISIT         | Visit Name                         | Char          | 200             | Perm                    |                 | Timing             | 30                |               |
  | IG 3.2       | Special Purpose | DM          | VISITDY       | Planned Study Day of Visit         | Num           | 8               | Perm                    |                 | Timing             | 31                |               |
  | IG 3.2       | Special Purpose | DM          | DMDTC         | Date/Time of Collection            | Char          | 200             | Perm                    |                 | Timing             | 32                |               |
  | IG 3.2       | Special Purpose | DM          | DMDY          | Study Day of Collection            | Num           | 8               | Perm                    |                 | Timing             | 33                |               |
  When the reference data is ingested
  Then the SDTM_STANDARDS table loaded in the reference dataset will contain
  | SDTM_VERSION | CLASS_NAME      | DOMAIN_NAME | VARIABLE_NAME | VARIABLE_LABEL                     | VARIABLE_TYPE | VARIABLE_LENGTH | VARIABLE_PERMISSIBILITY | VARIABLE_ORIGIN | VARIABLE_ROLE      | VARIABLE_POSITION | MAINTAIN_CASE |
  | IG 3.2       | Special Purpose | DM          | STUDYID       | Study Identifier                   | Char          | 200             | Req                     |                 | Identifier         | 1                 |               |
  | IG 3.2       | Special Purpose | DM          | DOMAIN        | Domain Abbreviation                | Char          | 2               | Req                     |                 | Identifier         | 2                 |               |
  | IG 3.2       | Special Purpose | DM          | USUBJID       | Unique Subject Identifier          | Char          | 200             | Req                     |                 | Identifier         | 3                 |               |
  | IG 3.2       | Special Purpose | DM          | POOLID        | Pool Identifier                    | Char          | 200             | Perm                    |                 | Identifier         | 4                 |               |
  | IG 3.2       | Special Purpose | DM          | SUBJID        | Subject Identifier for the Study   | Char          | 200             | Req                     |                 | Topic              | 5                 |               |
  | IG 3.2       | Special Purpose | DM          | RFSTDTC       | Subject Reference Start Date/Time  | Char          | 200             | Exp                     |                 | Record Qualifier   | 6                 |               |
  | IG 3.2       | Special Purpose | DM          | RFENDTC       | Subject Reference End Date/Time    | Char          | 200             | Exp                     |                 | Record Qualifier   | 7                 |               |
  | IG 3.2       | Special Purpose | DM          | RFXSTDTC      | Date/Time of First Study Treatment | Char          | 200             | Exp                     |                 | Record Qualifier   | 8                 |               |
  | IG 3.2       | Special Purpose | DM          | RFXENDTC      | Date/Time of Last Study Treatment  | Char          | 200             | Exp                     |                 | Record Qualifier   | 9                 |               |
  | IG 3.2       | Special Purpose | DM          | RFICDTC       | Date/Time of Informed Consent      | Char          | 200             | Exp                     |                 | Record Qualifier   | 10                |               |
  | IG 3.2       | Special Purpose | DM          | RFPENDTC      | Date/Time of End of Participation  | Char          | 200             | Exp                     |                 | Record Qualifier   | 11                |               |
  | IG 3.2       | Special Purpose | DM          | DTHDTC        | Date/Time of Death                 | Char          | 200             | Exp                     |                 | Record Qualifier   | 12                |               |
  | IG 3.2       | Special Purpose | DM          | DTHFL         | Subject Death Flag                 | Char          | 2               | Exp                     |                 | Record Qualifier   | 13                |               |
  | IG 3.2       | Special Purpose | DM          | SITEID        | Study Site Identifier              | Char          | 200             | Req                     |                 | Record Qualifier   | 14                |               |
  | IG 3.2       | Special Purpose | DM          | INVID         | Investigator Identifier            | Char          | 200             | Perm                    |                 | Record Qualifier   | 15                |               |
  | IG 3.2       | Special Purpose | DM          | INVNAM        | Investigator Name                  | Char          | 200             | Perm                    |                 | Synonym Qualifier  | 16                | Yes           |
  | IG 3.2       | Special Purpose | DM          | BRTHDTC       | Date/Time of Birth                 | Char          | 200             | Perm                    |                 | Record Qualifier   | 17                |               |
  | IG 3.2       | Special Purpose | DM          | AGE           | Age                                | Num           | 8               | Exp                     |                 | Record Qualifier   | 18                |               |
  | IG 3.2       | Special Purpose | DM          | AGEU          | Age Units                          | Char          | 6               | Exp                     |                 | Variable Qualifier | 19                |               |
  | IG 3.2       | Special Purpose | DM          | SEX           | Sex                                | Char          | 2               | Req                     |                 | Record Qualifier   | 20                |               |
  | IG 3.2       | Special Purpose | DM          | RACE          | Race                               | Char          | 200             | Exp                     |                 | Record Qualifier   | 21                |               |
  | IG 3.2       | Special Purpose | DM          | ETHNIC        | Ethnicity                          | Char          | 200             | Perm                    |                 | Record Qualifier   | 22                |               |
  | IG 3.2       | Special Purpose | DM          | ARMCD         | Planned Arm Code                   | Char          | 20              | Req                     |                 | Record Qualifier   | 23                |               |
  | IG 3.2       | Special Purpose | DM          | ARM           | Description of Planned Arm         | Char          | 200             | Req                     |                 | Synonym Qualifier  | 24                | Yes           |
  | IG 3.2       | Special Purpose | DM          | ACTARMCD      | Actual Arm Code                    | Char          | 20              | Req                     |                 | Record Qualifier   | 25                |               |
  | IG 3.2       | Special Purpose | DM          | ACTARM        | Description of Actual Arm          | Char          | 200             | Req                     |                 | Synonym Qualifier  | 26                | Yes           |
  | IG 3.2       | Special Purpose | DM          | COUNTRY       | Country                            | Char          | 3               | Req                     |                 | Record Qualifier   | 27                |               |
  | IG 3.2       | Special Purpose | DM          | DMXFN         | External File Name                 | Char          | 200             | Perm                    |                 | Record Qualifier   | 28                | Yes           |
  | IG 3.2       | Special Purpose | DM          | VISITNUM      | Visit Number                       | Num           | 8               | Perm                    |                 | Timing             | 29                |               |
  | IG 3.2       | Special Purpose | DM          | VISIT         | Visit Name                         | Char          | 200             | Perm                    |                 | Timing             | 30                |               |
  | IG 3.2       | Special Purpose | DM          | VISITDY       | Planned Study Day of Visit         | Num           | 8               | Perm                    |                 | Timing             | 31                |               |
  | IG 3.2       | Special Purpose | DM          | DMDTC         | Date/Time of Collection            | Char          | 200             | Perm                    |                 | Timing             | 32                |               |
  | IG 3.2       | Special Purpose | DM          | DMDY          | Study Day of Collection            | Num           | 8               | Perm                    |                 | Timing             | 33                |               |
  And the standards metadata is not stored with the study data</code></pre>
<h2 data-number="6.5" id="view-data-and-data-profile"><span class="header-section-number">6.5</span> View Data and Data Profile</h2>
<h3 data-number="6.5.1" id="base-flow---view-data-and-data-profiles"><span class="header-section-number">6.5.1</span> Base Flow - View Data and Data Profiles</h3>
<ol type="1">
<li>User navigates to the Ingest Data feature and selects the desired data provider</li>
<li>User selects a table previously loaded for the data provider</li>
<li>User selects Preview feature to view the data</li>
<li>User selects Profile feature to view metrics about the table such as number of variables, count by variable type</li>
</ol>
<h3 data-number="6.5.2" id="feature-details-4"><span class="header-section-number">6.5.2</span> Feature Details</h3>
<h4 data-number="6.5.2.1" id="view-and-profile-data"><span class="header-section-number">6.5.2.1</span> View and Profile Data</h4>
<h5 data-number="6.5.2.1.1" id="base-flow-scenarios-for-view-data-and-data-profiles"><span class="header-section-number">6.5.2.1.1</span> Base Flow Scenarios for: View Data and Data Profiles</h5>
<pre><code>Scenario: Select table details
  # Use for operational and study
  Given the user has navigated to the list of ingested tables for an individual data provider
  When the user selects a table
  Then the &#39;Summary&#39; screen is displayed
  And the following info is displayed Table Name, Table Size, Number of rows, Created, Last modified</code></pre>
<pre><code>Scenario: Display of Multi-tab Ingest files
  # Use for operational and study
  Given a multi-tab file has been ingested and the tab contents result in individual files
  When the user has navigated to the list of ingested tables for an individual data provider
  Then the &#39;Summary&#39; screen page is displayed
  And the following info is displayed Table Name, Table Size, Number of rows, Created, Last modified in page
  And each resulting tab contents in tables are identified as tab name
  And the multi-tab file itself is not displayed in page
  # While ingested files can handle zip files the resulting representation must be handle differently than a single file ingest
  # The zip file itself does not represent the contents because the individual file contents is what represents the table content</code></pre>
<pre><code>Scenario: Display metadata for a selected table
  # Use for operational and study
  Given the user has selected a table for an individual data provider
  When the user selects &#39;Metadata&#39;
  Then the &#39;Metadata&#39; screen is displayed
  And displays a table with Field Name and Type as the columns
  And a row appears for each column in the selected table</code></pre>
<pre><code>Scenario: Display preview for a selected table
  # Use for operational and study
  Given the user has selected a table for an individual data provider
  When the user selects &#39;Preview&#39;
  Then the &#39;Preview&#39; screen is displayed
  And displays a table where the columns match the columns in the selected table
  And the rows display up to 20 rows of data from the selected table</code></pre>
<pre><code>Scenario: Display profile for a selected table
  # Use for operational and study
  # profile reports are generated only for csv and xls files
  Given the user has selected a table for an individual data provider
  When the user selects &#39;Profile&#39;
  Then the &#39;Profile&#39; screen is displayed
  And the Great Expectations report is displayed</code></pre>
<h2 data-number="6.6" id="browse-query-and-snapshot-data"><span class="header-section-number">6.6</span> Browse, Query and Snapshot Data</h2>
<h3 data-number="6.6.1" id="base-flow---create-and-use-snapshot-data"><span class="header-section-number">6.6.1</span> Base Flow - Create and Use Snapshot Data</h3>
<ol type="1">
<li>User navigates to a Study</li>
<li>User selects Browse Data &amp; Snapshot option</li>
<li>User <a href="#create-data-filter">Creates Data Filters</a></li>
<li>User <a href="#create-data-query">Creates Data Queries</a></li>
<li>User [Creates Snapshot] using external criteria file (#create-snapshot-using-criteria-file)</li>
<li>User enters snapshot name and saves snapshot</li>
</ol>
<h3 data-number="6.6.2" id="alternatives-3"><span class="header-section-number">6.6.2</span> Alternatives</h3>
<h4 data-number="6.6.2.1" id="a---user-selects-produce-sdtm-option"><span class="header-section-number">6.6.2.1</span> 2a - User selects Produce SDTM option</h4>
<ol type="1">
<li>User selects Produce SDTM option</li>
<li>User selects View Output</li>
<li>Continue at step 3 of base flow</li>
</ol>
<h4 data-number="6.6.2.2" id="a---build-snapshot-using-filters"><span class="header-section-number">6.6.2.2</span> 3a - Build snapshot using filters</h4>
<ol type="1">
<li>User applies filters on tables</li>
<li>User selects create snapshot</li>
<li>User selects the filtered tabled from Filters tab</li>
<li>User submits the request for snapshot creation</li>
</ol>
<h4 data-number="6.6.2.3" id="a---build-snapshot-using-criteria-file"><span class="header-section-number">6.6.2.3</span> 5a - Build Snapshot Using Criteria File</h4>
<ol type="1">
<li>User selects Create Snapshot option</li>
<li>User imports criteria file with an exclusion list of values</li>
<li>System successfully imports file and creates snapshot</li>
</ol>
<h4 data-number="6.6.2.4" id="a---user-enters-invalid-snapshot-name-non-unique-or-with-invalid-characters"><span class="header-section-number">6.6.2.4</span> 6a - User enters invalid snapshot name (non-unique or with invalid characters)</h4>
<ol type="1">
<li>System notifies user of invalid snapshot name</li>
</ol>
<h3 data-number="6.6.3" id="feature-details-5"><span class="header-section-number">6.6.3</span> Feature Details</h3>
<h4 data-number="6.6.3.1" id="browse-query-and-snapshot-data-1"><span class="header-section-number">6.6.3.1</span> Browse, Query and Snapshot Data</h4>
<h5 data-number="6.6.3.1.1" id="base-flow-scenarios-for-create-and-use-snapshot-data"><span class="header-section-number">6.6.3.1.1</span> Base Flow Scenarios for: Create and Use Snapshot Data</h5>
<pre><code>Scenario Outline: Snapshot variable types should remain the same as the source if all values are missing
  Given the user is working in a &lt;context&gt; data browser context
  And the user has selected &lt;dataset&gt;
  And the metadata for the study is
  | table_name | source_variable | source_type | source_format | source_length | source_label | load_date                     | bq_type |
  | DEMOG      | STUDY_ID        | Char        |               | 3.0           | Study ID     | 2021-05-18 18:19:49.999510 UT | STRING  |
  | DEMOG      | SUBJECT_ID      | Char        |               | 11.0          | Subject ID   | 2021-05-18 18:19:49.999510 UT | STRING  |
  | DEMOG      | GENDER          | Char        |               | 1.0           | Gender       | 2021-05-18 18:19:49.999510 UT | STRING  |
  | DEMOG      | ETHNIC          | Char        |               | 3.0           | Ethnicity    | 2021-05-18 18:19:49.999510 UT | STRING  |
  | AE         | STUDY_ID        | Char        |               | 3.0           | Study ID     | 2021-05-18 18:19:49.999510 UT | STRING  |
  | AE         | SUBJECT_ID      | Char        |               | 11.0          | Subject ID   | 2021-05-18 18:19:49.999510 UT | STRING  |
  | AE         | VISIT           | Char        |               | 6.0           | Subject ID   | 2021-05-18 18:19:49.999510 UT | STRING  |
  | AE         | RECURRENCE      | Numeric     |               | 6.0           | Subject ID   | 2021-05-18 18:19:49.999510 UT | FLOAT   |
  | AE         | RECURRENCE2     | bigint      |               | 6.0           | Subject ID   | 2021-05-18 18:19:49.999510 UT | INTEGER |
  And the following source table DEMOG exists
  | STUDY_ID | SUBJECT_ID  | GENDER | ETHNIC |
  | AAA      | AAA-001-001 | M      |        |
  | AAA      | AAA-001-002 | F      |        |
  | AAA      | AAA-001-003 | F      |        |
  | AAA      | AAA-099-099 | F      |        |
  And the following source table AE exists
  | STUDY_ID | SUBJECT_ID  | VISIT  | RECURRENCE | RECURRENCE2 |
  | AAA      | AAA-001-001 | VISIT1 |            |             |
  | AAA      | AAA-001-001 | VISIT2 |            |             |
  | AAA      | AAA-001-001 | VISIT3 |            |             |
  | AAA      | AAA-001-002 | VISIT1 |            |             |
  | AAA      | AAA-001-002 | VISIT2 |            |             |
  | AAA      | AAA-099-099 | VISIT1 |            |             |
  | AAA      | AAA-099-099 |        |            |             |
  And a Study Snapshot criteria file
  | column 1       | column 2    | column 3 |
  | Exclude Tables |             |          |
  | Table Columns  | SUBJECT_ID  | VISIT    |
  |                | AAA-099-099 | NULL     |
  |                | AAA-001-001 | VISIT1   |
  |                | AAA-001-001 | VISIT2   |
  |                | AAA-001-002 | VISIT1   |
  When a snapshot named SNAPSHOT123 is created with the Study Snapshot criteria file
  Then the following table DEMOG is created in snapshot SNAPSHOT123
  | STUDY_ID | SUBJECT_ID  | GENDER | ETHNIC |
  | AAA      | AAA-001-001 | M      |        |
  | AAA      | AAA-001-002 | F      |        |
  | AAA      | AAA-001-003 | F      |        |
  | AAA      | AAA-099-099 | F      |        |
  And the following table AE is created in snapshot SNAPSHOT123
  | STUDY_ID | SUBJECT_ID  | VISIT  | RECURRENCE | RECURRENCE2 |
  | AAA      | AAA-001-001 | VISIT3 |            |             |
  | AAA      | AAA-001-002 | VISIT2 |            |             |
  | AAA      | AAA-099-099 | VISIT1 |            |             |
  And verify the metadata for snapshot SNAPSHOT123 is
  | table_name | column_name | data_type |
  | DEMOG      | STUDY_ID    | STRING    |
  | DEMOG      | SUBJECT_ID  | STRING    |
  | DEMOG      | GENDER      | STRING    |
  | DEMOG      | ETHNIC      | STRING    |
  | AE         | STUDY_ID    | STRING    |
  | AE         | SUBJECT_ID  | STRING    |
  | AE         | VISIT       | STRING    |
  | AE         | RECURRENCE  | FLOAT64   |
  | AE         | RECURRENCE2 | INT64     |
  And a message is displayed indicating successful creation of snapshot
  Examples:
  | context    | dataset       |
  | User SDTM  | Ingested Data |
  | Study SDTM | Output Data   |
  | Study      | SNAPSHOT1     |</code></pre>
<pre><code>Scenario Outline: Create a new snapshot with multiple queries
  Given the user is working in a &lt;context&gt; data browser context for study
  And the user has selected &lt;dataset&gt;
  And the following source table DEMOG exists
  | STUDY_ID | SUBJECT_ID  | GENDER |
  | AAA      | AAA-001-001 | M      |
  | AAA      | AAA-001-002 | F      |
  And the following source queries exist
  | query name | query value                            |
  | DEMOG_F    | SELECT * FROM DEMOG WHERE GENDER = &#39;F&#39; |
  | DEMOG_M    | SELECT * FROM DEMOG WHERE GENDER = &#39;M&#39; |
  When a snapshot named SS_DEMOG is created with query members
  | query name |
  | DEMOG_F    |
  | DEMOG_M    |
  # user selects &#39;create snapshot&#39; from actions drop-down, system displays selectable rows
  # metadata columns: query name, dataset (dataset label, not BQ dataset name), last updated date, query string
  # requirements to save: name of snapshot, description (optional)
  # system allows for query search
  # table names matches selected query name
  # from Study SDTM and Study contexts, create Study-level Snapshot BQ dataset: &lt;INSTANCE&gt;_STUDY_&lt;STUDYID&gt;_&lt;SNAPSHOTNAME&gt;_SS
  # from User SDTM, create User-level Snapshot BQ dataset: &lt;INSTANCE&gt;_STUDY_&lt;STUDYID&gt;_&lt;USER&gt;_&lt;SNAPSHOTNAME&gt;_SS
  Then the following table DEMOG_F is created in snapshot SS_DEMOG
  | STUDY_ID | SUBJECT_ID  | GENDER |
  | AAA      | AAA-001-002 | F      |
  And the following table DEMOG_M is created in snapshot SS_DEMOG
  | STUDY_ID | SUBJECT_ID  | GENDER |
  | AAA      | AAA-001-001 | M      |
  # table contents match selected query results
  And a message is displayed indicating successful creation of snapshot
  Examples:
  | context    | dataset       |
  | User SDTM  | Ingested Data |
  | Study SDTM | Output Data   |
  | Study      | SNAPSHOT1     |</code></pre>
<pre><code>Scenario Outline: Tables that have no records should have table structure in snapshots
  Given the user is working in a &lt;context&gt; data browser context for study
  And the user has selected &lt;dataset&gt;
  And the following source table DEMOG exists
  | STUDY_ID | SUBJECT_ID  | GENDER | AGE |
  | AAA      | AAA-001-001 | M      | 35  |
  | AAA      | AAA-001-002 | F      | 43  |
  | AAA      | AAA-001-003 | F      | 18  |
  And the following source table DEMOA exists
  | STUDY_ID | SUBJECT_ID  | AGE |
  | AAA      | AAA-001-001 | 27  |
  | AAA      | AAA-001-002 | 45  |
  | AAA      | AAA-001-003 | 20  |
  And the following source table DEMOB exists
  | STUDY_ID | SUBJECT_ID  | AGE | ETHNIC |
  | AAA      | AAA-001-001 | 27  | WHITE  |
  | AAA      | AAA-001-002 | 45  | WHITE  |
  | AAA      | AAA-001-003 | 20  | WHITE  |
  And the following source queries exist
  | query name | query value                            |
  | DEMOG_F    | SELECT * FROM DEMOG WHERE GENDER = &#39;F&#39; |
  | DEMOG_M    | SELECT * FROM DEMOG WHERE GENDER = &#39;M&#39; |
  | DEMOG_NULL | SELECT * FROM DEMOG WHERE GENDER = &#39;&#39;  |
  And the following filter exists
  | Filters          | Tables      |
  | GENDER = &#39;F&#39;     | DEMOG       |
  | AGE &lt; &#39;30&#39;       | DEMOA,DEMOG |
  | ETHNIC = &#39;ASIAN&#39; | DEMOB       |
  When a snapshot named SS_DEMOAG is created with queries and applied filters
  | query name |
  | DEMOG_F    |
  | DEMOG_M    |
  | DEMOG_NULL |
  | DEMOG      |
  | DEMOA      |
  | DEMOB      |
  Then the following table DEMOG_F is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID  | GENDER | AGE |
  | AAA      | AAA-001-002 | F      | 43  |
  | AAA      | AAA-001-003 | F      | 18  |
  And the following table DEMOG_M is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID  | GENDER | AGE |
  | AAA      | AAA-001-001 | M      | 35  |
  And the following table DEMOG_NULL is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID | GENDER | AGE |
  Then the following table DEMOG is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID  | GENDER | AGE |
  | AAA      | AAA-001-003 | F      | 18  |
  And the following table DEMOA is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID  | AGE |
  | AAA      | AAA-001-001 | 27  |
  | AAA      | AAA-001-003 | 20  |
  And the following table DEMOB is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID | AGE | ETHNIC |
  # table contents match selected query results
  And a message is displayed indicating successful creation of snapshot
  Examples:
  | context    | dataset       |
  | User SDTM  | Ingested Data |
  | Study SDTM | Output Data   |
  | Study      | SNAPSHOT1     |</code></pre>
<h5 data-number="6.6.3.1.2" id="alternative-scenarios-for-build-snapshot-using-filters"><span class="header-section-number">6.6.3.1.2</span> Alternative Scenarios for: Build snapshot using filters</h5>
<pre><code>Scenario Outline: Create a new snapshot with filters
  Given the user is working in a &lt;context&gt; data browser context for study
  And the user has selected &lt;dataset&gt;
  And the following source table DEMOG exists
  | STUDY_ID | SUBJECT_ID  | GENDER | AGE |
  | AAA      | AAA-001-001 | M      | 35  |
  | AAA      | AAA-001-002 | F      | 43  |
  | AAA      | AAA-001-003 | F      | 18  |
  And the following source table DEMOA exists
  | STUDY_ID | SUBJECT_ID  | AGE |
  | AAA      | AAA-001-001 | 27  |
  | AAA      | AAA-001-002 | 45  |
  | AAA      | AAA-001-003 | 20  |
  And the following filter exists
  | Filters      | Tables      |
  | GENDER = &#39;F&#39; | DEMOG       |
  | AGE &lt; &#39;30&#39;   | DEMOA,DEMOG |
  When a snapshot named SS_DEMOAG is created with applied filters
  | query name |
  | DEMOG      |
  | DEMOA      |
  # user selects &#39;create snapshot&#39; from actions drop-down, system displays selectable rows
  # metadata columns: query name, dataset (dataset label, not BQ dataset name), last updated date, query string
  # requirements to save: name of snapshot, description (optional)
  # system allows for query search
  # table names matches selected query name
  # from Study SDTM and Study contexts, create Study-level Snapshot BQ dataset: &lt;INSTANCE&gt;_STUDY_&lt;STUDYID&gt;_&lt;SNAPSHOTNAME&gt;_SS
  # from User SDTM, create User-level Snapshot BQ dataset: &lt;INSTANCE&gt;_STUDY_&lt;STUDYID&gt;_&lt;USER&gt;_&lt;SNAPSHOTNAME&gt;_SS
  Then the following table DEMOG is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID  | GENDER | AGE |
  | AAA      | AAA-001-003 | F      | 18  |
  And the following table DEMOA is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID  | AGE |
  | AAA      | AAA-001-001 | 27  |
  | AAA      | AAA-001-003 | 20  |
  # table contents match selected query results
  And a message is displayed indicating successful creation of snapshot
  Examples:
  | context    | dataset       |
  | User SDTM  | Ingested Data |
  | Study SDTM | Output Data   |
  | Study      | SNAPSHOT1     |</code></pre>
<pre><code>Scenario Outline: Create a new snapshot with filters and queries
  Given the user is working in a &lt;context&gt; data browser context for study
  And the user has selected &lt;dataset&gt;
  And the following source table DEMOG exists
  | STUDY_ID | SUBJECT_ID  | GENDER | AGE |
  | AAA      | AAA-001-001 | M      | 35  |
  | AAA      | AAA-001-002 | F      | 43  |
  | AAA      | AAA-001-003 | F      | 18  |
  And the following source table DEMOA exists
  | STUDY_ID | SUBJECT_ID  | AGE |
  | AAA      | AAA-001-001 | 27  |
  | AAA      | AAA-001-002 | 45  |
  | AAA      | AAA-001-003 | 20  |
  And the following source queries exist
  | query name | query value                            |
  | DEMOG_F    | SELECT * FROM DEMOG WHERE GENDER = &#39;F&#39; |
  | DEMOG_M    | SELECT * FROM DEMOG WHERE GENDER = &#39;M&#39; |
  And the following filter exists
  | Filters      | Tables      |
  | GENDER = &#39;F&#39; | DEMOG       |
  | AGE &lt; &#39;30&#39;   | DEMOA,DEMOG |
  When a snapshot named SS_DEMOAG is created with queries and applied filters
  | query name |
  | DEMOG_F    |
  | DEMOG_M    |
  | DEMOG      |
  | DEMOA      |
  # user selects &#39;create snapshot&#39; from actions drop-down, system displays selectable rows
  # metadata columns: query name, dataset (dataset label, not BQ dataset name), last updated date, query string
  # requirements to save: name of snapshot, description (optional)
  # system allows for query search
  # table names matches selected query name
  # from Study SDTM and Study contexts, create Study-level Snapshot BQ dataset: &lt;INSTANCE&gt;_STUDY_&lt;STUDYID&gt;_&lt;SNAPSHOTNAME&gt;_SS
  # from User SDTM, create User-level Snapshot BQ dataset: &lt;INSTANCE&gt;_STUDY_&lt;STUDYID&gt;_&lt;USER&gt;_&lt;SNAPSHOTNAME&gt;_SS
  Then the following table DEMOG_F is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID  | GENDER | AGE |
  | AAA      | AAA-001-002 | F      | 43  |
  | AAA      | AAA-001-003 | F      | 18  |
  And the following table DEMOG_M is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID  | GENDER | AGE |
  | AAA      | AAA-001-001 | M      | 35  |
  Then the following table DEMOG is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID  | GENDER | AGE |
  | AAA      | AAA-001-003 | F      | 18  |
  And the following table DEMOA is created in snapshot SS_DEMOAG
  | STUDY_ID | SUBJECT_ID  | AGE |
  | AAA      | AAA-001-001 | 27  |
  | AAA      | AAA-001-003 | 20  |
  # table contents match selected query results
  And a message is displayed indicating successful creation of snapshot
  Examples:
  | context    | dataset       |
  | User SDTM  | Ingested Data |
  | Study SDTM | Output Data   |
  | Study      | SNAPSHOT1     |</code></pre>
<pre><code>Scenario: Create snapshot with filter
  Given study1 has a datasource
  And datasource has multiple tables
  And filter is applied to column1 with value1
  When Create Snapshot is requested for table1 with filter
  Then snapshot is created with the filtered table</code></pre>
<h5 data-number="6.6.3.1.3" id="alternative-scenarios-for-build-snapshot-using-criteria-file"><span class="header-section-number">6.6.3.1.3</span> Alternative Scenarios for: Build Snapshot Using Criteria File</h5>
<pre><code>Scenario Outline: Create a new snapshot from criteria file
  Given the user is working in a &lt;context&gt; data browser context
  And the user has selected &lt;dataset&gt;
  And the following source table DEMOG exists
  | STUDY_ID | SUBJECT_ID  | GENDER |
  | AAA      | AAA-001-001 | M      |
  | AAA      | AAA-001-002 | F      |
  | AAA      | AAA-001-003 | F      |
  | AAA      | AAA-099-099 | F      |
  And the following source table AE exists
  | STUDY_ID | SUBJECT_ID  | VISIT  |
  | AAA      | AAA-001-001 | VISIT1 |
  | AAA      | AAA-001-001 | VISIT2 |
  | AAA      | AAA-001-001 | VISIT3 |
  | AAA      | AAA-001-002 | VISIT1 |
  | AAA      | AAA-001-002 | VISIT2 |
  | AAA      | AAA-099-099 | VISIT1 |
  And the following source table TABLEX exists
  | STUDY_ID | SUBJECT_ID  |
  | AAA      | AAA-001-001 |
  | AAA      | AAA-001-002 |
  | AAA      | AAA-001-003 |
  | AAA      | AAA-099-099 |
  And a Study Snapshot criteria file
  | column 1       | column 2    | column 3 |
  | Exclude Tables | TABLEX      |          |
  | Table Columns  | SUBJECT_ID  | VISIT    |
  |                | AAA-099-099 |          |
  |                | AAA-001-001 | VISIT1   |
  |                | AAA-001-001 | VISIT2   |
  |                | AAA-001-002 | VISIT1   |
  When a snapshot named SNAPSHOT123 is created with the Study Snapshot criteria file
  Then the following table DEMOG is created in snapshot SNAPSHOT123
  | STUDY_ID | SUBJECT_ID  | GENDER |
  | AAA      | AAA-001-001 | M      |
  | AAA      | AAA-001-002 | F      |
  | AAA      | AAA-001-003 | F      |
  And the following table AE is created in snapshot SNAPSHOT123
  | STUDY_ID | SUBJECT_ID  | VISIT  |
  | AAA      | AAA-001-001 | VISIT3 |
  | AAA      | AAA-001-002 | VISIT2 |
  And a message is displayed indicating successful creation of snapshot
  Examples:
  | context    | dataset       |
  | User SDTM  | Ingested Data |
  | Study SDTM | Output Data   |
  | Study      | SNAPSHOT1     |</code></pre>
<pre><code>Scenario Outline: Snapshot criteria values of NULL should be treated as NULL instead of being ignored
  Given the user is working in a &lt;context&gt; data browser context
  And the user has selected &lt;dataset&gt;
  And the following source table DEMOG exists
  | STUDY_ID | SUBJECT_ID  | GENDER |
  | AAA      | AAA-001-001 | M      |
  | AAA      | AAA-001-002 | F      |
  | AAA      | AAA-001-003 | F      |
  | AAA      | AAA-099-099 | F      |
  And the following source table AE exists
  | STUDY_ID | SUBJECT_ID  | VISIT  |
  | AAA      | AAA-001-001 | VISIT1 |
  | AAA      | AAA-001-001 | VISIT2 |
  | AAA      | AAA-001-001 | VISIT3 |
  | AAA      | AAA-001-002 | VISIT1 |
  | AAA      | AAA-001-002 | VISIT2 |
  | AAA      | AAA-099-099 | VISIT1 |
  | AAA      | AAA-099-099 |        |
  And the following source table TABLEX exists
  | STUDY_ID | SUBJECT_ID  |
  | AAA      | AAA-001-001 |
  | AAA      | AAA-001-002 |
  | AAA      | AAA-001-003 |
  | AAA      | AAA-099-099 |
  And a Study Snapshot criteria file
  | column 1       | column 2    | column 3 |
  | Exclude Tables | TABLEX      |          |
  | Table Columns  | SUBJECT_ID  | VISIT    |
  |                | AAA-099-099 | NULL     |
  |                | AAA-001-001 | VISIT1   |
  |                | AAA-001-001 | VISIT2   |
  |                | AAA-001-002 | VISIT1   |
  When a snapshot named SNAPSHOT123 is created with the Study Snapshot criteria file
  Then the following table DEMOG is created in snapshot SNAPSHOT123
  | STUDY_ID | SUBJECT_ID  | GENDER |
  | AAA      | AAA-001-001 | M      |
  | AAA      | AAA-001-002 | F      |
  | AAA      | AAA-001-003 | F      |
  | AAA      | AAA-099-099 | F      |
  And the following table AE is created in snapshot SNAPSHOT123
  | STUDY_ID | SUBJECT_ID  | VISIT  |
  | AAA      | AAA-001-001 | VISIT3 |
  | AAA      | AAA-001-002 | VISIT2 |
  | AAA      | AAA-099-099 | VISIT1 |
  And a message is displayed indicating successful creation of snapshot
  Examples:
  | context    | dataset       |
  | User SDTM  | Ingested Data |
  | Study SDTM | Output Data   |
  | Study      | SNAPSHOT1     |</code></pre>
<h5 data-number="6.6.3.1.4" id="alternative-scenarios-for-user-enters-invalid-snapshot-name-non-unique-or-with-invalid-characters"><span class="header-section-number">6.6.3.1.4</span> Alternative Scenarios for: User enters invalid snapshot name (non-unique or with invalid characters)</h5>
<pre><code>Scenario Outline: Do not save snapshot with a non-unique snapshot name
  Given the user is working in a &lt;context&gt; data browser context for a study
  And the following snapshots exist for the study
  | SNAPSHOT NAME | CONTEXT TYPE |
  | SNAPSHOT1     | User SDTM    |
  | SNAPSHOT2     | Study SDTM   |
  | SNAPSHOT3     | Study        |
  When the user saves a new snapshot and provides name &lt;new snapshot name&gt;
  # user, study, snapshot name
  Then the system performs &lt;system action on save&gt;
  Examples:
  | context    | new snapshot name | system action on save                                          |
  | User SDTM  | SNAPSHOT1         | displays a non-unique error message and does not save snapshot |
  | User SDTM  | SNAPSHOT2         | saves snapshot                                                 |
  | Study SDTM | SNAPSHOT1         | saves snapshot                                                 |
  | Study      | SNAPSHOT2         | displays a non-unique error message and does not save snapshot |</code></pre>
<pre><code>Scenario: Error message when snapshot name is invalid
  Given the user is working in a data browser context
  When the user saves the snapshot and provides an invalid snapshot name
  # use existing rules for study ID cleansing
  Then the system displays an invalid character error message
  And the system does not save snapshot</code></pre>
<h5 data-number="6.6.3.1.5" id="alternative-scenarios-for-user-selects-produce-sdtm-option"><span class="header-section-number">6.6.3.1.5</span> Alternative Scenarios for: User selects Produce SDTM option</h5>
<pre><code>Scenario: Covered by SDE use case
  # Draft Scenarios</code></pre>
<h2 data-number="6.7" id="create-data-filter"><span class="header-section-number">6.7</span> Create Data Filter</h2>
<h3 data-number="6.7.1" id="base-flow---create-data-filter"><span class="header-section-number">6.7.1</span> Base Flow - Create Data Filter</h3>
<ol type="1">
<li>User builds a table filter</li>
<li>System applies filter to applicable tables</li>
</ol>
<h3 data-number="6.7.2" id="feature-details-6"><span class="header-section-number">6.7.2</span> Feature Details</h3>
<h4 data-number="6.7.2.1" id="create-data-filter-1"><span class="header-section-number">6.7.2.1</span> Create Data Filter</h4>
<h5 data-number="6.7.2.1.1" id="base-flow-scenarios-for-create-data-filter"><span class="header-section-number">6.7.2.1.1</span> Base Flow Scenarios for: Create Data Filter</h5>
<pre><code>Scenario: Build a table filter column selection
  Given the user is on the browse data view
  And the user has selected study BROWSER001 dataset as Ingested Data
  When the user selects a filter by column from the dropdown
  Then a full list of distinct column names across the tables are present
  # Example:
  # | STUDY_ID   |
  # | SITEID     |
  # | USUBJID    |
  # | AEHLTTM    |
  # | AGE        |
  # | AGE_UOM    |
  # | WEIGHT     |
  # | WEIGHT_UOM |</code></pre>
<pre><code>Scenario: Build a table filter operator selection
  Given the user is on the browse data view
  And the user has selected study BROWSER001 dataset as SDTM
  When the user selects a filter by column
  And the user selects an operator
  Then a full list of operators are present based on selected column type
  # Example:
  # case sensitive
  # Numbers can be integer or decimal
  # Between - both numerical values are included
  # Can use a calendar pop-up for selecting date values - to discuss
  # User should be prompted to enter/select date value
  # | data type | filter operator | filter value      |
  # | string    | equals          | string            |
  # | string    | starts with     | string            |
  # | string    | ends with       | string            |
  # | string    | contains        | string            |
  # | string    | not contains    | string            |
  # | string    | is null         |                   |
  # | string    | is not null     |                   |
  # | string    | in list         | list of string(s) |
  # | string    | not in list     | list of string(s) |
  # | number    | equals                   | single numerical value     |
  # | number    | between                  | two numerical values       |
  # | number    | greater than             | single numerical value     |
  # | number    | greater than or equal to | single numerical value     |
  # | number    | less than                | single numerical value     |
  # | number    | less than or equal to    | single numerical value     |
  # | number    | not equal to             | single numerical value     |
  # | number    | in list                  | List of numerical value(s) |
  # | number    | not in list              | List of numerical value(s) |
  # | date      | equals          | single date_value |
  # | date      | between         | two date_values   |
  # | date      | greater than    | single date_value |
  # | date      | less than       | single date_value |</code></pre>
<pre><code>Scenario: Build a table filter value entry
  Given the user has selected study BROWSER001 dataset as SNAPSHOT
  And the user is working in a context data browser context for a study
  When the user selects a filter by column
  And the user selects an operator
  Then the user enters a value
  &quot;&quot;&quot;
  USUBJID = &#39;0010002&#39;
  &quot;&quot;&quot;</code></pre>
<pre><code>Scenario: The filter is applied to all applicable tables
  Given the user is on the browse data view
  And the user has selected study BROWSER001 dataset
  And Study BROWSER001 has more than 1 table present
  When the user builds a filter
  Then filter is applied to all applicable tables</code></pre>
<h5 data-number="6.7.2.1.2" id="alternative-scenarios-for-invalid-filter"><span class="header-section-number">6.7.2.1.2</span> Alternative Scenarios for: Invalid Filter</h5>
<h5 data-number="6.7.2.1.3" id="draft-base-flow-scenarios-for-create-data-filter"><span class="header-section-number">6.7.2.1.3</span> Draft Base Flow Scenarios for: Create Data Filter</h5>
<h2 data-number="6.8" id="create-data-query"><span class="header-section-number">6.8</span> Create Data Query</h2>
<h3 data-number="6.8.1" id="base-flow---create-data-query"><span class="header-section-number">6.8.1</span> Base Flow - Create Data Query</h3>
<ol type="1">
<li>User opens new query panel</li>
<li>User enters custom query</li>
<li>User runs query</li>
<li>System returns query results</li>
<li>User saves query and enters query name</li>
</ol>
<h3 data-number="6.8.2" id="alternatives-4"><span class="header-section-number">6.8.2</span> Alternatives</h3>
<h4 data-number="6.8.2.1" id="a---user-opens-saved-query"><span class="header-section-number">6.8.2.1</span> 1a - User opens saved query</h4>
<ol type="1">
<li>User opens saved query</li>
<li>User optionally modifies query</li>
<li>Continue at step 3 of base flow</li>
</ol>
<h4 data-number="6.8.2.2" id="a---use-query-builder"><span class="header-section-number">6.8.2.2</span> 2a - Use Query Builder</h4>
<ol type="1">
<li>User selects option for query builder</li>
<li>System prompts for user to select a subset of table columns and create filters</li>
<li>Continue at step 3 of base flow</li>
</ol>
<h4 data-number="6.8.2.3" id="a---invalid-query"><span class="header-section-number">6.8.2.3</span> 4a - Invalid query</h4>
<ol type="1">
<li>System notifies user query is invalid</li>
</ol>
<h4 data-number="6.8.2.4" id="a---user-enters-invalid-query-name-non-unique-or-with-invalid-characters"><span class="header-section-number">6.8.2.4</span> 5a - User enters invalid query name (non-unique or with invalid characters)</h4>
<ol type="1">
<li>System notifies user of invalid query name</li>
</ol>
<h3 data-number="6.8.3" id="feature-details-7"><span class="header-section-number">6.8.3</span> Feature Details</h3>
<h4 data-number="6.8.3.1" id="create-data-query-1"><span class="header-section-number">6.8.3.1</span> Create Data Query</h4>
<h5 data-number="6.8.3.1.1" id="base-flow-scenarios-for-create-data-query"><span class="header-section-number">6.8.3.1.1</span> Base Flow Scenarios for: Create Data Query</h5>
<pre><code>Scenario: Open a SQL Panel
  Given the user is working in a data browser context
  And a table has been selected
  And the summary screen is displayed
  When a new query is requested for selected table
  # behavior is same regardless of the context
  # select a table then select create &#39;new query&#39;
  # metadata tabs are displayed as default
  Then a new untitled query is opened
  And a Select * query is displayed for selected table</code></pre>
<pre><code>Scenario Outline: Enter and Run a SQL Query
  Given the user is working in a &lt;context&gt; data browser context for a study
  And the following snapshots exist for the user and study
  | SNAPSHOT NAME | CONTEXT TYPE |
  | SNAPSHOT1     | User SDTM    |
  | SNAPSHOT2     | Study SDTM   |
  | SNAPSHOT3     | User SDTM    |
  | SNAPSHOT4     | Study        |
  And the following table DM exists in Ingested Data
  | STUDY_ID   | SITEID | USUBJID |
  | BROWSER001 | 001    | 0010001 |
  | BROWSER001 | 001    | 0010002 |
  | BROWSER001 | 001    | 0010003 |
  | BROWSER001 | 001    | 0010004 |
  | BROWSER001 | 001    | 0010005 |
  And the following table DM exists in User Output Data
  | STUDY_ID   | SITEID | USUBJID |
  | BROWSER001 | 001    | 0010001 |
  | BROWSER001 | 001    | 0010002 |
  And the following table DM exists in Study Output Data
  | STUDY_ID   | SITEID | USUBJID |
  | BROWSER001 | 001    | 0010001 |
  | BROWSER001 | 001    | 0010002 |
  | BROWSER001 | 001    | 0010003 |
  And the following table DM exists in Snapshot Data
  | STUDY_ID   | SITEID | USUBJID |
  | BROWSER001 | 001    | 0010001 |
  | BROWSER001 | 001    | 0010002 |
  | BROWSER001 | 001    | 0010003 |
  | BROWSER001 | 001    | 0010004 |
  And the system provides a selection list containing &lt;dataset list&gt;
  When the user selects &lt;dataset&gt; and table DM, and enters and executes the SQL query
  &quot;&quot;&quot;
  SELECT USUBJID FROM DM
  &quot;&quot;&quot;
  Then the data is displayed for &lt;USUBJID list&gt;
  # Ingested data = &lt;INS&gt;_STUDY_&lt;STUDY&gt;_SOURCE
  # Output data (User SDTM) = &lt;INS&gt;_STUDY_&lt;STUDY&gt;_&lt;USER&gt;_TARGET
  # Output data (Study SDTM) = &lt;INS&gt;_STUDY_&lt;STUDY&gt;_PROD_SDTM
  # snapshot dataset (study level) = &lt;INSTANCE&gt;_STUDY_&lt;STUDYID&gt;_&lt;SNAPSHOTNAME&gt;_SS
  # snapshot dataset (user level) = &lt;INSTANCE&gt;_STUDY_&lt;STUDYID&gt;_&lt;USER&gt;_&lt;SNAPSHOTNAME&gt;_SS
  Examples:
  | context    | dataset list                                     | dataset       | USUBJID list                                |
  | User SDTM  | Ingested Data, Output Data, SNAPSHOT1, SNAPSHOT3 | Ingested Data | 0010001, 0010002, 0010003, 0010004, 0010005 |
  | User SDTM  | Ingested Data, Output Data, SNAPSHOT1, SNAPSHOT3 | Output Data   | 0010001, 0010002                            |
  | User SDTM  | Ingested Data, Output Data, SNAPSHOT1, SNAPSHOT3 | SNAPSHOT1     | 0010001, 0010002, 0010003, 0010004          |
  | Study SDTM | Ingested Data, Output Data, SNAPSHOT2, SNAPSHOT4 | Ingested Data | 0010001, 0010002, 0010003, 0010004, 0010005 |
  | Study SDTM | Ingested Data, Output Data, SNAPSHOT2, SNAPSHOT4 | Output Data   | 0010001, 0010002, 0010003                   |
  | Study SDTM | Ingested Data, Output Data, SNAPSHOT2, SNAPSHOT4 | SNAPSHOT2     | 0010001, 0010002, 0010003, 0010004          |
  | Study      | Ingested Data, SNAPSHOT2, SNAPSHOT4              | Ingested Data | 0010001, 0010002, 0010003, 0010004, 0010005 |
  | Study      | Ingested Data, SNAPSHOT2, SNAPSHOT4              | SNAPSHOT4     | 0010001, 0010002, 0010003, 0010004          |</code></pre>
<pre><code>Scenario Outline: Check for uniqueness before saving query
  Given the user is working in a &lt;context&gt; data browser context
  And the following queries exist for the user and study
  | QUERY NAME | CONTEXT TYPE | dataset label |
  | QUERY1     | User SDTM    | Ingested Data |
  | QUERY2     | Study SDTM   | Output Data   |
  | QUERY3     | User SDTM    | Output Data   |
  | QUERY4     | Study        | Ingested Data |
  When the user saves a new query for &lt;dataset label&gt; and provides name &lt;new query name&gt;
  # Unique index on query name, context value, and dataset label, i.e.
  # For User SDTM, query name must be unique for study, user, dataset label
  # For Study SDTM, query name must be unique for study, dataset label
  # For Study, query name must be unique for study, dataset label
  Then the system performs &lt;system action on save&gt;
  Examples:
  | context   | new query name | dataset label | system action on save                                       |
  | User SDTM | QUERY1         | Ingested Data | displays a non-unique error message and does not save query |
  | User SDTM | QUERY2         | Output Data   | saves query                                                 |
  | User SDTM | QUERY4         | Output Data   | saves query                                                 |
  | User SDTM | QUERY3         | Output Data   | displays a non-unique error message and does not save query |</code></pre>
<h5 data-number="6.8.3.1.2" id="alternative-scenarios-for-user-opens-saved-query"><span class="header-section-number">6.8.3.1.2</span> Alternative Scenarios for: User opens saved query</h5>
<pre><code>Scenario: Open a saved SQL query
  Given the user is working in a data browser context
  When the user opens a single saved query
  # user selects open query from page level actions, system displays browser window with list of saved queries
  # For User SDTM, list of queries for study, user, dataset name
  # For Study SDTM, list of queries for study, dataset name
  # For Study, list of queries for study, dataset name
  # browser window has search functionality
  # selected saved query opens in new query tab
  # query already opened system will return to opened tab, will not open new
  Then the saved query is displayed in a new SQL panel
  # System allows for user to have queries open from multiple datasets at the same time</code></pre>
<h5 data-number="6.8.3.1.3" id="alternative-scenarios-for-use-query-builder"><span class="header-section-number">6.8.3.1.3</span> Alternative Scenarios for: Use Query Builder</h5>
<pre><code>Scenario: Select a subset of columns in a table in Query builder
  Given the user is working in data browser context in Query builder
  And the full list of column names in the table is present
  When the user selects a subset of columns
  Then the table is displayed with selected columns and their values</code></pre>
<pre><code>Scenario: Filter a date column in a table in Query builder
  Given the user is working in data browser context in Query builder
  When the user selects a filter column with &lt;data type&gt;, &lt;filter operator&gt;, and &lt;filter value&gt;
  Then the table is displayed with selected columns and filter is applied
  # Can use a calendar pop-up for selecting date values - to discuss
  # User should be prompted to enter/select date value
  | data type | filter operator | filter value      |
  | date      | equals          | single date_value |
  | date      | between         | two date_values   |
  | date      | greater than    | single date_value |
  | date      | less than       | single date_value |</code></pre>
<pre><code>Scenario: Filter a numerical type column in a table in Query builder
  Given the user is working in data browser context in Query builder
  When the user selects a filter column with &lt;data type&gt;, &lt;filter operator&gt;, and &lt;filter value&gt;
  Then the table is displayed with selected columns and filter is applied
  # Numbers can be integer or decimal
  # Between - both numerical values are included
  | data type | filter operator          | filter value               |
  | number    | equals                   | single numerical value     |
  | number    | between                  | two numerical values       |
  | number    | greater than             | single numerical value     |
  | number    | greater than or equal to | single numerical value     |
  | number    | less than                | single numerical value     |
  | number    | less than or equal to    | single numerical value     |
  | number    | not equal to             | single numerical value     |
  | number    | in list                  | List of numerical value(s) |
  | number    | not in list              | List of numerical value(s) |</code></pre>
<pre><code>Scenario: Filter a string type column in a table in Query builder
  Given the user is working in data browser context in Query builder
  When the user selects a filter column with &lt;data type&gt;, &lt;filter operator&gt;, and &lt;filter value&gt;
  Then the table is displayed with selected columns and filter is applied
  # case sensitive
  | data type | filter operator | filter value      |
  | string    | equals          | string            |
  | string    | starts with     | string            |
  | string    | ends with       | string            |
  | string    | contains        | string            |
  | string    | not contains    | string            |
  | string    | is null         |                   |
  | string    | is not null     |                   |
  | string    | in list         | list of string(s) |
  | string    | not in list     | list of string(s) |</code></pre>
<pre><code>Scenario Outline: Build complex query with multiple filters in Query builder
  # complex query has multiple filters and filter groups
  Given the user is working in a data browser context in Query builder
  And the following table DM exists
  | STUDY_ID   | SITEID | USUBJID | AGE | AGE_UOM | WEIGHT | WEIGHT_UOM |
  | BROWSER001 | 001    | 0010001 | 20  | YEARS   | 110    | LBS        |
  | BROWSER001 | 001    | 0010002 | 30  | YEARS   | 150    | LBS        |
  | BROWSER001 | 001    | 0010003 | 40  | YEARS   | 155    | LBS        |
  | BROWSER001 | 002    | 0010004 | 50  | YEARS   | 165    | LBS        |
  | BROWSER001 | 002    | 0010005 | 35  | YEARS   | 125    | LBS        |
  And the following filter groups
  | filter group | group condition | filter                 |
  | group 1      | AND             | AGE &lt; 40, WEIGHT &lt; 140 |
  | group 2      | AND             | USUBJID = &#39;0010004&#39;    |
  | group 3      | OR              | AGE &lt; 40, WEIGHT &lt; 140 |
  When the user creates and runs a query with &lt;filter groups&gt; and linking group condition &lt;linking group condition&gt;
  # AND will be the default filter condition
  Then the data is displayed for &lt;USUBJID list&gt;
  Examples:
  | filter groups       | linking group condition | USUBJID list                       |
  | group 1             | not required            | 0010001, 0010005                   |
  | group 1 and group 2 | AND                     | 0010001, 0010004, 0010005          |
  | group 3             | not required            | 0010001, 0010002, 0010005          |
  | group 2 and group 3 | OR                      | 0010001, 0010002, 0010004, 0010005 |</code></pre>
<pre><code>Scenario Outline: Import query from a file
  Given the user is working in a &lt;context&gt; data browser context for a study
  And The JSON file has all required fields
  | Query_name | query            | query_description | source        | context_type | table_name |
  | IMP_2800   | Select * from DM | API Query         | Ingested Data | Study        | DM         |
  And the following table DM exists in Ingested Data
  | STUDY_ID   | SITEID | USUBJID |
  | BROWSER001 | 001    | 0010001 |
  | BROWSER001 | 001    | 0010002 |
  | BROWSER001 | 001    | 0010003 |
  | BROWSER001 | 001    | 0010004 |
  | BROWSER001 | 001    | 0010005 |
  When the query is imported to study
  Then the query is recorded with query_name as IMP_2800
  Examples:
  | context    |
  | User SDTM  |
  | Study SDTM |
  | Study      |</code></pre>
<h5 data-number="6.8.3.1.4" id="alternative-scenarios-for-invalid-query"><span class="header-section-number">6.8.3.1.4</span> Alternative Scenarios for: Invalid query</h5>
<pre><code>Scenario: Error message with a non-Select SQL query
  Given the user is working in a &lt;context&gt; data browser context
  When the user enters a query starting with key word other than SELECT
  Then a message indicating that it is not a browser query</code></pre>
<h5 data-number="6.8.3.1.5" id="alternative-scenarios-for-user-enters-invalid-query-name-non-unique-or-with-invalid-characters"><span class="header-section-number">6.8.3.1.5</span> Alternative Scenarios for: User enters invalid query name (non-unique or with invalid characters)</h5>
<pre><code>Scenario: Error message when query name is invalid
  Given the user is working in a data browser context
  When the user saves the query and provides an invalid query name
  # use existing rules for study ID cleansing
  Then the system displays an invalid character error message
  And the system does not save query</code></pre>
<h5 data-number="6.8.3.1.6" id="alternative-scenarios-for-fields-missing-in-imported-json-file"><span class="header-section-number">6.8.3.1.6</span> Alternative Scenarios for: Fields missing in imported JSON file</h5>
<h2 data-number="6.9" id="schedule-data-extraction"><span class="header-section-number">6.9</span> Schedule Data Extraction</h2>
<p>This is a feature built to allow clients extract the output files generated after <a href="#Run-Conversion">Run Conversion</a> has been executed from the Produce SDTM feature to their external system.</p>
<h3 data-number="6.9.1" id="base-flow---data-extraction"><span class="header-section-number">6.9.1</span> Base Flow - Data Extraction</h3>
<ol type="1">
<li>The user selects data extraction from the study info page.</li>
<li>The user selects to schedule run to schedule extraction.</li>
<li>In the schedule extraction page, the following details have to be filled:
<ul>
<li>Connection Name: A name to describe the extraction job</li>
<li>Host Name or IP Address: Address of the connecting host</li>
<li>Username: Name of the user</li>
<li>Target File Path: Path to the target location (external system)</li>
<li>Scheduling Details: All the timing details for when an extraction job should occur and its frequency</li>
</ul></li>
<li>The configuration details are saved when the user clicks Save.</li>
<li>The system extracts the generated XPT files for the given study to the target file path at the scheduled time.</li>
</ol>
<h3 data-number="6.9.2" id="alternatives-5"><span class="header-section-number">6.9.2</span> Alternatives</h3>
<h4 data-number="6.9.2.1" id="a---user-does-not-have-permission-to-extract"><span class="header-section-number">6.9.2.1</span> 2a - User does not have permission to extract</h4>
<ol type="1">
<li>Users without permission to access the schedule extraction page will not be able to schedule an extraction.</li>
</ol>
<h4 data-number="6.9.2.2" id="a---pauseresume-scheduling"><span class="header-section-number">6.9.2.2</span> 3a - Pause/Resume Scheduling</h4>
<ol type="1">
<li>The scheduled extraction job can be paused to suspend the extraction of files and resumed at any point when necessary.</li>
</ol>
<h3 data-number="6.9.3" id="feature-details-8"><span class="header-section-number">6.9.3</span> Feature Details</h3>
<h4 data-number="6.9.3.1" id="data-extraction"><span class="header-section-number">6.9.3.1</span> Data Extraction</h4>
<h5 data-number="6.9.3.1.1" id="base-flow-scenarios-for-data-extraction"><span class="header-section-number">6.9.3.1.1</span> Base Flow Scenarios for: Data Extraction</h5>
<pre><code>Scenario: Configuring a scheduled job to automate extraction of Study data via the API
  Given an extraction job is scheduled for a specified study with configuration details
  When the scheduled time is due
  Then the requested XPT files are exported to the target location</code></pre>
<pre><code>Scenario: Display schedule extraction button on study page
  Given an authorized user has logged into Intient clinical
  And has navigated to a given study not marked for archiving
  Then &quot;schedule extraction&quot; button is displayed</code></pre>
<pre><code>Scenario: Return previous saved configuration details for a given study
  Given an extraction job is scheduled for a specified study with configuration details
  And a user wants to edit the configuration form
  When schedule extraction is requested for a study with a saved configuration form
  Then the configuration form is returned with the saved details from prior save</code></pre>
<pre><code>Scenario: Update database and cloud schedule when configuration details are edited
  Given an extraction job is scheduled for a specified study with configuration details
  When the configuration form is edited and saved
  Then postgres database and cloud schedule are updated</code></pre>
<pre><code>Scenario: Pause Schedule extraction job
  Given the user has navigated to the schedule extraction configuration page
  And an extraction job has been scheduled
  When the user clicks on the &#39;pause scheduling&#39; button
  Then the scheduled extraction job is stopped and non-editable</code></pre>
<pre><code>Scenario: Resume Schedule extraction job
  Given the user has navigated to the schedule extraction configuration page
  And a scheduled extraction job has been stopped
  When the user clicks on the &#39;resume scheduling&#39; button
  Then the scheduled extraction job resumes</code></pre>
<pre><code>Scenario: Default scheduling details for Time Zone
  Given the user has navigated to the schedule extraction configuration page for a new study
  And the user fills out the schedule extraction configuration form
  Then a default time zone is pre-configured in the scheduling details</code></pre>
<h5 data-number="6.9.3.1.2" id="alternative-scenarios-for-download-locally"><span class="header-section-number">6.9.3.1.2</span> Alternative Scenarios for: Download locally</h5>
<pre><code>Scenario: Covered by &quot;Generate xpt at study level&quot;</code></pre>
<h5 data-number="6.9.3.1.3" id="alternative-scenarios-for-user-does-not-have-permission-to-extract"><span class="header-section-number">6.9.3.1.3</span> Alternative Scenarios for: User does not have permission to extract</h5>
<pre><code>Scenario: Covered by &quot;Prevent access to unauthorized users&quot;</code></pre>
<h5 data-number="6.9.3.1.4" id="alternative-scenarios-for-pauseresume-scheduling"><span class="header-section-number">6.9.3.1.4</span> Alternative Scenarios for: Pause/Resume Scheduling</h5>
<pre><code>Scenario: Schedule extraction pause
  Given an extraction job is scheduled for a specified study with configuration details
  When the schedule extraction is paused
  Then the scheduled extraction job is stopped</code></pre>
<pre><code>Scenario: Schedule extraction resume
  Given an extraction job is scheduled for a specified study with configuration details
  And the schedule extraction has been paused
  When the resume endpoint is invoked
  Then the scheduled extraction job resumes</code></pre>
<h2 data-number="6.10" id="on-demand-data-extraction"><span class="header-section-number">6.10</span> On-Demand Data Extraction</h2>
<p>To extract dataset from RAW,SDTM and Snapshot</p>
<h3 data-number="6.10.1" id="base-flow---data-extraction-on-demand"><span class="header-section-number">6.10.1</span> Base Flow - Data Extraction On Demand</h3>
<ol type="1">
<li>User selects Data Extraction</li>
<li>User selects On Demand<br />
</li>
<li>User selects datasets (RAW,Ingested,Snapshot)</li>
<li>User can choose to download locally or export to target system</li>
</ol>
<h3 data-number="6.10.2" id="feature-details-9"><span class="header-section-number">6.10.2</span> Feature Details</h3>
<h4 data-number="6.10.2.1" id="on-demand-data-extraction-1"><span class="header-section-number">6.10.2.1</span> On-Demand Data Extraction</h4>
<h5 data-number="6.10.2.1.1" id="base-flow-scenarios-for-data-extraction-on-demand"><span class="header-section-number">6.10.2.1.1</span> Base Flow Scenarios for: Data Extraction On Demand</h5>
<pre><code>Scenario Outline: Export data is requested
  Given study1 has multiple &lt;datasource&gt; datasource
  And multiple &lt;Tables&gt; tables
  And connection details are
  | Connection Name | Host Name or IP Address | User Name | Target File Path |
  | connection1     | blank@user.com          | user1     | .                |
  When export of data is requested in &lt;Format&gt; format for &lt;Tables&gt;
  Then data is exported to target location in &lt;Format&gt; format with one file per table
  And  Export Type is recorded as EXPORT
  And status is recorded as Success
  And export format is recorded as &lt;Format&gt;
  Examples:
  | Format | datasource | Tables         |
  | XPT    | RAW        | table1, table2 |
  | SAS    | SDTM       | table2         |</code></pre>
<pre><code>Scenario: Selection is enabled when type is Generate for Download
  Given study1 has datasources
  When file generation is requested
  Then a data record is added to the data extraction table
  And selection is enabled for files generated</code></pre>
<pre><code>Scenario Outline: Generate files for selected Data and record completion
  Given datasource &lt;datasource&gt; exist for Study1
  And datasource contain &lt;tables&gt; tables
  When file generation is requested for format &lt;Format&gt;
  Then status is in In Progress
  And the file generation is completed with Success Status
  Examples:
  | Format | datasource | tables |
  | XPT    | SNAPSHOTS  | table1 |
  | SAS    | SDTM       | table2 |</code></pre>
<pre><code>Scenario: All datasets related to study are shown
  Given user is on the On Demand page for Study1
  When user selects Select Datasets
  Then all &lt;datasources&gt; datasets and it&#39;s associated &lt;tables&gt; tables are shown in a tree view
  # Examples:
  # | datasources | tables         |
  # | Raw         | table1, table2 |
  # | SDTM        | table3         |
  # | Snapshot    | Snapshot 1     |</code></pre>
<pre><code>Scenario:  All datasets related to study are extracted
  Given user is on On Demand page for BROWSE001
  | Datasource | Workspace       | Table          | Snapshot name |
  | RAW        | STUDY_WORKSPACE | table1         |               |
  | SDTM       | STUDY_WORKSPACE | table1, table2 |               |
  | SDTM       | USER_WORKSPACE  | table1         |               |
  | SNAPSHOTS  | STUDY_WORKSPACE | table1         | SNAPSHOT1     |
  | SNAPSHOTS  | USER_WORKSPACE  | table1, table2 | SNAPSHOT1     |
  When Select Datasets is requested
  Then All datasources related to BROWSE001 are extracted
  | Datasource | Workspace       | Table          | Snapshot name |
  | RAW        | STUDY_WORKSPACE | table1         |               |
  | SDTM       | STUDY_WORKSPACE | table1, table2 |               |
  | SDTM       | USER_WORKSPACE  | table1         |               |
  | SNAPSHOTS  | STUDY_WORKSPACE | table1         | SNAPSHOT1     |
  | SNAPSHOTS  | USER_WORKSPACE  | table1, table2 | SNAPSHOT1     |</code></pre>
<pre><code>Scenario Outline: Record is added in grid view for Export/ Generate for download action
  Given datasource &lt;datasources&gt; is selected for study1
  And format is &lt;format&gt;
  When request type &lt;request type&gt; is selected
  Then record is added to the grid view for &lt;request type&gt; request type
  Examples:
  | datasources | format | request type          |
  | RAW         | XPT    | Export                |
  | SDTM        | SAS    | Generate for download |</code></pre>
<pre><code>Scenario Outline: Download specific Generated files for multiple selected Data
  Given a user navigates to the On Demand page
  And record with &lt;request type&gt; has &lt;status&gt; status for &lt;format&gt; format
  And record is selected
  When download is requested
  Then the download begins
  Examples:
  | request type          | status  | format |
  | Export                | Success | SAS    |
  | Generate for download | Success | XPT    |</code></pre>
<pre><code>Scenario: Selection is disabled when file generation for download fails
  When file generation for download fails
  Then status is displayed as Failed
  And selection is disabled for the data record that failed</code></pre>
<pre><code>Scenario: Tables selected for generate for download are partially successful for SAS format
  Given the BROWSE001 study contains
  | Datasource | Table name |
  | RAW        | table1     |
  | SDTM       | table_long |
  | SNAPSHOTS  | table3     |
  When Generate for download is requested with SAS format
  Then status is recorded for each table
  | Table name | status  | reason                                  |
  | table1     | Success |                                         |
  | table_long | Failed  | Table name has exceeded character limit |
  | table3     | Success |                                         |
  And overall status is Partial Success</code></pre>
<pre><code>Scenario: Files generated through Generate for Download shall expire on 7th day from requisition date
  Given the file is generated successfully 7 days before today for BROWSE001
  | dataset | Table name |
  | Raw     | table1     |
  Then the status of the generated file is recorded as Expired</code></pre>
<pre><code>Scenario: Only files generated successfully are available for download
  Given the selected tables for BROWSE001 are
  | Datasource | Table name |
  | RAW        | table1     |
  | RAW        | table2     |
  When Generate for download is requested
  And there is partial success of file generation
  | Table name | Status  |
  | table1     | Success |
  | table2     | Failed  |
  Then only table1 is available for download
  And table2 is not available for download</code></pre>
<h5 data-number="6.10.2.1.2" id="alternative-scenarios-for-export"><span class="header-section-number">6.10.2.1.2</span> Alternative Scenarios for: Export</h5>
<h5 data-number="6.10.2.1.3" id="alternative-scenarios-for-generate-for-download"><span class="header-section-number">6.10.2.1.3</span> Alternative Scenarios for: Generate for download</h5>
<h2 data-number="6.11" id="manual-study-registration"><span class="header-section-number">6.11</span> Manual Study Registration</h2>
<p>The Register Study use case is available with the Clinical Core Module.</p>
<h3 data-number="6.11.1" id="base-flow---register-study"><span class="header-section-number">6.11.1</span> Base Flow - Register Study</h3>
<ol type="1">
<li>User selects Study Registration.</li>
<li>User enters the <a href="#Study-Registration">Study Registration</a> data.</li>
<li>System verifies the Study ID to previously saved studies to ensure no duplication.</li>
<li>System saves the study and its data.</li>
</ol>
<h3 data-number="6.11.2" id="alternatives-6"><span class="header-section-number">6.11.2</span> Alternatives</h3>
<h4 data-number="6.11.2.1" id="a---user-lacks-permissions-to-register-a-study"><span class="header-section-number">6.11.2.1</span> 2a - User lacks permissions to register a study</h4>
<ol type="1">
<li>User is prevented from accessing the Study Registration screen due to role or permissions.</li>
</ol>
<h4 data-number="6.11.2.2" id="b---all-required-information-is-not-entered"><span class="header-section-number">6.11.2.2</span> 2b - All required information is not entered</h4>
<ol type="1">
<li>System detects that a required field is missing and displays an error message indicating missing fields.</li>
<li>Continue at step 2.</li>
</ol>
<h4 data-number="6.11.2.3" id="a---duplicate-study-id-detected"><span class="header-section-number">6.11.2.3</span> 3a - Duplicate Study ID detected</h4>
<ol type="1">
<li>System detects that the Study ID already exists and displays an error message.</li>
<li>Continue at step 2</li>
</ol>
<h4 data-number="6.11.2.4" id="a---user-attempts-to-bypass-ui-restrictions-to-register-a-study"><span class="header-section-number">6.11.2.4</span> 4a - User attempts to bypass UI restrictions to register a study</h4>
<ol type="1">
<li>The system rejects the attempt at registration because of a lack of user permissions.</li>
</ol>
<h3 data-number="6.11.3" id="feature-details-10"><span class="header-section-number">6.11.3</span> Feature Details</h3>
<h4 data-number="6.11.3.1" id="manual-study-registration-1"><span class="header-section-number">6.11.3.1</span> Manual Study Registration</h4>
<h5 data-number="6.11.3.1.1" id="base-flow-scenarios-for-register-study"><span class="header-section-number">6.11.3.1.1</span> Base Flow Scenarios for: Register Study</h5>
<pre><code>Scenario: Search for studies in a list
  Given a study list
  When a search is executed against the study list
  Then only records matching the Study ID, title, type, notes, status, phase or description are returned</code></pre>
<pre><code>Scenario Outline: Record the provided name exactly as typed during registration
  When a &lt;entity&gt; is registered
  Then the &lt;entity&gt; label should be recorded exactly as provided
  # And no conflicts should be created with a previously registered &lt;entity&gt; with respect to the NFR naming conventions
  Examples:
  | entity            |
  # | Provider          |
  | Study             |
  | Analytics Project |</code></pre>
<pre><code>Scenario: View Clinical home page menu
  When a user navigates to the Clinical Home page
  Then the user sees the Study List table
  And there is a set of navigation elements across the top of the page in the following order with the text
  | Home              |
  | Ingest Data       |
  | Analyze &amp; Report  |
  | Administer Access |
  | Tasks             |
  | Help              |
  # Note - any section without a definition can be blank or have TBD on it or anything inoffensive, really
  And selecting a navigation element displays the corresponding section</code></pre>
<pre><code>Scenario: Navigate to the study details page from the study list page
  Given a user is on the Study List page
  When the user selects a Study ID
  Then the user is brought to Study Info</code></pre>
<pre><code>Scenario: Compound field details from the study registration screen
  Given a user is on the Study registration page
  When the user selects the compound field
  Then the drop down list is empty and the field is not mandatory</code></pre>
<pre><code>Scenario: Therapeutic Area options for study registration
  When the list of Therapeutic Areas is requested
  Then the following list of Therapeutic Areas is returned
  | Autoimmune       |
  | Cardiovascular   |
  | Endocrine        |
  | Gastrointestinal |
  | Infectious       |
  | Mental Health    |
  | Neurology        |
  | Oncology         |
  | Other            |
  | Rare Diseases    |
  | Respiratory      |
  | Treatments       |</code></pre>
<pre><code>Scenario: Phase options for study registration
  When the list of Phases is requested
  Then the following list of Phases is returned
  | Phase I   |
  | Phase II  |
  | Phase III |
  | Phase IV  |
  | Other     |</code></pre>
<h5 data-number="6.11.3.1.2" id="alternative-scenarios-for-duplicate-study-id-detected"><span class="header-section-number">6.11.3.1.2</span> Alternative Scenarios for: Duplicate Study ID detected</h5>
<pre><code>Scenario Outline: Registration fails if the conformed name of an entity conflicts
  # Note that the specific labels used for testing here align to the NFR naming conventions expectations. If those change,
  # this should be updated
  Given an &lt;entity&gt; is already registered with a label of Test$Entity
  When a new &lt;entity&gt; is registered with the name Test#Entity
  Then registration fails
  And there is an error noting that an &lt;entity&gt; with that (conformed) name already exists
  Examples:
  | entity            |
  # | Provider          |
  | Study             |
  | Analytics Project |
  # Alternatives for: Manual Study Registration</code></pre>
<h5 data-number="6.11.3.1.3" id="alternative-scenarios-for-all-required-information-is-not-entered"><span class="header-section-number">6.11.3.1.3</span> Alternative Scenarios for: All required information is not entered</h5>
<pre><code>Scenario Outline: Failed study registration leaves the user on the Registration screen
  Given a user is on the Registration screen
  When the study registration fails because &lt;Reason&gt;
  Then the user remains on the Registration screen
  And an appropriate error message is displayed because &lt;Reason&gt;
  Examples:
  | Reason                             |
  | a required field is not filled out |
  | the study is already registered    |</code></pre>
<h5 data-number="6.11.3.1.4" id="alternative-scenarios-for-user-lacks-permissions-to-register-a-study"><span class="header-section-number">6.11.3.1.4</span> Alternative Scenarios for: User lacks permissions to register a study</h5>
<pre><code>Scenario: Covered by scenario &quot;Disable the Register Study functionality for Batch Registration Deployment&quot;</code></pre>
<h5 data-number="6.11.3.1.5" id="alternative-scenarios-for-user-attempts-to-bypass-ui-restrictions-to-register-a-study"><span class="header-section-number">6.11.3.1.5</span> Alternative Scenarios for: User attempts to bypass UI restrictions to register a study</h5>
<pre><code>Scenario: Covered by scenario &quot;Unauthorized API access attempts are denied&quot;
  # Rule: Administrators can control allowed data entry values</code></pre>
<h3 data-number="6.11.4" id="study-registration"><span class="header-section-number">6.11.4</span> Study Registration</h3>
<p>To register a Study in the system the required Study Registration data must be provided.</p>
<ol type="1">
<li>Study Data is associated to a Registration Provider</li>
</ol>
<ul>
<li>Key Values must be established upon implementation to enable study registration. Mandatory client specific list of values (LOVs) for Therapeutic Area, Drug Program and Development Phase must be established</li>
</ul>
<ol start="3" type="1">
<li>CSV files are ingested via SFTP to enable dynamic study registration</li>
</ol>
<ul>
<li>Registration data via SFTP and associated Provider of Registration data</li>
</ul>
<ol start="2" type="1">
<li>Data is provided in a CSV format for ingestion</li>
</ol>
<ul>
<li>Study ID</li>
<li>Study Title</li>
<li>Study Description</li>
<li>Therapeutic Area</li>
<li>Drug Program</li>
<li>Development Phase</li>
<li>Study Start Date</li>
<li>Study End Date</li>
<li>Archive Only flag</li>
<li>Blinded flag</li>
<li>Real Time Data flag</li>
<li>User Intient Firestore flag</li>
</ul>
<h2 data-number="6.12" id="batch-study-registration"><span class="header-section-number">6.12</span> Batch Study Registration</h2>
<p>The Register Study use case is available with the Clinical Core Module. Users have the ability to register a study via an integration with their organization’s systems (Automatic) or manually using the web interface. An organization’s environment is configured to have either automatic Study Registration or Manual Study registration. The Register Study Navigation will be removed if Batch Registration is deployed.</p>
<h3 data-number="6.12.1" id="base-flow---batch-study-registration"><span class="header-section-number">6.12.1</span> Base Flow - Batch Study Registration</h3>
<ol type="1">
<li>The system receives study registration files</li>
<li>The system initiates the ingestion process for the study registration files</li>
<li>The system captures any new or updated information and records it appropriately while maintaining an audit trail.</li>
<li>The system automatically registers all studies received</li>
</ol>
<h3 data-number="6.12.2" id="alternatives-7"><span class="header-section-number">6.12.2</span> Alternatives</h3>
<h4 data-number="6.12.2.1" id="a---multiple-registration-providers"><span class="header-section-number">6.12.2.1</span> 1a - Multiple Registration Providers</h4>
<ol type="1">
<li>More than one Registration Provider per instance is providing Registration data</li>
<li>Each Registration Provider must be manually mapped prior to ingestion of Registration data</li>
</ol>
<h4 data-number="6.12.2.2" id="a---incorrect-information-in-the-system-of-record"><span class="header-section-number">6.12.2.2</span> 3a - Incorrect information in the system of record</h4>
<ol type="1">
<li>Conflicting information in the system of record is detected, e.g., a unique identifier is found twice.</li>
<li>The system skips adding/updating records for the problematic entries.</li>
<li>The system logs the problem and sends a notification with details of the issue.</li>
</ol>
<h3 data-number="6.12.3" id="feature-details-11"><span class="header-section-number">6.12.3</span> Feature Details</h3>
<h4 data-number="6.12.3.1" id="batch-study-registration-1"><span class="header-section-number">6.12.3.1</span> Batch Study Registration</h4>
<h5 data-number="6.12.3.1.1" id="base-flow-scenarios-for-batch-study-registration"><span class="header-section-number">6.12.3.1.1</span> Base Flow Scenarios for: Batch Study Registration</h5>
<pre><code>Scenario: Initial ingestion of a Study Registration CSV file
  Given Batch Study Registration is enabled for the client
  And a therapeutic_area batch register table contains
  | THERAPEUTIC_AREA_NAME      |
  | BEHAVE_TEST_Oncology       |
  | BEHAVE_TEST_Cardiovascular |
  And a config_controlled_term batch register table contains
  | CONTROLLED_TERM_NAME | CONTROLLED_TERM_VALUE |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_IV        |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_I         |
  And a drug_program batch register table contains
  | DRUG_PROGRAM_CODE   |
  | BEHAVE_TEST_78C0039 |
  And a valid study Batch Study Registration file contains
  | STUDY_ID           | STUDY_TITLE           | STUDY_DESCRIPTION        | THERAPEUTIC_AREA_NAME      | DEVELOPMENT_PHASE | DRUG_PROGRAM_CODE   | is_Blinded | is_Archive_only |
  | BEHAVE_TEST_ABC123 | ABC123 Study Overview | ABC123 Study Description | BEHAVE_TEST_Oncology       | BEHAVE_TEST_IV    | BEHAVE_TEST_78C0039 | Yes        | No              |
  | BEHAVE_TEST_ABC456 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_I     | BEHAVE_TEST_78C0039 | Yes        | No              |
  When the batch registration file is loaded
  Then the following study table contains the following
  | STUDY_ID           | STUDY_TITLE           | STUDY_DESCRIPTION        | THERAPEUTIC_AREA_NAME      | DEVELOPMENT_PHASE | DRUG_PROGRAM_CODE   | is_Blinded | is_Archive_only |
  | BEHAVE_TEST_ABC123 | ABC123 Study Overview | ABC123 Study Description | BEHAVE_TEST_Oncology       | BEHAVE_TEST_IV    | BEHAVE_TEST_78C0039 | Yes        | No              |
  | BEHAVE_TEST_ABC456 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_I     | BEHAVE_TEST_78C0039 | Yes        | No              |</code></pre>
<pre><code>Scenario: Initial ingestion of a Therapeutic Area via a CSV file
  Given Batch Study Registration is enabled for the client
  And a valid therapeutic_area Batch Study Registration file contains
  | THERAPEUTIC_AREA_NAME      | THERAPEUTIC_AREA_DESCRIPTION  |
  | BEHAVE_TEST_Oncology       | Description of Oncology       |
  | BEHAVE_TEST_Cardiovascular | Description of Cardiovascular |
  When the batch registration file is loaded
  Then the following therapeutic_area table contains the following
  | THERAPEUTIC_AREA_NAME      | THERAPEUTIC_AREA_DESCRIPTION  |
  | BEHAVE_TEST_Oncology       | Description of Oncology       |
  | BEHAVE_TEST_Cardiovascular | Description of Cardiovascular |</code></pre>
<pre><code>Scenario: Initial ingestion of a Drug Program via a CSV file
  Given Batch Study Registration is enabled for the client
  And a valid drug_program Batch Study Registration file contains
  | DRUG_PROGRAM_CODE   |
  | BEHAVE_TEST_78C0039 |
  | BEHAVE_TEST_78C0040 |
  When the batch registration file is loaded
  Then the following drug_program table contains the following
  | DRUG_PROGRAM_CODE   |
  | BEHAVE_TEST_78C0039 |
  | BEHAVE_TEST_78C0040 |</code></pre>
<pre><code>Scenario: Initial ingestion of a Development Phase via a CSV file
  Given Batch Study Registration is enabled for the client
  And a valid development_phase Batch Study Registration file contains
  | DEVELOPMENT_PHASE    |
  | BEHAVE_TEST_Phase I  |
  | BEHAVE_TEST_Phase II |
  When the batch registration file is loaded
  Then the following Config Control Term table contains the following
  | CONTROLLED_TERM_NAME | CONTROLLED_TERM_VALUE |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_Phase I   |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_Phase II  |</code></pre>
<pre><code>Scenario: Updates to development phase via CSV batch processing
  Given Batch Study Registration is enabled for the client
  And Manual register study has been disabled
  And Initial ingestion of client config_controlled_term data exists
  | CONTROLLED_TERM_NAME | CONTROLLED_TERM_VALUE |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_Phase I   |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_Phase II  |
  And a valid CSV Batch development_phase file contains
  | DEVELOPMENT_PHASE     |
  | BEHAVE_TEST_Phase III |
  | BEHAVE_TEST_Phase IV  |
  When the batch development_phase file is loaded
  Then the config_controlled_term values contains
  | CONTROLLED_TERM_NAME | CONTROLLED_TERM_VALUE |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_Phase I   |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_Phase II  |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_Phase III |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_Phase IV  |</code></pre>
<pre><code>Scenario: Updates to therapeutic area via CSV batch processing
  Given Batch Study Registration is enabled for the client
  And Manual register study has been disabled
  And Initial ingestion of client therapeutic_area data exists
  | THERAPEUTIC_AREA_NAME      | THERAPEUTIC_AREA_DESCRIPTION  |
  | BEHAVE_TEST_Oncology       | Description of Oncology       |
  | BEHAVE_TEST_Cardiovascular | Description of Cardiovascular |
  And a valid CSV Batch therapeutic_area file contains
  # Change in Description and add new value
  | THERAPEUTIC_AREA_NAME      | THERAPEUTIC_AREA_DESCRIPTION  |
  | BEHAVE_TEST_Oncology       | Description of New Oncology   |
  | BEHAVE_TEST_Cardiovascular | Description of Cardiovascular |
  | BEHAVE_TEST_Autoimmune     | Description of Autoimmune     |
  When the batch therapeutic_area file is loaded
  Then the therapeutic_area values contains
  # Inserts and Updates if values exist
  | THERAPEUTIC_AREA_NAME      | THERAPEUTIC_AREA_DESCRIPTION  |
  | BEHAVE_TEST_Oncology       | Description of New Oncology   |
  | BEHAVE_TEST_Cardiovascular | Description of Cardiovascular |
  | BEHAVE_TEST_Autoimmune     | Description of Autoimmune     |</code></pre>
<pre><code>Scenario: Updates to Drug Program via CSV batch processing
  Given Batch Study Registration is enabled for the client
  And Manual register study has been disabled
  And Initial ingestion of client drug_program data exists
  | DRUG_PROGRAM_CODE   |
  | BEHAVE_TEST_78C0038 |
  | BEHAVE_TEST_78C0039 |
  And a valid CSV Batch drug_program file contains
  | DRUG_PROGRAM_CODE   |
  | BEHAVE_TEST_78C0038 |
  | BEHAVE_TEST_78C0039 |
  | BEHAVE_TEST_78C0040 |
  When the batch drug_program file is loaded
  Then the drug_program values contains
  # Inserts Only
  | DRUG_PROGRAM_CODE   |
  | BEHAVE_TEST_78C0038 |
  | BEHAVE_TEST_78C0039 |
  | BEHAVE_TEST_78C0040 |</code></pre>
<pre><code>Scenario: An existing Study with new study attributes added via a CSV file for Development Phase
  Given Batch Study Registration is enabled for the client
  And Manual register study has been disabled
  And a therapeutic_area batch register table contains
  | THERAPEUTIC_AREA_NAME      |
  | BEHAVE_TEST_Oncology       |
  | BEHAVE_TEST_Cardiovascular |
  And a config_controlled_term batch register table contains
  | CONTROLLED_TERM_NAME | CONTROLLED_TERM_VALUE |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_IV        |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_I         |
  And a drug_program batch register table contains
  | DRUG_PROGRAM_CODE   |
  | BEHAVE_TEST_78C0039 |
  And a valid study Batch Study Registration file contains
  | STUDY_ID           | STUDY_TITLE           | STUDY_DESCRIPTION        | THERAPEUTIC_AREA_NAME      | DEVELOPMENT_PHASE | DRUG_PROGRAM_CODE   | is_Blinded | is_Archive_only |
  | BEHAVE_TEST_ABC123 | ABC123 Study Overview | ABC123 Study Description | BEHAVE_TEST_Oncology       | BEHAVE_TEST_IV    | BEHAVE_TEST_78C0038 | Yes        | No              |
  | BEHAVE_TEST_ABC456 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_I     | BEHAVE_TEST_78C0039 | Yes        | No              |
  | BEHAVE_TEST_ABC457 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_III   | BEHAVE_TEST_78C0040 | Yes        | No              |
  # When the data is loaded
  When the batch registration file is loaded
  Then the following study table contains the following
  | STUDY_ID           | STUDY_TITLE           | STUDY_DESCRIPTION        | THERAPEUTIC_AREA_NAME      | DEVELOPMENT_PHASE | DRUG_PROGRAM_CODE   | is_Blinded | is_Archive_only |
  | BEHAVE_TEST_ABC123 | ABC123 Study Overview | ABC123 Study Description | BEHAVE_TEST_Oncology       | BEHAVE_TEST_IV    | BEHAVE_TEST_78C0038 | Yes        | No              |
  | BEHAVE_TEST_ABC456 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_I     | BEHAVE_TEST_78C0039 | Yes        | No              |
  | BEHAVE_TEST_ABC457 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_III   | BEHAVE_TEST_78C0040 | Yes        | No              |</code></pre>
<pre><code>Scenario: An existing Study with new study attributes added via a CSV file for Drug Program
  Given Batch Study Registration is enabled for the client
  And Manual register study has been disabled
  And a therapeutic_area batch register table contains
  | THERAPEUTIC_AREA_NAME      |
  | BEHAVE_TEST_Oncology       |
  | BEHAVE_TEST_Cardiovascular |
  And a config_controlled_term batch register table contains
  | CONTROLLED_TERM_NAME | CONTROLLED_TERM_VALUE |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_IV        |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_I         |
  And a drug_program batch register table contains
  | DRUG_PROGRAM_CODE   |
  | BEHAVE_TEST_78C0039 |
  And a valid study Batch Study Registration file contains
  | STUDY_ID           | STUDY_TITLE           | STUDY_DESCRIPTION        | THERAPEUTIC_AREA_NAME      | DEVELOPMENT_PHASE | DRUG_PROGRAM_CODE   | is_Blinded | is_Archive_only |
  | BEHAVE_TEST_ABC123 | ABC123 Study Overview | ABC123 Study Description | BEHAVE_TEST_Oncology       | BEHAVE_TEST_IV    | BEHAVE_TEST_78C0038 | Yes        | No              |
  | BEHAVE_TEST_ABC456 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_I     | BEHAVE_TEST_78C0039 | Yes        | No              |
  | BEHAVE_TEST_ABC457 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_III   | BEHAVE_TEST_78C0044 | Yes        | No              |
  # When the data is loaded
  When the batch registration file is loaded
  Then the following study table contains the following
  | STUDY_ID           | STUDY_TITLE           | STUDY_DESCRIPTION        | THERAPEUTIC_AREA_NAME      | DEVELOPMENT_PHASE | DRUG_PROGRAM_CODE   | is_Blinded | is_Archive_only |
  | BEHAVE_TEST_ABC123 | ABC123 Study Overview | ABC123 Study Description | BEHAVE_TEST_Oncology       | BEHAVE_TEST_IV    | BEHAVE_TEST_78C0038 | Yes        | No              |
  | BEHAVE_TEST_ABC456 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_I     | BEHAVE_TEST_78C0039 | Yes        | No              |
  | BEHAVE_TEST_ABC457 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_III   | BEHAVE_TEST_78C0044 | Yes        | No              |</code></pre>
<pre><code>Scenario: An existing Study with new study attributes added via a CSV file for Therapeutic Area
  Given Batch Study Registration is enabled for the client
  And Manual register study has been disabled
  And a therapeutic_area batch register table contains
  | THERAPEUTIC_AREA_NAME      |
  | BEHAVE_TEST_Oncology       |
  | BEHAVE_TEST_Cardiovascular |
  And a config_controlled_term batch register table contains
  | CONTROLLED_TERM_NAME | CONTROLLED_TERM_VALUE |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_IV        |
  | DEVELOPMENT PHASE    | BEHAVE_TEST_I         |
  And a drug_program batch register table contains
  | DRUG_PROGRAM_CODE   |
  | BEHAVE_TEST_78C0039 |
  And a valid study Batch Study Registration file contains
  | STUDY_ID           | STUDY_TITLE           | STUDY_DESCRIPTION        | THERAPEUTIC_AREA_NAME      | DEVELOPMENT_PHASE | DRUG_PROGRAM_CODE   | is_Blinded | is_Archive_only |
  | BEHAVE_TEST_ABC123 | ABC123 Study Overview | ABC123 Study Description | BEHAVE_TEST_Oncology       | BEHAVE_TEST_IV    | BEHAVE_TEST_78C0038 | Yes        | No              |
  | BEHAVE_TEST_ABC456 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_I     | BEHAVE_TEST_78C0039 | Yes        | No              |
  | BEHAVE_TEST_ABC457 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Autoimmune     | BEHAVE_TEST_III   | BEHAVE_TEST_78C0044 | Yes        | No              |
  # When the data is loaded
  When the batch registration file is loaded
  Then the following study table contains the following
  | STUDY_ID           | STUDY_TITLE           | STUDY_DESCRIPTION        | THERAPEUTIC_AREA_NAME      | DEVELOPMENT_PHASE | DRUG_PROGRAM_CODE   | is_Blinded | is_Archive_only |
  | BEHAVE_TEST_ABC123 | ABC123 Study Overview | ABC123 Study Description | BEHAVE_TEST_Oncology       | BEHAVE_TEST_IV    | BEHAVE_TEST_78C0038 | Yes        | No              |
  | BEHAVE_TEST_ABC456 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Cardiovascular | BEHAVE_TEST_I     | BEHAVE_TEST_78C0039 | Yes        | No              |
  | BEHAVE_TEST_ABC457 | ABC456 Study Overview | ABC456 Study Description | BEHAVE_TEST_Autoimmune     | BEHAVE_TEST_III   | BEHAVE_TEST_78C0044 | Yes        | No              |</code></pre>
<pre><code>Scenario: Disable the Register Study functionality for Batch Registration Deployment
  Given an organization has access to Batch Registration
  When a user attempts to access study registration feature
  Then the access to manual study registration is denied</code></pre>
<pre><code>Scenario: Update the provider to accept data category of registration and provision source folder in GCS
  Given a Data Provider name
  And a category of Registration
  When a user creates a Registration Provider
  Then the Registration folder is created in GCS for newly created provider
  And the registration data connection is created for newly created provider</code></pre>
<h5 data-number="6.12.3.1.2" id="alternative-scenarios-for-multiple-registration-providers"><span class="header-section-number">6.12.3.1.2</span> Alternative Scenarios for: Multiple Registration Providers</h5>
<pre><code>Scenario: Covered by Base Flow use cases</code></pre>
<h5 data-number="6.12.3.1.3" id="alternative-scenarios-for-incorrect-information-in-the-system-of-record"><span class="header-section-number">6.12.3.1.3</span> Alternative Scenarios for: Incorrect information in the system of record</h5>
<pre><code>Scenario: Covered by Base Flow use cases</code></pre>
<h2 data-number="6.13" id="clinical-insights"><span class="header-section-number">6.13</span> Clinical Insights</h2>
<p>The Clinical Insights module is accessible to Operational Data Analyst.</p>
<h3 data-number="6.13.1" id="base-flow---visualize-real-time-study-data"><span class="header-section-number">6.13.1</span> Base Flow - Visualize Real Time Study Data</h3>
<ol type="1">
<li>Create and monitor real time studies.</li>
<li>Access standard compliance dashboards.</li>
<li>Create self serve custom dashboards for the real time studies</li>
</ol>
<p>The following is the user role that has access to this use case:</p>
<ul>
<li>System Administrator</li>
<li>Study Administrator</li>
<li>Operational Data Analyst</li>
<li>Study Data Analyst</li>
<li>Study Data Lead</li>
</ul>
<h3 data-number="6.13.2" id="feature-details-12"><span class="header-section-number">6.13.2</span> Feature Details</h3>
<h4 data-number="6.13.2.1" id="navigate-to-clinical-studies-clinical-insights"><span class="header-section-number">6.13.2.1</span> Navigate to Clinical Studies-Clinical Insights</h4>
<h5 data-number="6.13.2.1.1" id="base-flow-scenarios-for-visualize-real-time-study-data"><span class="header-section-number">6.13.2.1.1</span> Base Flow Scenarios for: Visualize Real Time Study Data</h5>
<pre><code>Scenario: Access denied for user to navigate to clinops studies
  Given the application &quot;Intient Clinical&quot;
  And User does not have the role Operational Data Analyst
  Then &quot;Clinops Studies&quot; link not available in the left menu bar</code></pre>
<pre><code>Scenario: Monitor New Study - Navigate to Monitor Study
  Given the application &quot;Intient Clinical&quot;
  And User logs in with the role Operational Data Analyst
  When user clicks on Monitor New Study button
  And user redirected to studies list on Monitor New Study page</code></pre>
<pre><code>Scenario: User able to navigate to clinops studies
  Given the application &quot;Intient Clinical&quot;
  And User logs in with the role Operational Data Analyst
  Then &quot;Clinops Studies&quot; link available in the left menu bar</code></pre>
<pre><code>Scenario: Allow registration of a clinical study from study registration with FHIR data available checked
  Given the user navigates to the Study registration screen
  Then User enters study registration information along with FHIR Data Available checkbox checked
  When the user click on the submit button on Study registration
  Then Study registration success message is displayed</code></pre>
<pre><code>Scenario: Provide navigation from study-list directly to clinops study dashboard page
  Given a system administrator or study administrator user is viewing the study-list
  When the user selects an action on a study of assign user
  Then the user is redirected to the clinops Study dashboard page
  And the Selected Study dashboard is displayed</code></pre>
<pre><code>Scenario: Access Self Serve Explorer
  Given the application &quot;Clinical&quot;
  And User logs in with their credentials
  And User is on the clinical insights landing page
  And User select a study from the landing page
  When user selects the Self Serve Explorer
  Then the looker IFrame will redirect to the self serve explorer page</code></pre>
<pre><code>Scenario: Access Custom Reports - View custom report list
  Given the application &quot;Clinical&quot;
  And User logs in with their credentials
  And User is on the clinical insights landing page
  And user selects a study from dashboard to access Custom Reports
  When user clicks Custom Reports
  Then iFrame will redirect user to custom reports list</code></pre>
<pre><code>Scenario: Access Custom Reports – View Custom Report
  Given the application &quot;Clinical&quot;
  And User logs in with their credentials
  And User is on the clinical insights landing page
  And user selects a study from dashboard to access Custom Reports
  And user is on the Custom Reports list page of a study
  When user clicks on a report name from the list
  Then user will be redirected to the reports page
  And user will be able to view the custom report</code></pre>
<pre><code>Scenario: Access Custom Reports - Delete Report
  Given the application &quot;Clinical&quot;
  And User logs in with their credentials
  And User is on the clinical insights landing page
  And user selects a study from dashboard to access Custom Reports
  And user is on the Custom Reports list page of a study
  When user clicks on the delete icon of a report
  Then user will have the report deleted from the custom reports dashboard</code></pre>
<pre><code>Scenario: Access Compliance Dashboard
  Given the application &quot;Clinical&quot;
  And User logs in with their credentials
  And User is on the clinical insights landing page
  And user selects a study from dashboard to access Compliance Reports
  When user selects the Compliance Dashboard
  Then Compliance dashboard reports should be visible in the Looker iFrame</code></pre>
<h5 data-number="6.13.2.1.2" id="base-flow-scenarios-for-visualize-real-time-study-data-1"><span class="header-section-number">6.13.2.1.2</span> Base Flow Scenarios for: Visualize Real Time Study Data</h5>
<pre><code>Scenario Outline: Monitor study table shows studies that user has access with FHIR flag as &quot;True&quot;
  Given a user with an assigned role of the &lt;Role_name&gt;
  When a request is made to create a study
  Then the user has granted access to the study
  When a request is made for clinops studies that can be monitored
  Then Study information is available in the response
  Examples:
  | Role_name              |
  | Operations Data Analyst|</code></pre>
<pre><code>Scenario: Allow registration of a clinical study from study registration with FHIR data available
  Given a user assigned role Operations Data Analyst
  When a request is made to register a clinical study with FHIR available as True
  Then Study registered successfully
  Then FHIR_STORE has the study creation message
  And the study information is recorded
  |content     |
  |description |
  |id          |
  |note        |
  |resourceType|
  |title       |
  |status      |</code></pre>
<pre><code>Scenario: Start monitoring a study
  Given a request is made to start monitoring a study
  Then FHIR deidentification store is created for the Study
  And  Folders for data visualization are created
  And  Dataset is created in BQ for the FHIR data in the format of instance_studyname_fhir_data
  And the study information is displayed
  |bq_dataset_id          |
  |comp_folder_id         |
  |configuration_id       |
  |configuration_id_helper|
  |cust_folder_id         |
  |de_fhir_store_id       |
  |description            |
  |end_date               |
  |fhir_store_id          |
  |folder_id              |
  |job_scheduler_id       |
  |name                   |
  |process_config_id      |
  |purpose                |
  |self_folder_id         |
  |status                 |
  |study_id               |
  |version                |
  |group_id               |
  |external_group_id      |</code></pre>
<h2 data-number="6.14" id="operational-insights"><span class="header-section-number">6.14</span> Operational Insights</h2>
<p>The Operational Insights module is accessible to Operational Data Analyst.</p>
<h3 data-number="6.14.1" id="base-flow---operational-insights"><span class="header-section-number">6.14.1</span> Base Flow - Operational Insights</h3>
<ol type="1">
<li>Using selects a study based on therapeutic area, drug compound and partner allocated</li>
<li>User selects dashboard from the list of dashboards available</li>
<li>Dashboard data is displayed</li>
</ol>
<p>The following is the user role that has access to this use case:</p>
<ul>
<li>Portfolio Lead</li>
<li>Project Lead</li>
<li>Operational Data Analyst</li>
</ul>
<h3 data-number="6.14.2" id="alternatives-8"><span class="header-section-number">6.14.2</span> Alternatives</h3>
<h4 data-number="6.14.2.1" id="a---threshold-settings"><span class="header-section-number">6.14.2.1</span> 3a - Threshold settings</h4>
<ol type="1">
<li>User can modify the thresholds for Portfolio - Overview</li>
<li>User can modify the thresholds for Study - Study Performance</li>
</ol>
<h3 data-number="6.14.3" id="feature-details-13"><span class="header-section-number">6.14.3</span> Feature Details</h3>
<h4 data-number="6.14.3.1" id="navigate-to-clinical-studies-operational-insights"><span class="header-section-number">6.14.3.1</span> Navigate to Clinical Studies-Operational Insights</h4>
<h5 data-number="6.14.3.1.1" id="base-flow-scenarios-for-operational-insights"><span class="header-section-number">6.14.3.1.1</span> Base Flow Scenarios for: Operational Insights</h5>
<pre><code>Scenario: View the Operational Insights section
  When the user is on the Operational Insights section
  Then there is a navigation element that allows navigation to the following areas in the order given
  | Portfolio   |
  | Study       |
  | Startup     |
  | Conduct     |
  | Cross Stage |
  And the dashboards under the above groups are accessible</code></pre>
<pre><code>Scenario: Study global filter application
  When the user is on the Operational Insights section
  And the user has selected JAM007
  Then all remaining fields are auto-populated
  And all dashboards are showing details for JAM007</code></pre>
<pre><code>Scenario: Therapeutic Area global filter application
  When the user is on the Operational Insights section
  And the user has selected Therapeutic Area Oncology
  Then studies, drug compound values are where Oncology is applicable</code></pre>
<pre><code>Scenario Outline:Dashboard are enabled/disabled based on number of studies selected
  Given the user is in Operational Insights view
  When the study selected is &lt;Study&gt;
  Then dashboard selection is enabled based on number of studies selected
  Examples:
  | Study              | Dashboard enabled         |
  | R13STD002          | All                       |
  | R13STD002 , JAM007 | Only ones under Portfolio |</code></pre>
<pre><code>Scenario Outline:Embedded URL is called to populate the iframe
  Given the study &lt;Study&gt; is assigned
  When &lt;Dashboard&gt; dashboard is requested
  Then an embedded URL is called for &lt;Dashboard ID&gt;
  And &lt;Dashboard&gt; dashboard is populated
  Examples:
  | Study     | Dashboard          | Dashboard ID                 |
  | R13STD002 | Study Overview     | clinical::study_overview     |
  | R13STD002 | Portfolio Overview | clinical::portfolio_overview |</code></pre>
<pre><code>Scenario: Api call to fetch dashboard ids
  Given the Dashboards are configured
  | Dashboard ID                 |
  | clinical::study_performance  |
  | clinical::protocol_deviation |
  When the Dashboard details are requested
  Then the Dashboard details are fetched
  | Dashboard          | Dashboard ID                 |
  | Study Performance  | clinical::study_performance  |
  | Protocol Deviation | clinical::protocol_deviation |</code></pre>
<pre><code>Scenario: Studies shown are based on Partner allocated selection
  Given the user has R13STD002 assigned
  And is on the Operational Insights page
  When user selects R13CRO as Partner Allocated
  Then study dropdown has R13STD002 study</code></pre>
<pre><code>Scenario: Return to study list from study details
  Given a user is on the Study Details page for a study
  When the user selects the Home button
  Then the user is brought to the Study List page</code></pre>
<h5 data-number="6.14.3.1.2" id="alternative-scenarios-for-threshold-settings"><span class="header-section-number">6.14.3.1.2</span> Alternative Scenarios for: Threshold settings</h5>
<pre><code>Scenario Outline: Config setting is seen for applicable dashboards
  Given the user is in Operational Insights page
  When the user selects &lt;Dashboard&gt; dashboard
  Then Threshold settings is visible on the Operational Insights page
  Examples:
  | Dashboard          |
  | Portfolio Overview |
  | Study Performance  |</code></pre>
<pre><code>Scenario: Configuration settings are saved to personal config
  Given the user has Study in Operational Insights
  And the Threshold Setting is as following for default config
  | METRIC_NAME         | UPPER_THRESHOLD | LOWER_THRESHOLD | CATEGORY          |
  | Site Activation     | 90              | 80              | STUDY PERFORMANCE |
  | Subjects Enrolled   | 90              | 80              | STUDY PERFORMANCE |
  | Screen Failure Rate | 20              | 30              | STUDY PERFORMANCE |
  When the thresholds are modified as following for personal config
  | METRIC_NAME         | UPPER_THRESHOLD | LOWER_THRESHOLD | CATEGORY          |
  | Site Activation     | 85              | 70              | STUDY PERFORMANCE |
  | Subjects Enrolled   | 70              | 50              | STUDY PERFORMANCE |
  | Screen Failure Rate | 5               | 15              | STUDY PERFORMANCE |
  Then the Thresholds are saved to the personal config</code></pre>
<pre><code>Scenario: Default configurations are changed
  Given the user has Study in Operational Insights
  And the Threshold Setting is as following for default config
  | METRIC_NAME        | UPPER_THRESHOLD | LOWER_THRESHOLD | CATEGORY          |
  | Subjects Retention | 90              | 80              | STUDY PERFORMANCE |
  | Milestone Overdue  | 20              | 50              | STUDY PERFORMANCE |
  | Budget Spent       | 50              | 20              | STUDY PERFORMANCE |
  When the thresholds are modified as following for default config
  | METRIC_NAME        | UPPER_THRESHOLD | LOWER_THRESHOLD | CATEGORY          |
  | Subjects Retention | 80              | 70              | STUDY PERFORMANCE |
  | Milestone Overdue  | 30              | 60              | STUDY PERFORMANCE |
  | Budget Spent       | 40              | 10              | STUDY PERFORMANCE |
  Then the Thresholds are saved to the default config</code></pre>
<pre><code>Scenario: Remove personal config
  Given the user has saved to Personal Config
  | METRIC_NAME         | UPPER_THRESHOLD | LOWER_THRESHOLD | CATEGORY          |
  | Site Activation     | 85              | 70              | STUDY PERFORMANCE |
  | Subjects Enrolled   | 70              | 50              | STUDY PERFORMANCE |
  | Screen Failure Rate | 5               | 15              | STUDY PERFORMANCE |
  When Remove Personal Config is requested
  Then the user rows are deleted in the table</code></pre>
<pre><code>Scenario: Configuration changes are reflected in the dashboards
  Given user is in Operational Insights page
  And The Threshold settings are changed as follows
  | Metric       | Upper Limit | Lower Limit |
  | Locked CRF   | 31          | 66          |
  | Budget Spent | 14          | 41          |
  And is saved to personal config
  When user selects Study Performance for R13STD002
  Then the thresholds in donuts representing Locked CRF, Budget Spent is reflecting as follows
  | Locked CRF   | &lt;31 , 31 &lt;= 66 , &gt;66 |
  | Budget Spent | &lt;14 , 14&lt;=41 . &gt;41   |</code></pre>
<pre><code>Scenario: Default values are loaded when user logs in after log out
  Given the user has saved Threshold Settings to personal config
  And user logs out
  When user logs in again
  Then Default values are loaded in Threshold settings
  # Deprecated</code></pre>
<pre><code>Scenario: Go to enrollment overview dashboard from study insights section
  # StudyID needs to be passed from the UI to the Tableau dashboard embedded in Enrollment Overview page
  When the user is on the Study Insights section
  And the user selects the Enrollment Overview navigation option from the Conduct menu
  Then the user is brought to the Enrollment Overview page for the Study</code></pre>
<h2 data-number="6.15" id="administer-access"><span class="header-section-number">6.15</span> Administer Access</h2>
<p>The Administer Access use case is included in the Clinical Core Module. Please refer to the <a href="#role-information">Role Information</a> for details on each role.</p>
<p>The following are the user roles that have access to this use case:</p>
<ul>
<li>System Administrator</li>
<li>Study Administrator</li>
</ul>
<h3 data-number="6.15.1" id="base-flow---administer-access"><span class="header-section-number">6.15.1</span> Base Flow - Administer Access</h3>
<ol type="1">
<li>User selects Administer Access.</li>
<li>User <a href="#associate-user-to-roles">Associate User to Roles</a>.</li>
<li>User <a href="#associate-user-to-studies">Associate User to Studies</a>.</li>
<li>User <a href="#associate-user-to-projects">Associate User to Projects</a>.</li>
<li>User submits the information.</li>
<li>System captures the information and records it appropriately while maintaining an audit trail.</li>
</ol>
<h3 data-number="6.15.2" id="alternatives-9"><span class="header-section-number">6.15.2</span> Alternatives</h3>
<h4 data-number="6.15.2.1" id="a---user-lacks-permission-to-administer-access"><span class="header-section-number">6.15.2.1</span> 1a - User lacks permission to Administer Access</h4>
<ol type="1">
<li>The user is blocked from the Administer Access Screen</li>
</ol>
<h4 data-number="6.15.2.2" id="a-3a-4a---the-system-provides-user-list"><span class="header-section-number">6.15.2.2</span> 2a, 3a, 4a - The system provides user list</h4>
<ol type="1">
<li>A User is added in OKTA and assigned to Clinical</li>
<li>The user appears in the Administer Access user list</li>
<li>A user is removed from assignment to Clinical in OKTA and the Administer Access user assignments are removed and the user no longer appears in the list</li>
<li>A user is deactivated in OKTA and the Administer Access user assignments are removed and the user no longer appears in the list</li>
</ol>
<h3 data-number="6.15.3" id="feature-details-14"><span class="header-section-number">6.15.3</span> Feature Details</h3>
<h4 data-number="6.15.3.1" id="administer-user-roles"><span class="header-section-number">6.15.3.1</span> Administer User Roles</h4>
<h5 data-number="6.15.3.1.1" id="base-flow-scenarios-for-administer-access"><span class="header-section-number">6.15.3.1.1</span> Base Flow Scenarios for: Administer Access</h5>
<pre><code>Scenario: Provide access to the features based on the assigned user role
  # test data provided below are example data, not the full list of the role can access or can not access
  Given A user with an assigned role of Study Data Analyst
  When the user accesses the system
  Then the user has access to a list of features that contains
  | Resource_Name |
  | produce-SDTM  |
  | ingest-data   |
  And the user does not have access to a list of features that contains
  | Resource_Name      |
  | analytics-project  |
  | analyze-and-report |</code></pre>
<pre><code>Scenario: Provide access to the features based on the multiple assigned user roles
  # test data provided below are example data, not the full list of the role can access or can not access
  Given A user with an assigned role of Study Data Analyst and Study Administrator
  When the user accesses the system
  Then the user with multiple roles has access to a list of features that contains
  | Resource_Name                    |
  | produce-SDTM                     |
  | ingest-data                      |
  | administer-access-assign-studies |
  And the user with multiple roles does not have access to a list of features that contains
  | Resource_Name      |
  | analytics-project  |
  | analyze-and-report |</code></pre>
<pre><code>Scenario: Provide access to only the studies a user is assigned
  Given A user with assigned studies
  When the user accesses UI pages where a list of studies is displayed
  Then only the studies that the user is assigned are displayed</code></pre>
<pre><code>Scenario: Provide access to only the projects a user is assigned
  Given A user with assigned projects
  When the user accesses UI pages where a list of projects is displayed
  Then only the projects that the user is assigned are displayed</code></pre>
<h5 data-number="6.15.3.1.2" id="alternative-scenarios-for-user-lacks-permission-to-administer-access"><span class="header-section-number">6.15.3.1.2</span> Alternative Scenarios for: User lacks permission to Administer Access</h5>
<pre><code>Scenario: Covered by Administer Access</code></pre>
<h5 data-number="6.15.3.1.3" id="alternative-scenarios-for-the-system-provides-user-list"><span class="header-section-number">6.15.3.1.3</span> Alternative Scenarios for: The system provides user list</h5>
<pre><code>Scenario: Covered by sub-usecases</code></pre>
<h2 data-number="6.16" id="associate-user-to-roles"><span class="header-section-number">6.16</span> Associate User to Roles</h2>
<p>The Administer Access capability includes the assignment of role(s) to users which give the users access to the appropriate features</p>
<p>The following is the user role that has access to this use case:</p>
<ul>
<li>System Administrator</li>
</ul>
<h3 data-number="6.16.1" id="base-flow---associate-user-to-role"><span class="header-section-number">6.16.1</span> Base Flow - Associate User to Role</h3>
<ol type="1">
<li>User selects a User Name(s).</li>
<li>User selects Role(s).</li>
<li>User provides association between the Role and the User Name selected.</li>
<li>System makes the corresponding association.</li>
<li>Assigned role limits access of user to modules as defined.</li>
</ol>
<h3 data-number="6.16.2" id="feature-details-15"><span class="header-section-number">6.16.2</span> Feature Details</h3>
<h4 data-number="6.16.2.1" id="administer-user-roles-1"><span class="header-section-number">6.16.2.1</span> Administer User Roles</h4>
<h5 data-number="6.16.2.1.1" id="base-flow-scenarios-for-associate-user-to-role"><span class="header-section-number">6.16.2.1.1</span> Base Flow Scenarios for: Associate User to Role</h5>
<pre><code>Scenario: List users with assigned roles
  # if a user was not previously assigned a role, an empty role value is displayed
  Given one or more users were previously assigned to a role
  When admin lists users with assigned roles
  Then list of Clinical users with assigned roles is returned and displayed</code></pre>
<pre><code>Scenario: Assign roles to a user
  Given users are in Clinical group
  When admin assigns roles to user
  Then user and role associations are stored in database</code></pre>
<pre><code>Scenario: retrieval of users from Okta triggers removing any users that have status of deprovisioned
  Given one or more users were previously associated to roles or studies in Clinical
  And user has status of deprovisioned in Okta
  When user list is queried from Okta
  Then User is removed from clinical user list</code></pre>
<pre><code>Scenario: Remove assigned role from a user
  Given one or more users were previously assigned to a role
  When admin removes user role
  Then user and role association is removed from database</code></pre>
<pre><code>Scenario: List users with assigned roles with user entered filter criteria
  Given one or more users were previously assigned to roles
  And a search criteria is specified to filter by user name or role
  When admin executes the search
  Then filtered list of users with assigned roles is returned and displayed</code></pre>
<h2 data-number="6.17" id="associate-user-to-studies"><span class="header-section-number">6.17</span> Associate User to Studies</h2>
<p>The Administer Access capability includes the assignment of studies to users which give the users access to the assigned studies</p>
<p>The following are the user roles that have access to this use case:</p>
<ul>
<li>System Administrator</li>
<li>Study Administrator</li>
</ul>
<h3 data-number="6.17.1" id="base-flow---user-to-study-access-association"><span class="header-section-number">6.17.1</span> Base Flow - User to Study Access Association</h3>
<ol type="1">
<li>User selects User Name.</li>
<li>User selects Studies.</li>
<li>User must confirm that selected User to Study association is approved.</li>
<li>User provides association between the Studies and the User Name selected.</li>
<li>System makes the corresponding association.</li>
<li>The assigned user is limited to access the studies to which they are associated.</li>
</ol>
<h3 data-number="6.17.2" id="feature-details-16"><span class="header-section-number">6.17.2</span> Feature Details</h3>
<h4 data-number="6.17.2.1" id="administer-user-studies"><span class="header-section-number">6.17.2.1</span> Administer User Studies</h4>
<h5 data-number="6.17.2.1.1" id="base-flow-scenarios-for-user-to-study-access-association"><span class="header-section-number">6.17.2.1.1</span> Base Flow Scenarios for: User to Study Access Association</h5>
<pre><code>Scenario: List users with assigned studies
  # user to study association is currently independent of user assigned role
  Given one or more users were previously assigned to one or more studies
  When admin lists user assigned to studies
  Then list contains clinical users with assigned studies</code></pre>
<pre><code>Scenario: Assign studies to users with only active user assignments being saved
  # if a study was already assigned to a user, this API is no op for the existing assignment
  # this API does not remove existing assignment
  Given a list of studies
  When admin assigns studies to users
  Then valid users and studies associations are stored in database</code></pre>
<pre><code>Scenario: Remove a study from a user base on user action
  # API signature should be defined to accept a list of studies and a list of users even though this scenario only
  # calls for single study and single user to avoid having to deal with API backward compatibility issue in the
  # future when we extend to multiples.
  # If user to study association does not exist in Postgres, the API should not fail
  Given a user is previously assigned to studies
  When admin removes a study from a user
  Then user and study association is removed from database</code></pre>
<pre><code>Scenario: List users with assigned studies with search criteria
  # will extend to &#39;completed&#39; with server-side pagination and Behave tests in Sprint 5
  Given one or more users were previously assigned to one or more studies
  And a search criteria is specified to filter by user name or study attributes
  When admin user executes the search
  Then filtered list of active Clinical users in Okta with assigned studies is returned and displayed</code></pre>
<h2 data-number="6.18" id="associate-user-to-projects"><span class="header-section-number">6.18</span> Associate User to Projects</h2>
<p>The Administer Access capability includes the assignment of projects to users, which gives the users access to the assigned projects</p>
<p>The following are the user roles that have access to this use case:</p>
<ul>
<li>System Administrator</li>
<li>Clinical Programmer</li>
</ul>
<h3 data-number="6.18.1" id="base-flow---user-to-project-access-association"><span class="header-section-number">6.18.1</span> Base Flow - User to Project Access Association</h3>
<ol type="1">
<li>User selects Username(s) of users to be added to one or more projects.</li>
<li>User selects Project(s) and submits request.</li>
<li>System provisions access to the selected project(s) for the selected user(s) and notifies the user of completion.</li>
<li>The assigned user(s) can access the project(s) to which they are associated.</li>
</ol>
<h3 data-number="6.18.2" id="alternatives-10"><span class="header-section-number">6.18.2</span> Alternatives</h3>
<h4 data-number="6.18.2.1" id="a---user-does-not-have-access-to-one-or-more-studies-associated-to-the-analytics-project"><span class="header-section-number">6.18.2.1</span> 3a - User does not have access to one or more studies associated to the Analytics Project</h4>
<ol type="1">
<li>System does not provision access and displays a warning message indicating that the user does not have access to all studies for the project.</li>
</ol>
<h3 data-number="6.18.3" id="feature-details-17"><span class="header-section-number">6.18.3</span> Feature Details</h3>
<h4 data-number="6.18.3.1" id="administer-user-projects"><span class="header-section-number">6.18.3.1</span> Administer User Projects</h4>
<h5 data-number="6.18.3.1.1" id="base-flow-scenarios-for-user-to-project-access-association"><span class="header-section-number">6.18.3.1.1</span> Base Flow Scenarios for: User to Project Access Association</h5>
<pre><code>Scenario: List users with assigned projects
  # user to project association is currently independent of user assigned role
  # For admin user all users and all studies are shown
  Given one or more users were previously assigned to one or more projects
  When admin lists user assigned to projects
  Then list contains clinical users with assigned projects</code></pre>
<pre><code>Scenario: List users with assigned projects for a user who is not admin but is creator of projects
  # while system administrator can assign users to all projects a user who is assigned a project (owner) can only add users to their projects
  # Any user assigned to a project is not able to assign users project and would not see projects in the list
  # as part of the assignment the user is also given contributor assignment in Domino
  Given a user created a new project
  And one or more users were assigned to the new project by the creator
  When user lists users assigned to projects
  Then list contains clinical users with assigned projects only for projects that user is the creator</code></pre>
<pre><code>Scenario: retrieval of users from Okta triggers removing any users that have status of deprovisioned
  Given one or more users were previously associated to projects in Clinical
  And user has status of deprovisioned in Okta
  When user list is queried from Okta
  Then User and assigned projects are removed from clinical user list</code></pre>
<pre><code>Scenario: Assign projects to users
  # if a project was already assigned to a user, this API is no op for the existing assignment
  # System Admin can assign any project to any user as a contributor
  # Project Creator can only add contributors to the projects that they created
  # this API does not remove existing assignment
  Given a list of projects
  When user assigns projects to users
  Then users and projects associations are stored in database</code></pre>
<pre><code>Scenario: Remove a project from a user based on user action
  # API signature should be defined to accept a list of projects and a list of users even though this scenario only
  # calls for single project and single user to avoid having to deal with API backward compatibility issue in the
  # future when we extend to multiples.
  # If user to project association does not exist in Postgres, the API should not fail
  Given a user is previously assigned to projects
  When user removes a project from a user
  Then user and project association is removed from database</code></pre>
<pre><code>Scenario: List users with assigned projects with search criteria
  Given a user was previously assigned to one or more projects
  And a search criteria is specified to filter by user name or project attributes
  When user executes the search
  Then filtered list of active Clinical users in Okta with assigned projects is returned and displayed</code></pre>
<h5 data-number="6.18.3.1.2" id="alternative-scenarios-for-user-does-not-have-access-to-one-or-more-studies-associated-to-the-analytics-project"><span class="header-section-number">6.18.3.1.2</span> Alternative Scenarios for: User does not have access to one or more studies associated to the Analytics Project</h5>
<pre><code>Scenario: Prevent access to an Analytics Project when a user does not have access to all the Project&#39;s associated Studies
  Given user1 has permissions to add users to Analytics Project project1
  And user2 has permissions to be added to Analytics projects
  And project1 has three studies associated to it
  And user2 has permissions for only two of those studies
  When user1 requests to add user2 to project1
  Then access to project1 for user2 is denied
  # Must move these scenarios (or delete) to the appropriate Security feature file</code></pre>
<h2 data-number="6.19" id="role-information"><span class="header-section-number">6.19</span> Role Information</h2>
<table id="tcss">
<colgroup>
<col style="width: 37%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Role</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Portfolio Lead</td>
<td style="text-align: left;">Control Tower access to view portfolio dashboards.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Study Administrator</td>
<td style="text-align: left;">Clinical Core access to register studies, provide access to studies, ingest operational and study data.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Study Data Analyst</td>
<td style="text-align: left;">Study Data Engine access to automated engines such as Produce SDTM.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Study Data Lead</td>
<td style="text-align: left;">Study Data Engine access to Produce SDTM for a Clinical deliverable.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">System Administrator</td>
<td style="text-align: left;">System level access to all modules and features.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Project Lead</td>
<td style="text-align: left;">Control Tower access to view portfolio and study dashboards.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Integration Analyst</td>
<td style="text-align: left;">Clinical Core access to ingest data, create data providers, transform data for Information Marts use.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Clinical Programmer</td>
<td style="text-align: left;">Managed Analytics Environment access to the analytics environment, create analytics projects, create custom programs.</td>
</tr>
</tbody>
</table>
<h2 data-number="6.20" id="archive-data"><span class="header-section-number">6.20</span> Archive Data</h2>
<p>Archive study data and recurring study submissions. Archive study documents and artifacts such as study reports and documents within a submission package.</p>
<h3 data-number="6.20.1" id="process-archived-files"><span class="header-section-number">6.20.1</span> Process Archived Files</h3>
<ol type="1">
<li>An application is developed which pulls archived files from an external system.</li>
<li>Files are placed in a landing zone in GCS.</li>
<li>The files include a metadata file ending in “.archive”. This archive file triggers the ingestion process into the archive directory.</li>
<li>Missing Metadata File: If the metadata file is not the last file transferred, the files sent do not get archived.</li>
<li>View History Failure: A support ticket can be issued to check on the ingestion of specific files in the archive directory.</li>
</ol>
<h3 data-number="6.20.2" id="base-flow---archive-data"><span class="header-section-number">6.20.2</span> Base Flow - Archive Data</h3>
<ol type="1">
<li><a href="#register-provider">Register Provider</a> with Archive as its data category.</li>
<li><a href="#register-study">Register Study</a> with the Archive Only attribute set.</li>
<li>Via an external system integration, archived files are processed as indicated in <a href="#process-archived-files">Process_Archived_Files</a>.<br />
</li>
<li>The system versions the files once ingested.</li>
<li>The system administrator browses to the study and navigates to View History.</li>
<li>The system administrator expands the Event Files to view the archived files loaded.</li>
<li>The system administrator expands the All Files to view all archived files loaded as of that point in time.</li>
</ol>
<h3 data-number="6.20.3" id="feature-details-18"><span class="header-section-number">6.20.3</span> Feature Details</h3>
<h4 data-number="6.20.3.1" id="data-archive"><span class="header-section-number">6.20.3.1</span> Data Archive</h4>
<h5 data-number="6.20.3.1.1" id="base-flow-scenarios-for-archive-data"><span class="header-section-number">6.20.3.1.1</span> Base Flow Scenarios for: Archive Data</h5>
<pre><code>Scenario Outline: Ingest various types of archive data files
  Given more than one &lt;file_type&gt; files from a study marked for archiving in a directory
  When the files are ingested for archive
  Then the files are ingested into the respective archive only study folder
  Examples:
  | file_type |
  | json      |
  | zip       |</code></pre>
<pre><code>Scenario: Ingestion of files for a study marked for archiving in GCS
  Given a provider has been registered for archiving with &quot;archive only&quot; enabled
  And a study has been registered for archiving with &quot;archive only&quot; enabled
  When the files for a study are uploaded to the landing area
  And the metadata file is the last file uploaded
  Then archiving of the files are triggered
  And the files are ingested into the respective archive only study</code></pre>
<pre><code>Scenario: Ingestion of files for a provider/study that has not been registered
  Given a provider has been registered for archiving without &quot;archive only&quot; enabled
  And a study has been registered for archiving without &quot;archive only&quot; enabled
  When the files for a study are uploaded to the landing area
  And the metadata file is the last file uploaded
  Then archiving of the files are triggered
  And the archiving process does not perform the ingestion</code></pre>
<pre><code>Scenario: View history in an Archive study
  Given a study E2E_ARCHIVE_STUDY has been registered for archiving with &quot;archive only&quot; enabled
  And the user navigates to the archive study
  Then the user can view all of the version history with date and times of all ingested files</code></pre>
<pre><code>Scenario: Archived studies are not returned in lists where functionality that does not apply to them
  Given a study is marked as archived
  When the study endpoint is called with is_archive_only = False
  Then the list of studies returned has no archive studies</code></pre>
<pre><code>Scenario Outline: Disable routing to functional modules for archived studies
  Given a study is marked as archived
  When the URL for that route is accessed for &lt;module&gt;
  Then an error page is returned
  Examples:
  | module                        |
  | Clinical Control Tower        |
  | Study Data Engine             |
  | Managed Analytics Environment |</code></pre>
<pre><code>Scenario Outline: Disable UI elements for functional modules for archived studies
  Given a study is marked as archived
  When the user navigates to the &lt;page&gt;
  Then &lt;elements&gt; are not displayed
  Examples:
  | page          | elements                                                                                     |
  | Home          | Actions Menu                                                                                 |
  | Study Details | Produce SDTM, Analytics Project, Browse Data &amp; Snapshot, Schedule Extraction |</code></pre>
<h2 data-number="6.21" id="process-versioning"><span class="header-section-number">6.21</span> Process Versioning</h2>
<p>Goal of this feature is system will give ability to fetch the input, parameters and programs used to produce the output and will be able to reproduce same results in future. This is mainly useful when Drug agency comes with query after submission.</p>
<h3 data-number="6.21.1" id="base-flow---user-level-actions"><span class="header-section-number">6.21.1</span> Base Flow - User Level Actions</h3>
<ol type="1">
<li>User performs the actions on user workspace(Ex: Verify Mapping, Run conversion, etc)</li>
<li>A new folder created under user name which has the following data
<ol type="1">
<li>Input Data</li>
<li>Configuration used</li>
<li>Outputs produced</li>
<li>Logs</li>
</ol></li>
</ol>
<h3 data-number="6.21.2" id="base-flow---study-level-actions"><span class="header-section-number">6.21.2</span> Base Flow - Study Level Actions</h3>
<ol type="1">
<li>User perform action on Study workspace(Ex: Verify Mapping, Run conversion, etc)</li>
<li>A new folder created under user name which has the following data
<ol type="1">
<li>Input Data</li>
<li>Configuration used</li>
<li>Outputs produced</li>
<li>Logs</li>
</ol></li>
</ol>
<h3 data-number="6.21.3" id="feature-details-19"><span class="header-section-number">6.21.3</span> Feature Details</h3>
<h4 data-number="6.21.3.1" id="process-versioning-1"><span class="header-section-number">6.21.3.1</span> Process Versioning</h4>
<h5 data-number="6.21.3.1.1" id="base-flow-scenarios-for-versioning"><span class="header-section-number">6.21.3.1.1</span> Base Flow Scenarios for: Versioning</h5>
<h5 data-number="6.21.3.1.2" id="base-flow-scenarios-for-user-level-actions"><span class="header-section-number">6.21.3.1.2</span> Base Flow Scenarios for: User Level Actions</h5>
<pre><code>Scenario: Versioning Event is created for run conversion at user level
  Given ingestion of one or more files is done for a study
  And a configuration file is available
  When a user executes run conversion on a configuration file
  Then a versioning event for the user is created for run conversion
  And a run record is created that lists the inputs, configuration, reports, logs, and target table names
  And the run record file and the associated files are all added to the version history with a standard message
  And the user can view all of the version history with date and times</code></pre>
<pre><code>Scenario: Versioning Event is created for run conversion at user level with snapshot
  Given ingestion of one or more files is done for a study
  And a snapshot is available
  And the configuration file metadata calls for the use of the snapshot
  And a configuration file is available
  When a user executes Run Conversion on a configuration file
  Then a versioning event for the user is created for run conversion
  And a run record is created that lists the shapshot, configuration, reports, logs, and target table names
  And the run record file and the associated files are all added to the version history with a standard message
  And the user can view all of the version history with date and times</code></pre>
<pre><code>Scenario: Versioning Event is created for verify mapping at user level
  Given a configuration file is available
  When a user executes verify mapping on the configuration file
  Then a run record is created that lists the configuration, reports and logs
  And the run record and the new artifacts are all added to the version history with a standard message</code></pre>
<pre><code>Scenario: Versioning Event for verify mapping is viewable at user level
  Given a configuration file is available
  When a user executes verify mapping on the configuration file
  Then a run record is created that lists the configuration, reports and logs
  And the user can view the run record in the version history</code></pre>
<h5 data-number="6.21.3.1.3" id="base-flow-scenarios-for-study-level-actions"><span class="header-section-number">6.21.3.1.3</span> Base Flow Scenarios for: Study Level Actions</h5>
<pre><code>Scenario: Versioning Event is created for run conversion at study level
  Given ingestion of one or more files is done for a study
  And a promoted configuration file is available
  When a user executes Run Conversion on a configuration file
  Then a versioning event for the user is created for run conversion
  And a run record is created that lists the inputs, configuration, reports, logs, and target table names
  And the run record file and the associated files are all added to the study version history with a standard message
  And the user can view all of the version history with date and times</code></pre>
<pre><code>Scenario: Versioning Event is created for run conversion at study level with snapshot
  Given ingestion of one or more files is done for a study
  And a snapshot is available
  And a promoted configuration file is available
  And the configuration file metadata calls for the use of the snapshot
  When a user executes Run Conversion on a configuration file
  Then a versioning event for the study is created
  And a run record is created that lists the shapshot, configuration, reports, logs, and target table names
  And the run record file and the associated files are all added to the study version history with a standard message
  And the user can view all of the version history with date and times</code></pre>
<pre><code>Scenario: A selection of successful run conversion generated xpt files can be tagged and committed as a milestone
  Given configuration files have been promoted to the study workspace
  And a user executes run conversion on a configuration file
  And the export files have been generated
  And the study files in manage study package are selected for inclusion in a milestone
  When the user chooses to label study package
  And the user assigns a name to the package
  Then the milestone event and all selected XPT files are commited to the study workspace history</code></pre>
<h2 data-number="6.22" id="non-functional-requirements"><span class="header-section-number">6.22</span> Non-Functional Requirements</h2>
<p>Non-functional requirements (NFR) are described in the non-functional feature file. NFRs include:</p>
<ul>
<li>User interface (UI) design that support the various use cases included in the INTIENT Clinical suite.</li>
<li>Other characteristics for the UI such as use of style sheets.</li>
<li>Audit trail scenarios that are the bases for transactional records management.</li>
<li>GCS buckets and folder structures, as well as BigQuery Dataset naming conventions.</li>
<li>Versioning that records transaction events with a time-point registry and allows review of version history and subsequent recovery of history data.</li>
</ul>
<h3 data-number="6.22.1" id="feature-details-20"><span class="header-section-number">6.22.1</span> Feature Details</h3>
<h4 data-number="6.22.1.1" id="non-functional-requirements-1"><span class="header-section-number">6.22.1.1</span> Non-Functional Requirements</h4>
<h5 data-number="6.22.1.1.1" id="base-flow-scenarios-for-user-interface-requirements"><span class="header-section-number">6.22.1.1.1</span> Base Flow Scenarios for: User Interface Requirements</h5>
<pre><code>Scenario: UI adheres to the INTIENT style guide
  When a UI page is displayed
  Then the UI page adheres to the INTIENT style guide &quot;INTIENT_Iris-Design-System-Guidelines_v1.4.pdf&quot;</code></pre>
<pre><code>Scenario: UI adheres to wireframe document
  When a UI page is displayed
  Then the UI page adheres to the wireframes document &quot;wireframesUpdate-V21.pdf&quot;</code></pre>
<h5 data-number="6.22.1.1.2" id="base-flow-scenarios-for-audit-trail"><span class="header-section-number">6.22.1.1.2</span> Base Flow Scenarios for: Audit Trail</h5>
<pre><code>Scenario: Capture audit trail information for transactional records
  Given a table test_audit_trail
  | column name | data type    | is key |
  | test_id     | integer      | Y      |
  | test_name   | varchar(100) | N      |
  | test_date   | timestamp    | N      |
  And test_audit_trail has the insert, update, and delete triggers applied
  And the corresponding audit table is created in the admin_audit schema
  When the records are modified in test_audit_trail with the following operations and data
  | operation | test_id | test_name    | test_date           |
  | insert    | 1       | INSERT TEST  | &lt;current_timestamp&gt; |
  | update    | 1       | UPDATE TEST  |                     |
  | update    | 1       | UPDATE TEST2 |                     |
  | delete    | 1       |              |                     |
  Then the following records exist in the audit table
  | test_id | test_name    | test_date           | audit_operation | audit_reason             | audit_datetime         | audit_user     |
  | 1       | INSERT TEST  | &lt;current_timestamp&gt; | I               | test audit trail created | &lt;current UTC datetime&gt; | &lt;current user&gt; |
  | 1       | UPDATE TEST  | &lt;current_timestamp&gt; | U               | test audit trail update  | &lt;current UTC datetime&gt; | &lt;current user&gt; |
  | 1       | UPDATE TEST2 | &lt;current_timestamp&gt; | U               | test audit trail update  | &lt;current UTC datetime&gt; | &lt;current user&gt; |
  | 1       | UPDATE TEST2 | &lt;current_timestamp&gt; | D               | test audit trail delete  | &lt;current UTC datetime&gt; | &lt;current user&gt; |</code></pre>
<pre><code>Scenario: Audit trail capture is set up on all data entities
  Given a transactional schema
  And an audit schema
  When transactional tables are present in the transactional schema
  Then all transactional tables have an audit trigger
  And all transactional tables have default create utc datetime
  And all transactional tables have a corresponding table in the audit schema
  And all columns in the transactional table are present in the corresponding table in the audit schema</code></pre>
<h5 data-number="6.22.1.1.3" id="base-flow-scenarios-for-control-of-module-visibility"><span class="header-section-number">6.22.1.1.3</span> Base Flow Scenarios for: Control of Module Visibility</h5>
<pre><code>Scenario Outline: Enable control of accessible functionality and UI visibility via feature flags
  # Restricted access to functionality may based on individual permissions, role,
  # organization not purchasing a module, unreleased/beta-only features, etc.
  Given an organization does not have access to &lt;functionality&gt;
  # We could be explicit about what should not be visible, but specifying it explicitly will be brittle - can discuss
  Then visibility to &lt;element name&gt; is turned off in &lt;element location&gt;
  And access to &lt;target location&gt; is removed
  Examples:
  | functionality          | element name                   | element location       | target location                   |
  | Demo                   | Tasks                          | Main menu              | Tasks                             |
  | Demo                   | Milestones                     | Study Details screen   | Milestones Tab on Study Details   |
  | Demo                   | Milestones                     | Project Details screen | Milestones Tab on Project Details |
  | Study Data Engine      | Produce SDTM                   | Actions menu           | SDTM pages                        |
  | Study Data Engine      | Produce SDTM                   | Study Details          | SDTM pages                        |
  | Managed Analytics      | Analytics Project              | Actions menu           | Project Details                   |
  | Managed Analytics      | Analytics Project              | Study Details          | Project Details                   |
  | Managed Analytics      | Analyze &amp; Report               | Main menu              | Analyze &amp; Report                  |
  | Managed Analytics      | Analytics Project Registration | Analyze &amp; Report       | Register Analytics Project        |
  | Managed Analytics      | Projects                       | Administer Access      | Project Administration            |
  | Clinical Control Tower | Monitor Study                  | Actions menu           | Monitor Study                     |
  | Clinical Control Tower | Monitor Study                  | Study Details          | Monitor Study                     |
  | Clinical Control Tower | Monitor Portfolio              | Main menu              | Monitor Portfolio                 |</code></pre>
<h5 data-number="6.22.1.1.4" id="base-flow-scenarios-for-gcp-naming-conventions-gcs-buckets-and-bq-datasets"><span class="header-section-number">6.22.1.1.4</span> Base Flow Scenarios for: GCP Naming Conventions (GCS Buckets and BQ Datasets)</h5>
<pre><code>Scenario Outline: All GCS folders are created according to a chosen naming convention
  When a GCS folder is created for an &lt;entity&gt;
  Then it conforms to the folder structure laid out in docs/architecture/designs/intient-clinical-design.html Load Data section
  Examples:
  | entity                |
  | study                 |
  | provider              |
  | user under a study    |
  | user under a provider |</code></pre>
<pre><code>Scenario Outline: All BQ Datasets are created according to a chosen naming convention
  When a &lt;projectType&gt; BQ Dataset is created at the &lt;level&gt; level
  Then the Dataset name is entirely in capital letters
  And the name of the Dataset includes a prefix of the form &lt;instance&gt;_&lt;projectType&gt;_
  And the remainder of the Dataset name follows the conventions appropriate to the dataset being created as outlined in docs/architecture/designs/intient-clinical-design.html BigQuery section
  And special characters are replaced by underscores combined in such a way as to have no more than one consecutive underscore
  And there are no leading or trailing underscores
  Examples:
  | level  | projectType | instance                |
  | entity | STUDY       | &lt;manifestInstanceValue&gt; |
  | entity | OPS         | &lt;manifestInstanceValue&gt; |
  | user   | STUDY       | &lt;manifestInstanceValue&gt; |
  | user   | OPS         | &lt;manifestInstanceValue&gt; |</code></pre>
<h5 data-number="6.22.1.1.5" id="base-flow-scenarios-for-user-attempts-to-bypass-ui-restrictions-to-register-a-study"><span class="header-section-number">6.22.1.1.5</span> Base Flow Scenarios for: User attempts to bypass UI restrictions to register a study</h5>
<pre><code>Scenario: Unauthorized API access attempts are denied
  Given a comprehensive list of API endpoints
  When each endpoint is called by an account with a token that corresponds to a user that is not assigned a role
  Then the request is denied and an error is returned</code></pre>
<h5 data-number="6.22.1.1.6" id="base-flow-scenarios-for-performance-of-intient-clinical"><span class="header-section-number">6.22.1.1.6</span> Base Flow Scenarios for: Performance of INTIENT Clinical</h5>
<h1 data-number="7" id="revision-history"><span class="header-section-number">7</span> Revision History</h1>
<table id="tcss">
<colgroup>
<col style="width: 8%" />
<col style="width: 13%" />
<col style="width: 14%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Version</th>
<th style="text-align: left;">Date Modified</th>
<th style="text-align: left;">Revised By</th>
<th style="text-align: left;">Role</th>
<th style="text-align: left;">Section Affected</th>
<th style="text-align: left;">Change Details</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">11-Aug-2020</td>
<td style="text-align: left;">Lourdes Devenney</td>
<td style="text-align: left;">Product Manager</td>
<td style="text-align: left;">All sections</td>
<td style="text-align: left;">New Version</td>
</tr>
<tr class="even">
<td style="text-align: left;">2.0</td>
<td style="text-align: left;">16-Nov-2020</td>
<td style="text-align: left;">Lourdes Devenney</td>
<td style="text-align: left;">Product Manager</td>
<td style="text-align: left;">Added  Updated : Manual Study Registration, Batch Study Registration,<br />Administer Access, Associate User to Roles, Associate User to Studies, <br />Associate User to Projects, Role Information &amp; Non-Functional Requirements,<br /> data extraction on demand</td>
<td style="text-align: left;">Revised Version</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3.0</td>
<td style="text-align: left;">15-Feb-2021</td>
<td style="text-align: left;">James Taylor</td>
<td style="text-align: left;">Business Analyst</td>
<td style="text-align: left;">Added  Updated : Versioning &amp; NFR</td>
<td style="text-align: left;">Revised Version</td>
</tr>
<tr class="even">
<td style="text-align: left;">4.0</td>
<td style="text-align: left;">09-Aug-2021</td>
<td style="text-align: left;">Dhruva Kusha</td>
<td style="text-align: left;">Business Analyst</td>
<td style="text-align: left;"><br />5.0<br /><br />6.2<br /><br />6.11<br /><br />6.13<br /><br />6.14</td>
<td style="text-align: left;">Revised as part of GA 1.3:<br /><br />Updated Product Overview<br /><br />Updated Ingest Data<br /><br />Updated Study Registration<br /><br />Clinical Insights section added<br /><br />Operational Insights section added</td>
</tr>
</tbody>
</table>
<h1 data-number="8" id="document-approvals"><span class="header-section-number">8</span> Document Approvals</h1>
<table id="tcss">
<thead>
<tr class="header">
<th style="text-align: left;">Role</th>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Organizational Capacity</th>
<th style="text-align: left;">Signature</th>
<th style="text-align: left;">Date</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Author</td>
<td style="text-align: left;">Dhruva Kusha</td>
<td style="text-align: left;">Business Analyst</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Level 1 Approver</td>
<td style="text-align: left;">Nathaniel Pung</td>
<td style="text-align: left;">Product Manager</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Level 1 Approver</td>
<td style="text-align: left;">Anil Karunakaran</td>
<td style="text-align: left;">Technical Lead</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Level 1 Approver</td>
<td style="text-align: left;">Swathi Balakrishna</td>
<td style="text-align: left;">Validation Manager</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Level 2 Approver</td>
<td style="text-align: left;">Sarada Ramamoorthy</td>
<td style="text-align: left;">Quality Assurance Lead</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</body>
</html>
